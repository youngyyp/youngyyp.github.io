<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>transformer基础 | young&#39;s blog</title>
  <meta name="keywords" content="">
  <meta name="description" content="transformer基础 | young&#39;s blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="Oops～，我崩溃了！找不到你想要的页面了">
<meta property="og:type" content="website">
<meta property="og:title" content="404">
<meta property="og:url" content="https://youngyyp.github.io/404.html">
<meta property="og:site_name" content="young&#39;s blog">
<meta property="og:description" content="Oops～，我崩溃了！找不到你想要的页面了">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-02-23T11:37:07.000Z">
<meta property="article:modified_time" content="2021-02-10T02:51:50.601Z">
<meta property="article:author" content="young">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/avatar.png">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 6.0.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="young's blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/"
   class="avatar_target">
    <img class="avatar"
         src="/img/1.png"/>
</a>
<div class="author">
    <span>young</span>
</div>

<div class="icon">
    
</div>




<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(47)</small>
            
        </div>
    </li>
    
        
            
                <li>
                    <div data-rel="计算机视觉">
                        
                        计算机视觉
                        <small>(20)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="嵌入式和硬件">
                        
                        嵌入式和硬件
                        <small>(2)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="软件使用">
                        
                        软件使用
                        <small>(3)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="深度学习基础">
                        
                        深度学习基础
                        <small>(2)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="CPP">
                        
                        CPP
                        <small>(13)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="FPGA">
                        
                        FPGA
                        <small>(6)</small>
                        
                    </div>
                    
                </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
    </div>
    <div>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="47">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>目标检测</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>目标识别</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>深度学习</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>神经网络</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>事件相机</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>数电</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>特征匹配</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>图像处理</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>图像匹配</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>位姿估计</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>线路故障检测</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>障碍物识别</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>正交解调，FPGA</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>自平衡电桥</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Atlas</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>batch norm</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>dehazing</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>haze removal</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>hexo指令</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>heze removal</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>HLS</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>modelsim</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>pytorch</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>SLAM</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>test time adaptation</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>ubuntu</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>verilog</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>vivado</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>yolo</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>ZYNQ</a>
            </li>
        
    </div>

</div>

    
    <nav id="title-list-nav">
        
        
        <a  class="全部文章 "
           href="/2023/06/26/work/%E7%AE%80%E5%8E%86%E4%BC%98%E5%8C%96/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-06-26 22:14:08">2023/06/26</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/04/27/cpp/%E5%9B%BE/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="图">图</span>
            <span class="post-date" title="2023-04-27 10:13:33">2023/04/27</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/03/01/cpp/ACM%E6%A8%A1%E5%BC%8F/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ACM模式">ACM模式</span>
            <span class="post-date" title="2023-03-01 21:14:33">2023/03/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/03/01/cpp/%E5%8D%95%E8%B0%83%E6%A0%88/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="单调栈">单调栈</span>
            <span class="post-date" title="2023-03-01 21:14:33">2023/03/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/03/01/cpp/%E5%89%91%E6%8C%87offer/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="剑指offer">剑指offer</span>
            <span class="post-date" title="2023-03-01 21:14:33">2023/03/01</span>
        </a>
        
        
        <a  class="全部文章 深度学习基础 "
           href="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E7%9B%B8%E5%85%B3/%E7%BB%8F%E5%85%B8%E5%85%AB%E8%82%A1/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="经典八股">经典八股</span>
            <span class="post-date" title="2023-03-01 21:14:33">2023/03/01</span>
        </a>
        
        
        <a  class="全部文章 深度学习基础 "
           href="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="transformer基础">transformer基础</span>
            <span class="post-date" title="2023-03-01 21:14:33">2023/03/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="动态规划">动态规划</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E5%AD%97%E7%AC%A6%E4%B8%B2/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="字符串">字符串</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E6%95%B0%E7%BB%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="数组">数组</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E9%93%BE%E8%A1%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="链表">链表</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="贪心算法">贪心算法</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E5%93%88%E5%B8%8C%E8%A1%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="哈希表">哈希表</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E5%9B%9E%E6%BA%AF/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="回溯">回溯</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="栈与队列">栈与队列</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 CPP "
           href="/2023/02/01/cpp/%E4%BA%8C%E5%8F%89%E6%A0%91/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="二叉树">二叉树</span>
            <span class="post-date" title="2023-02-01 21:14:33">2023/02/01</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/"
           data-tag="pytorch"
           data-author="" >
            <span class="post-title" title="pytorch冻结网络模型">pytorch冻结网络模型</span>
            <span class="post-date" title="2022-11-08 09:49:41">2022/11/08</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/UAV&Satellite/%E5%BC%80%E9%A2%98%E7%9B%B8%E5%85%B3/"
           data-tag="图像匹配"
           data-author="" >
            <span class="post-title" title="UAV和卫星图像配准思路">UAV和卫星图像配准思路</span>
            <span class="post-date" title="2022-11-08 09:49:41">2022/11/08</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/UAV&Satellite/%E8%B0%83%E7%A0%94/"
           data-tag="图像匹配"
           data-author="" >
            <span class="post-title" title="UAV和卫星图像配准论文阅读">UAV和卫星图像配准论文阅读</span>
            <span class="post-date" title="2022-11-08 09:49:41">2022/11/08</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/Feature%20Matching/LoFTR/"
           data-tag="特征匹配"
           data-author="" >
            <span class="post-title" title="LoFTR代码">LoFTR代码</span>
            <span class="post-date" title="2022-11-07 09:49:41">2022/11/07</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/test_time_adaptation/%E7%90%86%E8%A7%A3BN/"
           data-tag="batch norm"
           data-author="" >
            <span class="post-title" title="理解BN">理解BN</span>
            <span class="post-date" title="2022-11-07 09:49:41">2022/11/07</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/test_time_adaptation/Low%20Dimensional%20Trajectory%20Hypothesis%20is%20True/"
           data-tag="test time adaptation"
           data-author="" >
            <span class="post-title" title="子空间论文阅读">子空间论文阅读</span>
            <span class="post-date" title="2022-11-07 09:49:41">2022/11/07</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/test_time_adaptation/TENT/"
           data-tag="test time adaptation"
           data-author="" >
            <span class="post-title" title="TENT论文阅读">TENT论文阅读</span>
            <span class="post-date" title="2022-11-07 09:49:41">2022/11/07</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/test_time_adaptation/%E5%BC%80%E9%A2%98/"
           data-tag="test time adaptation"
           data-author="" >
            <span class="post-title" title="TTA开题">TTA开题</span>
            <span class="post-date" title="2022-11-07 09:49:41">2022/11/07</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/test_time_adaptation/%E6%B3%9B%E8%AF%BB/"
           data-tag="test time adaptation"
           data-author="" >
            <span class="post-title" title="TTA论文泛读">TTA论文泛读</span>
            <span class="post-date" title="2022-11-07 09:49:41">2022/11/07</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/10/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"
           data-tag="目标检测"
           data-author="" >
            <span class="post-title" title="目标检测基础知识">目标检测基础知识</span>
            <span class="post-date" title="2022-10-26 15:49:06">2022/10/26</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/04/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/Feature%20Matching/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"
           data-tag="图像匹配"
           data-author="" >
            <span class="post-title" title="特征匹配论文">特征匹配论文</span>
            <span class="post-date" title="2022-04-10 16:52:31">2022/04/10</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2022/01/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/yolo/"
           data-tag="yolo"
           data-author="" >
            <span class="post-title" title="YOLO使用">YOLO使用</span>
            <span class="post-date" title="2022-01-01 13:23:06">2022/01/01</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/"
           data-tag="位姿估计"
           data-author="" >
            <span class="post-title" title="飞机6D位姿估计">飞机6D位姿估计</span>
            <span class="post-date" title="2021-12-10 16:52:31">2021/12/10</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/11/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/"
           data-tag="SLAM"
           data-author="" >
            <span class="post-title" title="SLAM十四讲代码bug及解决">SLAM十四讲代码bug及解决</span>
            <span class="post-date" title="2021-11-24 18:50:44">2021/11/24</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA/%E8%B0%83%E7%A0%94/"
           data-tag="事件相机"
           data-author="" >
            <span class="post-title" title="事件相机调研">事件相机调研</span>
            <span class="post-date" title="2021-11-10 19:49:06">2021/11/10</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/10/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8%E5%8E%BB%E9%9B%BE/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%95%B4%E7%90%86/"
           data-tag="dehazing,haze removal"
           data-author="" >
            <span class="post-title" title="图像去雾去雨论文整理">图像去雾去雨论文整理</span>
            <span class="post-date" title="2021-10-22 12:00:00">2021/10/22</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"
           data-tag="位姿估计"
           data-author="" >
            <span class="post-title" title="卫星姿态估计调研">卫星姿态估计调研</span>
            <span class="post-date" title="2021-10-20 09:49:41">2021/10/20</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/"
           data-tag="位姿估计"
           data-author="" >
            <span class="post-title" title="位姿估计基础及论文">位姿估计基础及论文</span>
            <span class="post-date" title="2021-10-20 09:49:41">2021/10/20</span>
        </a>
        
        
        <a  class="全部文章 软件使用 "
           href="/2021/10/08/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/%E5%85%B6%E4%BB%96/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="其他软件使用">其他软件使用</span>
            <span class="post-date" title="2021-10-08 09:49:41">2021/10/08</span>
        </a>
        
        
        <a  class="全部文章 软件使用 "
           href="/2021/09/23/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/ubuntu(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220829094535)/"
           data-tag="ubuntu"
           data-author="" >
            <span class="post-title" title="ubuntu相关">ubuntu相关</span>
            <span class="post-date" title="2021-09-23 10:29:44">2021/09/23</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/09/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8%E5%8E%BB%E9%9B%BE/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE/"
           data-tag="dehazing,heze removal"
           data-author="" >
            <span class="post-title" title="图像去雾">图像去雾</span>
            <span class="post-date" title="2021-09-23 10:29:44">2021/09/23</span>
        </a>
        
        
        <a  class="全部文章 FPGA "
           href="/2021/09/05/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/HDLBits%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"
           data-tag="verilog,数电"
           data-author="" >
            <span class="post-title" title="HDLBits刷题笔记">HDLBits刷题笔记</span>
            <span class="post-date" title="2021-09-05 11:16:10">2021/09/05</span>
        </a>
        
        
        <a  class="全部文章 FPGA "
           href="/2021/08/04/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/ZYNQ%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
           data-tag="ZYNQ,vivado"
           data-author="" >
            <span class="post-title" title="ZYNQ学习笔记">ZYNQ学习笔记</span>
            <span class="post-date" title="2021-08-04 19:03:34">2021/08/04</span>
        </a>
        
        
        <a  class="全部文章 FPGA "
           href="/2021/07/30/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/HLS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
           data-tag="HLS"
           data-author="" >
            <span class="post-title" title="HLS学习笔记">HLS学习笔记</span>
            <span class="post-date" title="2021-07-30 10:15:16">2021/07/30</span>
        </a>
        
        
        <a  class="全部文章 FPGA "
           href="/2021/07/30/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/vivado2020-2%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"
           data-tag="vivado"
           data-author="" >
            <span class="post-title" title="vivado2020.2安装教程">vivado2020.2安装教程</span>
            <span class="post-date" title="2021-07-30 09:39:08">2021/07/30</span>
        </a>
        
        
        <a  class="全部文章 FPGA "
           href="/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"
           data-tag="图像处理"
           data-author="" >
            <span class="post-title" title="【FPGA】图像处理">【FPGA】图像处理</span>
            <span class="post-date" title="2021-07-23 14:52:46">2021/07/23</span>
        </a>
        
        
        <a  class="全部文章 FPGA "
           href="/2021/07/16/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/verilog%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
           data-tag="verilog,modelsim"
           data-author="" >
            <span class="post-title" title="verilog学习笔记">verilog学习笔记</span>
            <span class="post-date" title="2021-07-16 15:50:28">2021/07/16</span>
        </a>
        
        
        <a  class="全部文章 嵌入式和硬件 "
           href="/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/"
           data-tag="线路故障检测,自平衡电桥,正交解调，FPGA"
           data-author="" >
            <span class="post-title" title="【电赛】线路负载及故障检测装置">【电赛】线路负载及故障检测装置</span>
            <span class="post-date" title="2021-03-13 20:43:36">2021/03/13</span>
        </a>
        
        
        <a  class="全部文章 嵌入式和硬件 "
           href="/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"
           data-tag="Atlas,深度学习,神经网络,障碍物识别,目标识别"
           data-author="" >
            <span class="post-title" title="【本科毕业设计】基于Atlas_200_DK的障碍物识别系统设计与实现">【本科毕业设计】基于Atlas_200_DK的障碍物识别系统设计与实现</span>
            <span class="post-date" title="2021-03-12 19:20:57">2021/03/12</span>
        </a>
        
        
        <a  class="全部文章 软件使用 "
           href="/2021/02/12/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/%E5%B8%B8%E7%94%A8hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/"
           data-tag="hexo指令"
           data-author="" >
            <span class="post-title" title="常用hexo博客操作">常用hexo博客操作</span>
            <span class="post-date" title="2021-02-12 13:14:45">2021/02/12</span>
        </a>
        
        
        <a  class="全部文章 计算机视觉 "
           href="/2021/01/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1review/"
           data-tag="位姿估计"
           data-author="" >
            <span class="post-title" title="卫星姿态估计综述">卫星姿态估计综述</span>
            <span class="post-date" title="2021-01-16 10:42:41">2021/01/16</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-深度学习基础/transformer/基础知识" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">transformer基础</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            <i class="iconfont icon-category"></i>
            
            
            <a  data-rel="深度学习基础">深度学习基础</a>
            
        </span>
        
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title='最后更新: 2023-06-26 22:08:09'>2023-03-01 21:14</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-text">特点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%B9%B3%E7%A7%BB%E7%AD%89%E5%8F%98%E6%80%A7%EF%BC%9A"><span class="toc-text">卷积的平移等变性：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B9%9F%E6%9C%89%E8%AE%BA%E6%96%87%E8%AE%A4%E4%B8%BAcnn%E4%B8%8D%E5%85%B7%E5%A4%87%E5%B9%B3%E7%A7%BB%E7%AD%89%E5%8F%98%E6%80%A7%EF%BC%9A"><span class="toc-text">也有论文认为cnn不具备平移等变性：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-text">结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-text">参考链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%85%AB%E8%82%A1"><span class="toc-text">常见八股</span></a></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>能够适应大数据，泛化性更强（transformer上限高，cnn下限高）</p>
<p>全局感受野</p>
<p>更好的处理序列数据能力</p>
<p>并行计算能力</p>
<p>更容易扩展到其他任务</p>
<p>缺点：</p>
<p>没有平移、缩放、形变的不变性（CNN存在归纳偏置，局部性和平移等变性）</p>
<p>CNN有强归纳偏置性,所以在小数据集上表现更好。但如果有大量的数据可用,CNN的强归纳偏置性反而会限制模型的能力</p>
<h3 id="卷积的平移等变性："><a href="#卷积的平移等变性：" class="headerlink" title="卷积的平移等变性："></a>卷积的平移等变性：</h3><p><strong>卷积+最大池化等于平移等变性</strong>。</p>
<p>卷积：简单地说，<strong>图像经过平移，相应的特征图上的表达也是平移的。</strong>输入图像的左下角有一个人脸，经过卷积，人脸的特征（眼睛，鼻子）也位于特征图的左下角</p>
<p>池化：比如最大池化，它返回感受野中的最大值，<strong>如果最大值被移动了，但是仍然在这个感受野中</strong>，那么池化层也仍然会输出相同的最大值。这就有点平移不变的意思了。</p>
<h4 id="也有论文认为cnn不具备平移等变性："><a href="#也有论文认为cnn不具备平移等变性：" class="headerlink" title="也有论文认为cnn不具备平移等变性："></a>也有论文认为cnn不具备平移等变性：</h4><p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.12177.pdf">arxiv.org/pdf/1805.12177.pdf</a></p>
<p>希伯来大学的Ah<a href="https://link.zhihu.com/?target=https%3A//www.jqr.com/service/company%3Fbusiness%3D17">ar</a>on Azulay和Yair Weiss近期发表的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.12177.pdf">Why do deep convolutional networks generalize so poorly to small image transformations?</a> 这篇文章发现当小尺寸图像发生平移后，CNN会出现识别错误的现象，而且这一现象是普遍的。</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>Transformer采用Encoder-Decoder架构，N一般取6</p>
<p><strong>encoder</strong>主要包括self-attention和前馈网络两个模块，每个模块之后还会加上残差连接和layernorm</p>
<p>attention本质上就是通过加权求和获得对上下文的全局感知，qk计算权重，再乘以v嵌入上下文信息</p>
<p>每个向量的q 和 所有向量的k 相乘得到的权重就是attention，用softmax过滤掉不相干的单词，乘以v向量进行加权求和</p>
<p>encoder基本结构：Embedding + Position Embedding，Self-Attention，Add + LN，FN，Add + LN</p>
<p><strong>decoder</strong>主要包括masked multihead attention(self-attention)、encoder-decoder attention(cross attention) 和 feed forward network三个模块，每个模块之后还会加上残差连接和layernorm</p>
<p>encoder-decoder attention不仅考虑已经翻译的内容，还要考虑encoder中上下文的信息。解码器的输出经过线性层和softmax层，得到输出</p>
<p>每个self-attention又会分解成几个部分，称为multi-head self-attention（通常8头），在这之中使用了不同的权重矩阵qkv进行了8次计算，这样做的目的是为了消除QKV初始值的影响（一件事找八个人做，万一哪个不靠谱也不影响）</p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230426094514229.png" alt="image-20230426094514229" style="zoom:50%;"></p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230426095027000.png" alt="image-20230426095027000" style="zoom:50%;"></p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230426095826505.png" alt="image-20230426095826505" style="zoom: 33%;"></p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230426095902928.png" alt="image-20230426095902928" style="zoom: 33%;"></p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230426100259964.png" alt="image-20230426100259964" style="zoom:50%;"></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer">很好的讲解transformer的文章</a></p>
<h2 id="常见八股"><a href="#常见八股" class="headerlink" title="常见八股"></a>常见八股</h2><p>1.Transformer为何使用多头注意力机制？（为什么不使用一个头）</p>
<ul>
<li>Transformer使用多头注意力机制是为了增强模型的表达能力，可以让模型在不同的表征空间内学习到不同的特征表示。如果只使用一个头，模型就只能学习到单一的特征表示，而无法利用多个空间内的特征信息。</li>
<li>通俗来说就是从多个方面来进行理解</li>
</ul>
<p>2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？ （注意和第一个问题的区别）</p>
<ul>
<li><p>简单回答就是，使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</p>
</li>
<li><p><em>q</em>就是query，k就是key，v就是value，(k,v)就是键值对、也就是用query关键词去找到最相关的检索结果</p>
</li>
<li><p>Q和K使用不同的权重矩阵生成是因为它们的含义不同。Q是查询向量，K是键向量，它们代表了不同的信息，因此使用不同的权重矩阵可以让它们学习不同的表示。</p>
</li>
</ul>
<p>3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</p>
<ul>
<li>Transformer计算attention时选择点乘而不是加法，是因为点乘可以反映不同向量之间的相似度，更好地捕捉到语义信息的相关性。而加法只是简单地把两个向量相加，不能反映语义信息的相关性。从计算复杂度和效果上看，点乘更适合文本序列的处理。</li>
</ul>
<p>4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230511101521508.png" alt="image-20230511101521508" style="zoom:50%;"></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jokerxsy/article/details/116299343">Transformer中的attention为什么要做scale？</a></p>
</li>
<li><p>一句话：</p>
</li>
<li><p><strong>如果不对softmax的输入做缩放，那么万一输入的数量级很大，输出会趋向1，softmax的梯度就会趋向于0，导致梯度消失。</strong>假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为<script type="math/tex">d_k</script>。方差越大，就有更可能取到很大的值，所以我们除以dk的平方根，使其方差为1。</p>
</li>
</ul>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pva2VyeHN5,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述"></p>
<p>5.在计算attention score的时候如何对padding做mask操作？</p>
<ul>
<li><p><strong>Padding Mask</strong></p>
<p>  什么是padding mask呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的Attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p>
<p>  具体的做法是，<strong>把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0</strong>！ 而我们的padding mask 实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方</p>
</li>
<li><p><strong>Sequence mask</strong></p>
<p>  文章前面也提到，sequence mask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p>
<p>  sequence mask的目的是防止Decoder “seeing the future”，就像防止考生偷看考试答案一样。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。下面是个10维度的下三角矩阵：</p>
<p>  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</p>
<p>   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],</p>
<p>   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</p>
<p>   [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],</p>
<p>   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</p>
<p>   [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],</p>
<p>   [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],</p>
<p>   [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],</p>
<p>   [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],</p>
<p>   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]</p>
</li>
</ul>
<p>6.为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）</p>
<ul>
<li>借鉴CNN多核的思想，在更低的维度，在多个独立的特征空间，更容易学习到更丰富的特征信息。</li>
<li>将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量</li>
</ul>
<p>7.大概讲一下Transformer的Encoder模块？</p>
<ul>
<li><strong>encoder</strong>主要包括self-attention和前馈网络两个模块，每个模块之后还会加上残差连接和layernorm</li>
</ul>
<p>8.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</p>
<ul>
<li><p>对矩阵乘以embedding size的开方是为了将词向量的scale调整到合适的范围，避免过大或过小的梯度对模型的训练产生影响。</p>
</li>
<li><p>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p>
</li>
</ul>
<p>9.简单介绍一下Transformer的位置编码？有什么意义和优缺点？</p>
<ul>
<li><p>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</p>
</li>
<li><p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230512112447304.png" alt="image-20230512112447304" style="zoom:50%;"></p>
</li>
<li><p>理解transformer的作者为什么设计这样的编码方式来生成位置向量呢? 因为三角函数有个性质</p>
<p>  ​          sin(a+b) = sin(a) <em> cos(b) + cos(a) </em> sin(b)</p>
<p>  ​         cos(a+b) = cos(a) <em> cos(b) - sin(a) </em> sin(b)</p>
<p>   因此可以推导出，两个位置向量的点积是一个与他们两个位置差值（即相对位置）有关，而与绝对位置无关。这个性质使得在计算注意力权重的时候(两个向量做点积)，使得相对位置对注意力发生影响，而不是绝对位置，这更符合常理。</p>
</li>
</ul>
<p>10.<strong>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong></p>
<ul>
<li><p>相对位置编码（RPE）</p>
<p>  1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。</p>
<p>  2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置</p>
<p>  3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</p>
</li>
</ul>
<p>11.简单讲一下Transformer中的残差结构以及意义。</p>
<ul>
<li><p>encoder和decoder的self-attention层和ffn层都有残差连接</p>
</li>
<li><p>反向传播的时候不会造成梯度消失。</p>
</li>
<li><p>残差网络与普通网络不同的地方就是引入了跳连接,这可以使上一个残差块的信息没有阻碍的流入到下一个残差块,提高了信息流通,并且也避免了由与网络过深所引起的消失梯度问题和退化问题。</p>
</li>
<li><p>残差链接的方式：</p>
<p>  H(x)=F(x)+x 通道相同，所以采用计算方式为H(x)=F(x)+x</p>
<p>  H(x)=F(x)+x  通道不同，采用的计算方式为H(x)=F(x)+Wx，其中W是卷积操作，用来调整x维度的。</p>
</li>
</ul>
<p>12.为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</p>
<p>layernorm的<script type="math/tex">x_i</script> 是样本内，bn是样本间</p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230512103216297.png" alt="image-20230512103216297" style="zoom:50%;"></p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20230512103229670.png" alt="image-20230512103229670" style="zoom:50%;"></p>
<ul>
<li>LN：针对每个样本序列进行求均值和方差，归一化到正态分布后再缩放到合适的均值和方差，没有样本间的依赖。位置在每个Atention和FFN后。</li>
<li>CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</li>
</ul>
<p>13.简单讲一下BatchNorm技术，以及它的优缺点。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Icanhearwawawa/article/details/123347946">【面试总结】BN的利弊</a></p>
<ul>
<li><p>BN优点：</p>
<ul>
<li>第一个就是可以<strong>解决内部协变量偏移</strong>，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。</li>
<li>BN 层通常设置在激活函数之前。BN层的作用机制是对隐藏层输入的分布进行平滑，缓解随机梯度下降权重更新对后续层的负面影响。对sigmoid和tanh而言，放非线性激活之前，能缓解sigmoid或者 tanh的<strong>梯度衰减</strong>问题，而对ReLU而言，BTTA相关实验结果和代码N放到relu之前，可以防止某一层的激活值全部都被抑制，从而防止从这一层往前传的梯度全都变成０，也就是防止<strong>梯度消失</strong>。（当然也可以防止<strong>梯度爆炸</strong>）</li>
</ul>
</li>
<li><p>BN缺点：</p>
<ul>
<li>第一个，batch_size较小的时候，效果差。</li>
<li>第二个缺点就是 BN 在NLP中效果不好，因为文本的输入长度是动态的。</li>
<li>在测试阶段若出现样本长度超过训练集的最大长度，则无法处理，需要对样本进行截断</li>
<li>测试阶段的均值和方差的计算可能与训练集的相差较大</li>
</ul>
</li>
</ul>
<p>14.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</p>
<p>就是全连接层加残差连接和LN  </p>
<p>使用的激活函数是ReLU()</p>
<p><a target="_blank" rel="noopener" href="https://dandelioncloud.cn/article/details/1473009113708978178/">深度神经网络中常用的激活函数的优缺点分析</a></p>
<p>15.Encoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）</p>
<p><img src="/2023/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/transformer/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/resize,m_fixed,w_1184.webp" alt="面试宝典二：nlp常见知识点随机梯度下降SGD：这个算法的流程就是在每次更新的时候使用一个样本进行梯度下降，所谓的随机二字，就是说我们可以随机用一个样本来表示所有的样本_面试_03"></p>
<ul>
<li>Cross Self-Attention，Decoder提供Q，Encoder提供K，V</li>
<li>Encoder端和Decoder端通过attention机制进行交互，在Encoder端中，每个时间步都会计算一个attention向量，然后将其与输入向量相加得到新的向量；在Decoder端中，除了计算attention向量外还需要计算一个mask向量来避免模型看到未来的信息</li>
</ul>
<p>16.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)</p>
<ul>
<li>Decoder阶段的多头自注意力和encoder的多头自注意力最大的区别就是decoder自注意力需要进行sequence mask来避免模型看到未来信息</li>
</ul>
<p>17.Transformer的并行化体现在哪个地方？Decoder端可以做并行化吗？</p>
<ul>
<li><p>Transformer的并行化我认为主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出。因为单词之间没有依赖关系，不需要先输入前一个单词再输入下一个，可以同时输入所有单词。</p>
</li>
<li><p>Decode引入sequence mask就是为了并行化训练，推理过程不并行。 Transformer的Decoder部分，引入了一种“teacher force”的概念，就是每个时刻的输入不依赖上一时刻的输出，而是依赖之前所有正确的样本，而正确的样本在训练集中已经全部提供了。正是这种“teacher force”的思想，才可以在Transformer的Decoder部分进行并行化计算，</p>
</li>
<li><p>值得注意的一点是：Decoder的并行化仅在训练阶段，在测试阶段，因为我们没有label，所以t时刻的输入必然依赖t-1时刻的输出，这时跟之前的NLP中的序列预测就没什么区别了。</p>
<ul>
<li>NLP中的RNN之所以不能并行化，是因为其是一个马尔可夫过程，即当前状态只与前一个状态有关,而与再之前的所有状态无关。它天生是个时序结构，t时刻依赖t-1时刻的输出，而t-1时刻又依赖t-2时刻，如此循环往前，我们可以说t时刻依赖了前t时刻所有的信息。</li>
</ul>
</li>
</ul>
<p>19.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</p>
<ul>
<li>Transformer训练时学习率会有一个warmup的过程，即先升高再逐渐减小；Dropout一般设置为0.1-0.2之间，并且只在训练时使用，在测试时不需要使用</li>
</ul>
<p>20解码端的残差结构有没有把后续未被看见的mask信息添加进来，造成信息的泄露。</p>
<ul>
<li>解码端的残差结构没有把后续未被看见的mask信息添加进来，因此不会造成信息泄露³。</li>
</ul>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。 </span>
    </div>
</article>







    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2017-2023 Youngyep
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

</body>
<script src="/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 562px;
    }
    .nav.fullscreen {
        margin-left: -562px;
    }
    .nav-left {
        width: 140px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 512px;
        }
        .nav.fullscreen {
            margin-left: -512px;
        }
        .nav-left {
            width: 120px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 512px;
            margin-left: -512px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>







</html>
