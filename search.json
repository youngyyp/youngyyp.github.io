[{"url":"/2023/03/11/深度学习基础/网络训练技巧/","content":"## 冻结模型\n\nhttps://www.zhihu.com/question/311095447/answer/589307812\n\n```python\n# 冻结\nmodel.fc1.weight.requires_grad = False\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.1)\n# \n# compute loss \n# loss.backward()\n# optmizer.step()\n\n# 解冻\nmodel.fc1.weight.requires_grad = True\noptimizer.add_param_group({'params': model.fc1.parameters()})\n```\n\n**终极方法代码实现：**\n\n```python\nfrom collections.abc import Iterable\n\ndef set_freeze_by_names(model, layer_names, freeze=True):\n    if not isinstance(layer_names, Iterable):\n        layer_names = [layer_names]\n    for name, child in model.named_children():\n        if name not in layer_names:\n            continue\n        for param in child.parameters():\n            param.requires_grad = not freeze\n            \ndef freeze_by_names(model, layer_names):\n    set_freeze_by_names(model, layer_names, True)\n\ndef unfreeze_by_names(model, layer_names):\n    set_freeze_by_names(model, layer_names, False)\n\ndef set_freeze_by_idxs(model, idxs, freeze=True):\n    if not isinstance(idxs, Iterable):\n        idxs = [idxs]\n    num_child = len(list(model.children()))\n    idxs = tuple(map(lambda idx: num_child + idx if idx < 0 else idx, idxs))\n    for idx, child in enumerate(model.children()):\n        if idx not in idxs:\n            continue\n        for param in child.parameters():\n            param.requires_grad = not freeze\n            \ndef freeze_by_idxs(model, idxs):\n    set_freeze_by_idxs(model, idxs, True)\n\ndef unfreeze_by_idxs(model, idxs):\n    set_freeze_by_idxs(model, idxs, False)\n```\n\n```python\n# 冻结第一层\nfreeze_by_idxs(model, 0)\n# 冻结第一、二层\nfreeze_by_idxs(model, [0, 1])\n#冻结倒数第一层\nfreeze_by_idxs(model, -1)\n# 解冻第一层\nunfreeze_by_idxs(model, 0)\n# 解冻倒数第一层\nunfreeze_by_idxs(model, -1)\n\n\n# 冻结 em层\nfreeze_by_names(model, 'em')\n# 冻结 fc1, fc3层\nfreeze_by_names(model, ('fc1', 'fc3'))\n# 解冻em, fc1, fc3层\nunfreeze_by_names(model, ('em', 'fc1', 'fc3'))\n```"},{"url":"/2023/03/11/软件使用/其他/","content":"## zotero\n\n#### 数据恢复\n\n在数据目录中，将 zotero.sqlite 重命名为 zotero.sqlite.old，将其中一个原始.bak文件（基于时间戳）重命名为 zotero.sqlite，然后重新启动 Zotero。现在，您应该会看到库的备份版本。\n\n#### 插件\n\nlatex公式https://github.com/windingwind/zotero-better-notes/releases （其实zotero自带latex，但是公式后要多加一个空格，这个插件是一个笔记插件）\n\nzotero-citationcounts：用于从各种来源自动获取引用计数 \n\nzotero-tag ：加标签\n\n\n\n\n\n## windows软链接\n\n[(54条消息) windows创建软链接_六点零六的博客-CSDN博客_windows 软连接](https://blog.csdn.net/m0_51977577/article/details/125416891)\n\nwin + R，输入cmd，不要直接按回车键，按ctrl + shift + enter进入命令操作窗口；（进入管理员模式）\n\n![image-20221111101710227](其他/image-20221111101710227.png)"},{"url":"/2023/03/11/计算机视觉/目标检测/基础知识/","content":"## 参考资料\n\n目标检测究竟发展到了什么程度? | CVHub带你聊一聊目标检测发展的这22年https://mp.weixin.qq.com/s/tZztakvzU9kl0nPbl8oCww\n\n视频学习：https://space.bilibili.com/18161609/channel/seriesdetail?sid=244160\n\n\n\n# 目标检测综述\n\n**Anchor based**方法(一阶段，二阶段)和**Anchor free**方法\n\n![图片](基础知识/640.jpeg)\n\n## 传统算法\n\n### HOG\n\nHOG (Histogram of Oriented Gradients，方向梯度直方图) \n\n1）主要思想：\n\n在一副图像中，局部目标的表象和形状能够被梯度或边缘的方向密度分布很好地描述。其本质为：梯度的统计信息，而梯度主要存在于边缘的地方。\n\nHog+SVM做行人识别\n\n2）实现方法：\n\n把样本图像分割为若干个像素的单元，把梯度方向平均划分为多个区间，在每个单元里面对所有像素的梯度方向在各个方向区间进行直方图统计。最后把这些直方图组合起来，就可以构成特征描述符。\n\n### 传统方法的缺点\n\n1）基于滑动窗口的区域选择策略没有针对性,时间复杂度高,窗口冗余\n\n2）手工设计的特征对于多样性的变化没有很好的鲁棒性\n\n### \n\n## 二阶段anchor based - RCNN系列\n\n二阶段检测算法主要分为以下两个阶段\n**Stage1**：从图像中生成region proposals\n**Stage2**：从region proposals生成最终的物体边框。\n\n[RCNN系列]https://zhuanlan.zhihu.com/p/383167028\n\n### RCNN\n\n![img](基础知识/v2-2ebc675f93f886e008c222880200a274_1440w.webp)\n\nRCNN首先通过选择性搜索算法Selective Search从一组对象候选框中选择可能出现的对象框，然后将这些选择出来的对象框中的图像resize到某一固定尺寸的图像，并喂入到CNN模型和svm模型\n\n### Fast RCNN\n\nFast R-CNN与R-CNN相比主要有以下几点不同：\n\n1. 不再是对每一个候选区域单独提取特征，而是在提取整个图像的特征后，将每一个候选区域映射到特征图上\n2. 在R-CNN中为了统一输入使用了暴力缩放的方法，但在Fast R-CNN中，使用了RoI Pooling\n3. 使用了多任务的损失函数来简化R-CNN中的多阶段训练\n\nFast RCNN不足：\n\n- - 候选区域的选取还是通过selective search，并且只能在CPU中运行这个算法，所以这个阶段浪费了大量时间。（这也是Faster RCNN改进的点）\n\n### Faster RCNN\n\nFaster RCNN就是RPN+Fast RCNN，具体算法步骤如下\n\n1. 输入图像到特征提取器中，得到整张图片的feature map。\n2. 使用RPN生成候选框，并投影到feature map上，得到每一个候选区域的特征矩阵。\n3. 将每一个特征矩阵经过ROI Pooling缩放到7*7大小，然后经过展平处理后通过全连接层获得预测的分类以及候选区域位置偏移信息。\n\nFaster RCNN有三个部分需要训练，分别是特征提取器VGG16，RPN以及RoIHead\n\n![img](基础知识/v2-23115fb3ba6ff0cdd2fc2c51b354bc49_r.jpg)\n\n问：为什么要生成一堆anchor，再对它们进行修正，而不是一开始直接预测anchor的坐标？\n\n答：其实YOLO v1就是没有使用anchor，直接对候选区域的坐标进行预测，但作者发现，效果并不好，主要是因为网络很难收敛，训练难度较大，所以YOLO的作者后来就将Faster RCC的RPN进行了相关的修改，加入到了YOLO v2中，效果有了显著的提高。\n\n\n\n## 一阶段anchor based - YOLO系列\n\n<img src=\"基础知识/4cee39e6039ca5ee032f7c488c4b21f7.png\" alt=\"img\" style=\"zoom:50%;\" />\n\nbackbone https://blog.csdn.net/qq_38668236/article/details/127378254\n\nneck https://blog.csdn.net/qq_38668236/article/details/127400899\n\nhead https://ac.nowcoder.com/discuss/1031260?type=0&order=0&page=1&channel=-1\n\n\n\n通俗易懂的YOLO系列（从V1到V5）模型解读：\n\nhttps://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247579969&idx=3&sn=2c4d437570b83b62bdc268cc94aab3ce&chksm=fb545badcc23d2bb5185f4d5131dac466abc75abbd7022d1910916cc44cb9d1d1417173371cb&scene=27\n\n总结：\n\n**YOLO v1：直接回归位置。**\n\n**YOLO v2：全流程多尺度方法。**\n\n**YOLO v3：多尺度检测头，resblock darknet53**\n\n**YOLO v4：cspdarknet53，spp，panet，tricks**\n\n### YOLO V1\n\nYOLO的作者当时是这么想的：你分类器输出一个one-hot vector，那我把它换成(x,y,w,h,c)，c表示confidence置信度，把问题转化成一个回归问题，直接回归出Bounding Box的位置不就好了吗？\n\nYOLOv1的缺陷：\n\n1. 由于YOLOv1每个网格的检测框只有2个，对于密集型目标检测和小物体检测都不能很好适用。 \n2. Inference时，当同一类物体出现的不常见的长宽比时泛化能力偏弱。 \n3. 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。\n\n### YOLO V2\n\n从**直接预测位置**改为**预测一个偏移量**，基于**Anchor框的宽和高**和**grid的先验位置**的**偏移量**，得到最终目标的位置，这种方法也叫作**location prediction**。\n\n直接预测位置会导致神经网络在一开始训练时不稳定，使用偏移量会使得训练过程更加稳定\n\nYOLO v2先对每个区域得到了5个anchor作为参考，这些anchor是通过GT聚类得到的\n\n### YOLO V3\n\n<img src=\"基础知识/640-167817488587921.jpeg\" alt=\"图片\" style=\"zoom: 80%;\" />\n\n\n\n分出了3个分支，分别为**32倍下采样，16倍下采样，8倍下采样**，分别取预测**大，中，小目标**\n\n### YOLO V4\n\nloss做了很多改进\n\n<img src=\"基础知识/image-20230307154723215.png\" alt=\"image-20230307154723215\" style=\"zoom:50%;\" />\n\nYOLO v4对输入端进行了改进，主要包括**数据增强Mosaic、cmBN、SAT自对抗训练**，使得在卡不是很多时也能取得不错的结果。\n\nYOLO v4具有极大的工程意义，将近年来深度学习领域最新研究的tricks都引入到了YOLO v4做验证测试\n\n### 小结\n\n<img src=\"基础知识/640-167817537029823.jpeg\" alt=\"图片\" style=\"zoom:67%;\" />\n\n为什么一代比一代检测头更加复杂呢？答案是：因为它们的提特征网络更加强大了，能够支撑起检测头做更加复杂的操作\n\n## Anchor-Free 系列\n\n基于Anchor的检测算法由于Anchor太多导致计算复杂，及其所带来的大量超参数都会影响模型性能。近年的Anchor free技术则摒弃Anchor，通过确定关键点的方式来完成检测，大大减少了网络超参数的数量。\n\n **CornerNet**是Anchor free技术路线的开创之作，该网络提出了一种新的对象检测方法，将网络对目标边界框的检测转化为一对关键点的检测(即左上角和右下角)，通过将对象检测为成对的关键点，而无需设计Anchor box作为先验框。\n\n## 基于transformer的目标检测\n\n### DETR\n\nDETR 是 Facebook 团队于 2020 年提出的基于 Transformer 的端到端目标检测，没有非极大值抑制 NMS 后处理步骤、没有 anchor 等先验知识和约束，整个由网络实现端到端的目标检测实现，大大简化了目标检测的 pipeline\n\n\n\n## 目标检测常见指标\n\n![image-20230307102437011](基础知识/image-20230307102437011-167815588021814.png)\n\n![在这里插入图片描述](基础知识/70.png)\n\nprecision高可能会漏检，recall高可能出现很多冗余框\n\n通过调整不同的confidence阈值来构造PR曲线；对于recall相同的点，只取precision最高的。如下：\n\n![image-20230307104915451](基础知识/image-20230307104915451.png)\n\n## 非极大值抑制\n\n<img src=\"基础知识/20201020204029755.png#pic_center\" alt=\"img\" style=\"zoom:50%;\" />\n\n我们可以看到，在图中对狗检测的有很多个框，0.9(a)，0.8(b)，0.7(c)，还有一个对猫的0.9(d)。 NMS极大值抑制就是在不影响猫的框的情况下去除对狗检测的其它得分小的框。 设置阈值th，用a分别对b,c,d,计算IOU值，将IOU值大于th的框都去除。\n\n<img src=\"基础知识/v2-4e9fab45f2fd1a862dbb656296a6ae85_1440w.webp\" alt=\"img\" style=\"zoom:50%;\" />\n\n面试的时候会问这样一个问题：NMS的适用情况是什么？\n\n答：1图多目标检测时用NMS。\n\n\n\n\n\n## 遥感目标检测\n\n近年来，随着遥感图像分辨率的提高，遥感图像目标检测(如飞机、船舶、油罐等的检测)成为研究热点，遥感图像目标检测具有广泛的应用，如军事侦查、灾害救援、城市交通管理等等。\n\n<img src=\"基础知识/640-167817795102025.png\" alt=\"图片\" style=\"zoom:80%;\" />\n\n遥感目标检测任务中存在的难点与挑战可简要概括为如下四点：\n\n1. 遥感图像分辨率巨大\n    如上图8-5(a)所示，由于遥感图像分辨率巨大，因此如何快速准确的检测出遥感目标仍然是一个挑战性的问题。\n\n2. 目标遮挡问题\n    如上图8-5(b)所示，超过50%的目标被云雾所遮挡，因此目标遮挡问题也是遥感图像目标检测所面临的一个挑战。\n\n3. 域适应问题\n    由不同传感器所捕获的遥感图像仍然存在很大差异。\n\n4. 目标小而密集，且任意旋转角\n\n    \n\n不同传感器所捕获的遥感图像可能会出现以下差异：\n\n- [光谱差异：不同传感器的波长范围和波段数量可能不同，导致遥感图像的光谱信息和分辨率有所区别](https://www.zhihu.com/question/60276621)[1](https://www.zhihu.com/question/60276621)[2](https://zhuanlan.zhihu.com/p/428385850)。\n- [空间差异：不同传感器的观测角度和分辨元大小可能不同，导致遥感图像的空间信息和分辨率有所区别](https://zhuanlan.zhihu.com/p/428385850)[2](https://zhuanlan.zhihu.com/p/428385850)[3](https://baike.baidu.com/item/多源遥感/22042704)。\n- [辐射差异：不同传感器的响应特性和校正方法可能不同，导致遥感图像的辐射信息和准确度有所区别](https://zhuanlan.zhihu.com/p/428385850)[2](https://zhuanlan.zhihu.com/p/428385850)。\n\n[因此，在使用多源遥感图像进行分析时，需要进行一定的预处理，如配准、归一化、投影变换等，以消除或减小这些差异对结果的影响](https://www.zhihu.com/question/60276621)[1](https://www.zhihu.com/question/60276621)[2](https://zhuanlan.zhihu.com/p/428385850)。\n\n## 领域自适应的目标检测\n\n任何目标检测算法的训练过程本质上都可以看成数据在独立同分布(i.i.d.)下的似然估计过程，而对于不满足(i.i.d.)的目标检测数据(特别是对于一些现实世界的应用程序)仍然是一个挑战，GAN在领域自适应方面已经显示出 了强大的效果，这对于目标检测技术来讲应该具有很大的促进作用。"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/相关工作分类/","content":"![image-20230209151850229](相关工作分类/image-20230209151850229.png)\n\nDomain Generalization (DG):  source data √     target data ×\n\nDomain Adaptation (DA) ---> Unsupervised DA (UDA) ---> Source-free UDA (SFUDA) ---> Online/Offline SFUDA \n\nonline--数据只见一次---tent\n\noffline--shot\n\n![image-20230209152435394](相关工作分类/image-20230209152435394.png)"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/理解BN/","content":"\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/85bab1e5493a4f74a97b8e7b473ad406.jpeg#pic_center)\n\n- BN和Dropout在训练和测试时的差别\n\n    (https://www.zhihu.com/people/hai-chen-wei)\n\n- https://zhuanlan.zhihu.com/p/61725100\n\n- 我们**在训练和推理过程中BatchNorm有不同的行为**。在训练中，我们记录均值和方差的指数移动平均值，以供以后在推理时使用。其原因是，在训练期间处理批次时，我们可以获得输入随时间变化的均值和方差的更好估计，然后将其用于推理。在推理过程中使用输入批次的平均值和方差将不太准确，因为其大小可能比训练中使用的小得多，大数定律在这里发挥了作用。\n\n- 即model.train()模式下会计算统计参数\n\n- BN在训练时是在每个batch上计算均值和方差来进行归一化，每个batch的样本量都不大，所以每次计算出来的均值和方差就存在差异。预测时一般传入一个样本，所以不存在归一化，其次哪怕是预测一个batch，但batch计算出来的均值和方差是偏离总体样本的，所以通常是通过滑动平均结合训练时所有batch的均值和方差来得到一个总体均值和方差。\n\n- Dropout在训练时会随机丢弃一些神经元，这样会导致输出的结果变小。而预测时往往关闭dropout，保证预测结果的一致性（不关闭dropout可能同一个输入会得到不同的输出，不过输出会服从某一分布。另外有些情况下可以不关闭dropout，比如文本生成下，不关闭会增大输出的多样性）。\n\n[（Pytorch）BatchNorm中的track_running_stats参数_Think@的博客-CSDN博客_track_running_stats](https://blog.csdn.net/qq_40728667/article/details/121865517)"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/思考/","content":"测试时会出现大量样本为无效样本的情况，如何筛选出这些样本？\n\n置信度高的样本，熵值很小，如何利用？    而置信度小的样本又容易导致偏差\n\n\n\n## 最优微调策略\n\n模拟退火算法(Simulated Annealing)是一种元启发式算法，其灵感来自于固体物质的退火过程。它可以在搜索过程中接受一定的劣解，以避免局部最优解，并且可以在搜索过程中逐渐减小搜索空间，以加快搜索速度。\n\n粒子群算法(Particle Swarm Optimization)是一种基于群体智能的优化算法，其灵感来自于鸟群或鱼群的集体行为。在该算法中，优化问题被看作是在搜索空间中的多个粒子的位置和速度的优化问题。每个粒子在搜索过程中通过与群体中其他粒子的交互来逐渐优化其位置和速度，以找到最优解。\n\n遗传算法，模拟退火算法，粒子群算法，神经网络等智能算法的作用？https://www.zhihu.com/question/29762576?sort=created\n\n\n\n## 自动停止策略\n\n#### 最大均值差异\n\n[您好，这是Bing。我可以帮您找到一些最大均值差异的域适应算法和相关论文。根据网络搜索结果](https://blog.csdn.net/a529975125/article/details/81176029)[1](https://blog.csdn.net/a529975125/article/details/81176029)[2](https://www.csdn.net/tags/NtzaEg2sODYzNTUtYmxvZwO0O0OO0O0O.html)[3](https://blog.csdn.net/hy592070616/article/details/122401205)，最大均值差异（Maximum Mean Discrepancy，MMD）是迁移学习，尤其是域适应（Domain Adaptation）中使用最广泛的一种损失函数，主要用来度量两个不同但相关的分布的距离。以下是一些相关论文的标题和摘要：\n\n- A Kernel Method for the Two-Sample-Problem. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf and Alexander Smola. Advances in Neural Information Processing Systems 19 (NIPS 2006). 摘要：我们提出了一种新的非参数假设检验方法，用于判断两组观测是否来自相同的分布。我们的方法基于最大均值差异（MMD）统计量，它是一个简单易计算的核函数下两组样本均值之间的距离。我们证明了在合适选择核函数后，MMD统计量能够检测出任意两个分布之间的差异，并且给出了一个基于矩阵特征值分解和bootstrap方法的显著性检验过程。\n- Domain Adaptation via Transfer Component Analysis. Sinno Jialin Pan, Ivor Wai-Hung Tsang and Qiang Yang. IEEE Transactions on Neural Networks (TNN), Vol. 22(2), pp. 199-210, February 2011. 摘要：域适应（domain adaptation）是迁移学习（transfer learning）中一个重要且具有挑战性的问题。在域适应中，目标域数据通常很少或没有标签信息，而源域数据则有充足的标签信息。因此，在目标域上训练一个有效分类器变得困难。为了解决这个问题，我们提出了一种新颖且有效的机器学习技术——转移成分分析（transfer component analysis，TCA）。TCA试图通过寻找一个特征子空间来减少源域和目标域之间最大均值差异（maximum mean discrepancy），从而实现跨域知识转移。\n- Deep Transfer Learning with Joint Adaptation Networks. Mingsheng Long, Han Zhu and Jianmin Wang. Proceedings of the 34th International Conference on Machine Learning (ICML 2017). 摘要：深度神经网络已经在许多视觉识别任务上取得了显著进展，但它们通常需要大量有标注数据来避免过拟合问题。然而，在许多情况下，收集和标注足够数量和质量的数据是不可行或代价高昂的。因此，在缺少充足有标注数据时利用其他领域或任务中已有数据进行知识转移成为了一种自然而有效地解决方案。本文提出了一种新颖且强大地深度迁移学习方法——联合自适应网络（joint adaptation networks），它能够同时利用特征表示、任务关系和分布对齐三个层面进行深度知识转移，并通过最小化多个领域之间最大均值差异（maximum mean discrepancy）实现端到端地优化\n"},{"url":"/2023/03/11/cpp/动态规划/","content":"## 动态规划基础知识\n\n动态规划，Dynamic Programming，简称DP，如果某一问题有很多重叠子问题，使用动态规划是最有效的。\n\n动规是由前一个状态推导出来的，而贪心是局部直接选最优的\n\n动规五部曲：\n\n1. 确定dp数组（dp table）以及下标的含义\n2. 确定递推公式\n3. dp数组如何初始化\n4. 确定遍历顺序\n5. 举例推导dp数组\n\n## 509. 斐波那契数\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/fibonacci-number/)\n\n斐波那契数，通常用 F(n) 表示，形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是： F(0) = 0，F(1) = 1 F(n) = F(n - 1) + F(n - 2)，其中 n > 1 给你n ，请计算 F(n) 。\n\n我的：\n\n```cpp\nclass Solution {\npublic:\n    int fib(int n) {\n        if(n == 0) return 0;\n        if(n == 1) return 1;\n        return fib(n-1) + fib(n-2);\n    }\n};\n```\n\n题解动规：\n\n```cpp\nclass Solution {\npublic:\n    int fib(int N) {\n        if (N <= 1) return N;\n        vector<int> dp(N + 1);\n        dp[0] = 0;\n        dp[1] = 1;\n        for (int i = 2; i <= N; i++) {\n            dp[i] = dp[i - 1] + dp[i - 2];\n        }\n        return dp[N];\n    }\n};\n```\n\n## 70. 爬楼梯\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/climbing-stairs/)\n\n假设你正在爬楼梯。需要 n 阶你才能到达楼顶。\n\n每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？\n\n注意：给定 n 是一个正整数。\n\n**用递归和回溯都超出时间限制**\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    int climbStairs(int n) {\n        if (n <= 2) return n;\n        vector<int> dp(n+1);\n        dp[1] = 1;\n        dp[2] = 2;\n        for (int i = 3; i <= n; i++) {\n            dp[i] = dp[i - 1] + dp[i - 2];//关键\n        }\n        return dp[n];\n    }\n};\n```\n\n## 746. 使用最小花费爬楼梯\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/min-cost-climbing-stairs/)\n\n给你一个整数数组 `cost` ，其中 `cost[i]` 是从楼梯第 `i` 个台阶向上爬需要支付的费用。一旦你支付此费用，即可选择向上爬一个或者两个台阶。\n\n你可以选择从下标为 `0` 或下标为 `1` 的台阶开始爬楼梯。\n\n请你计算并返回达到楼梯顶部的最低花费。\n\n```cpp\nclass Solution {\npublic:\n    int minCostClimbingStairs(vector<int>& cost) {\n        vector<int> dp(cost.size() + 1);\n        dp[0] = 0; // 默认第一步都是不花费体力的\n        dp[1] = 0;\n        for (int i = 2; i <= cost.size(); i++) {\n            dp[i] = min(dp[i - 1] + cost[i - 1], dp[i - 2] + cost[i - 2]);\n        }\n        return dp[cost.size()];\n    }\n};\n```\n\n# 62.不同路径\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/unique-paths/)\n\n一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。\n\n机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。\n\n问总共有多少条不同的路径？\n\n```cpp\nclass Solution {\npublic:\n    int uniquePaths(int m, int n) {\n        vector<vector<int>> dp(m, vector<int>(n, 0));\n        //初始化，最好吧初始化和逻辑处理分开写，比较明了\n        for (int i = 0; i < m; i++) dp[i][0] = 1;\n        for (int j = 0; j < n; j++) dp[0][j] = 1;\n\n        for(int i = 1; i < m; i++) {\n            for(int j = 1; j < n; j++){\n                dp[i][j] = dp[i-1][j] + dp[i][j-1];\n            }\n        }\n        return dp[m-1][n-1];\n    }\n};\n```\n\n#  63. 不同路径 II\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/unique-paths-ii/)\n\n一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。\n\n机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。\n\n现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    int uniquePathsWithObstacles(vector<vector<int>>& obstacleGrid) {\n        int m = obstacleGrid.size(); //！！记住这里的方式\n        int n = obstacleGrid[0].size();\n        vector<vector<int>> dp(m, vector<int>(n, 0));\n        \n        /* 这块的判断写复杂了\n        int sig = 1;\n        for (int i = 0; i < m; i++) {\n            if(obstacleGrid[i][0] == 1) {\n                dp[i][0] = 0;\n                sig = 0;\n            }\n            if(sig) dp[i][0] = 1;\n        }\n        sig = 1;\n        for (int j = 0; j < n; j++) {\n            if(obstacleGrid[0][j] == 1) {\n                dp[0][j] = 0;\n                sig = 0;\n            }\n            if(sig) dp[0][j] = 1;\n        }\n        */\n        //题解的初始化判断\n        for (int i = 0; i < m && obstacleGrid[i][0] == 0; i++) dp[i][0] = 1;\n        for (int j = 0; j < n && obstacleGrid[0][j] == 0; j++) dp[0][j] = 1;\n        \n        for(int i = 1; i < m; i++) {\n            for(int j = 1; j < n; j++){\n                if(obstacleGrid[i][j] == 1) dp[i][j] = 0;\n                else dp[i][j] = dp[i-1][j] + dp[i][j-1];\n            }\n        }\n        return dp[m-1][n-1];\n    }\n};\n```\n\n## 343. 整数拆分\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/integer-break/)\n\n给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。\n\n思路：遍历，比较`dp[i - j] * j, (i - j) * j`大小，即比较拆成两个和拆成两个以上的乘积大小。\n\n```cpp\nclass Solution {\npublic:\n    int integerBreak(int n) {\n        vector<int> dp(n+1);\n        dp[2] = 1;\n        for(int i = 3; i <= n; i++){\n            for (int j = 2; j < i; j++){\n                dp[i] = max(dp[i],max(dp[i - j] * j, (i - j) * j));\n            }\n        }\n        return dp[n];\n    }\n};\n```\n\n## 96.不同的二叉搜索树\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/unique-binary-search-trees/)\n\n给定一个整数 n，求以 1 ... n 为节点组成的**二叉搜索树**有多少种？\n\n思路：\n\n![image-20230310111953290](动态规划/image-20230310111953290.png) \n\ndp[3] = dp[0]dp[2] + dp[1]dp[1]+dp[2]dp[0];\n\n节点的数值其实关系不大，只要每个值不一样就行；\n\ndp[i] += dp[j - 1] * dp[i - j];\n\n代码：\n\n```cpp\nclass Solution {\npublic:\n    int numTrees(int n) {\n        vector<int> dp(n+1);\n        dp[0] = 1;\n        dp[1] = 1;\n        for(int i = 2; i <= n; i++){\n            for(int j = 1; j <= i; j++){\n                dp[i] += dp[j - 1] * dp[i - j];\n            }\n        }\n        return dp[n];\n    }\n};\n```\n\n## 01背包\n\n有N件物品和⼀个最多能背重量为W 的背包。第i件物品的重量是weight[i]，得到的价值是value[i] 。每件物品只能⽤⼀次，求解将哪些物品装⼊背包⾥物品价值总和最⼤。\n\n例：\n\n背包最⼤重量为4。\n              重量    价值\n物品0      1          15\n物品1      3          20\n物品2      4          30\n\n思路：\n\n对于背包问题，有⼀种写法， 是使⽤**⼆维数组**，即`dp[i] [j]` 表⽰从下标为`[0-i]`的物品⾥任意取，放进容量为`j`的背包，价值总和最⼤是多少。\n\n不放物品i:  `dp[i-1] [j]`\n\n放物品i:` dp[i-1] * [j - weight[ i ]] + value[ i ]`\n\n```cpp\n//测试代码\nvoid test_2_wei_bag_problem1() {\n    vector<int> weight = {1, 3, 4};\n    vector<int> value = {15, 20, 30};\n    int bagweight = 4;\n\n    // 二维数组\n    vector<vector<int>> dp(weight.size(), vector<int>(bagweight + 1, 0));\n\n    // 初始化\n    for (int j = weight[0]; j <= bagweight; j++) {\n        dp[0][j] = value[0];\n    }\n\n    // weight数组的大小 就是物品个数\n    for(int i = 1; i < weight.size(); i++) { // 遍历物品\n        for(int j = 0; j <= bagweight; j++) { // 遍历背包容量\n            if (j < weight[i]) dp[i][j] = dp[i - 1][j];\n            else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]);\n\n        }\n    }\n\n    cout << dp[weight.size() - 1][bagweight] << endl;\n}\n\nint main() {\n    test_2_wei_bag_problem1();\n}\n```"},{"url":"/2023/03/11/cpp/栈与队列/","content":"## 栈与队列理论基础\n\n队列是先进先出，栈是先进后出。\n\n**我们常用的SGI STL，如果没有指定底层实现的话，默认是以deque为缺省情况下栈的底层结构。**\n\ndeque是一个双向队列，只要封住一段，只开通另一端就可以实现栈的逻辑了。\n\n**SGI STL中队列一样是以deque为缺省情况下的底部结构**\n\nSTL 栈和队列都不被归类为容器，而被归类为container adapter（ 容器适配器）。\n\n##  用栈实现队列\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/implement-queue-using-stacks/)\n\n使用栈实现队列的下列操作：\n\npush(x) -- 将一个元素放入队列的尾部。\npop() -- 从队列首部移除元素。\npeek() -- 返回队列首部的元素。\nempty() -- 返回队列是否为空。\n\n示例:\n\n```cpp\nMyQueue queue = new MyQueue();\nqueue.push(1);\nqueue.push(2);\nqueue.peek();  // 返回 1\nqueue.pop();   // 返回 1\nqueue.empty(); // 返回 false\n```\n\n说明:\n\n- 你只能使用标准的栈操作 -- 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。\n- 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。\n- 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作）。\n\n**思路：**\n\n<img src=\"https://code-thinking.cdn.bcebos.com/gifs/232.%E7%94%A8%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97%E7%89%88%E6%9C%AC2.gif\" alt=\"232.用栈实现队列版本2\" style=\"zoom: 67%;\" />\n\n在push数据的时候，只要数据放进输入栈就好，**但在pop的时候，操作就复杂一些，输出栈如果为空，就把进栈数据全部导入进来（注意是全部导入）**，再从出栈弹出数据，如果输出栈不为空，则直接从出栈弹出数据就可以了。\n\n最后如何判断队列为空呢？**如果进栈和出栈都为空的话，说明模拟的队列为空了。**\n\n**代码：**\n\n类的构造函数\n\nhttps://www.runoob.com/cplusplus/cpp-constructor-destructor.html\n\n```cpp\nstack<type> s; // 初始化\ns.push(value); // 入栈\ns.pop(); // 出栈，返回void\ns.empty(); // 判断空否\ns.top(); // 返回栈顶元素\ns.size();  // 返回栈元素个数\n```\n\n在pop()之前要先用top()访问，pop()本身是不返回元素的。这个想法看来还真是有意义的\n\n```cpp\nclass MyQueue {\npublic:\n    stack<int> stIn;\n    stack<int> stOut;\n\n    MyQueue() {\n\n    }\n    \n    void push(int x) {\n        stIn.push(x);\n    }\n    \n    int pop() {\n        if(stOut.empty()){\n            while(!stIn.empty()){\n                stOut.push(stIn.top());\n                stIn.pop();\n            }\n        }\n        int result = stOut.top();\n        stOut.pop();\n        return result;\n    }\n    \n    int peek() {\n        int res = this->pop(); // 直接使用已有的pop函数\n        stOut.push(res); // 因为pop函数弹出了元素res，所以再添加回去\n        return res;\n    }\n    \n    bool empty() {\n        if(stOut.empty()&&stIn.empty()) return true;\n        else return false;\n    }\n};\n\n/**\n * Your MyQueue object will be instantiated and called as such:\n * MyQueue* obj = new MyQueue();\n * obj->push(x);\n * int param_2 = obj->pop();\n * int param_3 = obj->peek();\n * bool param_4 = obj->empty();\n */\n```\n\n##  225. 用队列实现栈\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/implement-stack-using-queues/)\n\n使用单向队列实现栈的下列操作：\n\n- push(x) -- 元素 x 入栈\n- pop() -- 移除栈顶元素\n- top() -- 获取栈顶元素\n- empty() -- 返回栈是否为空\n\n注意:\n\n- 你只能使用队列的基本操作-- 也就是 push to back, peek/pop from front, size, 和 is empty 这些操作是合法的。\n- 你所使用的语言也许不支持队列。 你可以使用 list 或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。\n- 你可以假设所有操作都是有效的（例如, 对一个空的栈不会调用 pop 或者 top 操作）。\n\n**代码：**\n\nqueue操作：\n\n```cpp\nq.front();\t\t//获取队首\nq.back();\t\t//获取队尾\nq.push(x);\t\t//插入元素,x表示要插入的值,什么都行（但是类型必须和定义的相同）\nq.pop();\t\t//将队头弹出,无返回值\nq.size();\t\t//返回队列里有多少个元素\nq.empty();\t\t//如果队列为空，返回true，否则返回false（ 等同于q.size()==0 ）\nq.swap(q2);\t\t//交换q和q2里面的值(q2需要和q是一个类型)\n```\n\n**一个队列在模拟栈弹出元素的时候只要将队列头部的元素（除了最后一个元素外） 重新添加到队列尾部，此时再去弹出元素就是栈的顺序了。**\n\n```cpp\nclass MyStack {\npublic:\n    queue<int> que;\n\n    MyStack() {\n\n    }\n    \n    void push(int x) {\n        que.push(x);\n    }\n    \n    int pop() {\n        int size = que.size();\n        while(size >1){\n            que.push(que.front());\n            que.pop();\n            size--;\n        }\n        int result = que.front();\n        que.pop();\n        return result;\n    }\n    \n    int top() {\n        int result = this->pop();\n        que.push(result);\n        return result;\n    }\n    \n    bool empty() {\n        return que.empty();\n    }\n};\n\n/**\n * Your MyStack object will be instantiated and called as such:\n * MyStack* obj = new MyStack();\n * obj->push(x);\n * int param_2 = obj->pop();\n * int param_3 = obj->top();\n * bool param_4 = obj->empty();\n */\n```\n\n##  20. 有效的括号\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/valid-parentheses/)\n\n给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断字符串是否有效。\n\n有效字符串需满足：\n\n- 左括号必须用相同类型的右括号闭合。\n- 左括号必须以正确的顺序闭合。\n- 注意空字符串可被认为是有效字符串。\n\n示例 1:\n\n- 输入: \"()\"\n- 输出: true\n\n示例 2:\n\n- 输入: \"()[]{}\"\n- 输出: true\n\n示例 3:\n\n- 输入: \"(]\"\n- 输出: false\n\n示例 4:\n\n- 输入: \"([)]\"\n- 输出: false\n\n示例 5:\n\n- 输入: \"{[]}\"\n- 输出: true\n\n\n\n我的代码：\n\n使用单个栈实现\n\n```cpp\nclass Solution {\npublic:\n    bool isValid(string s) {\n        stack<char> t;\n        for(int i = 0; i < s.size(); i++){\n            if(s[i] == '(' or s[i] == '[' or s[i] == '{'){\n                t.push(s[i]);\n            }else if(!t.empty()){\n                if((t.top() == '(' && s[i] == ')') or (t.top() == '[' && s[i] == ']') or (t.top() == '{' && s[i] == '}')) t.pop();\n                else return false;\n            }else return false;\n        }\n        if(t.size() == 0) return true;\n        else return false;\n    }\n};\n```\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    bool isValid(string s) {\n        if (s.size() % 2 != 0) return false; // 如果s的长度为奇数，一定不符合要求\n        stack<char> st;\n        for (int i = 0; i < s.size(); i++) {\n            if (s[i] == '(') st.push(')');\n            else if (s[i] == '{') st.push('}');\n            else if (s[i] == '[') st.push(']');\n            // 第三种情况：遍历字符串匹配的过程中，栈已经为空了，没有匹配的字符了，说明右括号没有找到对应的左括号 return false\n            // 第二种情况：遍历字符串匹配的过程中，发现栈里没有我们要匹配的字符。所以return false\n            else if (st.empty() || st.top() != s[i]) return false;\n            else st.pop(); // st.top() 与 s[i]相等，栈弹出元素\n        }\n        // 第一种情况：此时我们已经遍历完了字符串，但是栈不为空，说明有相应的左括号没有右括号来匹配，所以return false，否则就return true\n        return st.empty();\n    }\n};\n```\n\n\n\n## 1047. 删除字符串中的所有相邻重复项\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/remove-all-adjacent-duplicates-in-string/)\n\n给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。\n\n在 S 上反复执行重复项删除操作，直到无法继续删除。\n\n在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。\n\n示例：\n\n- 输入：\"abbaca\"\n- 输出：\"ca\"\n- 解释：例如，在 \"abbaca\" 中，我们可以删除 \"bb\" 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 \"aaca\"，其中又只有 \"aa\" 可以执行重复项删除操作，所以最后的字符串为 \"ca\"。\n\n提示：\n\n- 1 <= S.length <= 20000\n- S 仅由小写英文字母组成。\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    string removeDuplicates(string s) {\n        stack<char> t;\n        for(int i = 0; i < s.size(); i++){\n            if(t.empty() || s[i] != t.top()) t.push(s[i]);\n            else t.pop();\n        }\n        string result = \"\";\n        while (!t.empty()){\n            result += t.top(); //注意字符串的+操作\n            t.pop();\n        }\n        reverse (result.begin(), result.end()); // 此时字符串需要反转一下\n        return result;\n    }\n};\n\n```\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    string removeDuplicates(string S) {\n        stack<char> st;\n        for (char s : S) {\n            if (st.empty() || s != st.top()) {\n                st.push(s);\n            } else {\n                st.pop(); // s 与 st.top()相等的情况\n            }\n        }\n        string result = \"\";\n        while (!st.empty()) { // 将栈中元素放到result字符串汇总\n            result += st.top();\n            st.pop();\n        }\n        reverse (result.begin(), result.end()); // 此时字符串需要反转一下\n        return result;\n\n    }\n};\n```\n\n还可以拿字符串直接作为栈，这样省去了栈还要转为字符串的操作。\n\n代码如下：\n\n```cpp\nclass Solution {\npublic:\n    string removeDuplicates(string S) {\n        string result;\n        for(char s : S) {\n            if(result.empty() || result.back() != s) {\n                result.push_back(s); //注意字符串操作\n            }\n            else {\n                result.pop_back();//注意字符串操作\n            }\n        }\n        return result;\n    }\n};\n```\n\n## 150. 逆波兰表达式求值\n\n[力扣题目链接](https://leetcode.cn/problems/evaluate-reverse-polish-notation/)\n\n根据 逆波兰表示法，求表达式的值。\n\n有效的运算符包括 + , - , * , / 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。\n\n说明：\n\n整数除法只保留整数部分。 给定逆波兰表达式总是有效的。换句话说，表达式总会得出有效数值且不存在除数为 0 的情况。\n\n示例 1：\n\n- 输入: [\"2\", \"1\", \"+\", \"3\", \" * \"]\n- 输出: 9\n- 解释: 该算式转化为常见的中缀算术表达式为：((2 + 1) * 3) = 9\n\n逆波兰表达式主要有以下两个优点：\n\n- 去掉括号后表达式无歧义，上式即便写成 1 2 + 3 4 + * 也可以依据次序计算出正确结果。\n- 适合用栈操作运算：遇到数字则入栈；遇到运算符则取出栈顶两个数字进行计算，并将结果压入栈中。\n\n\n\n**代码：**\n\nC++字符串转换(stoi；stol；stoul；stoll；stoull；stof；stod；stold)\n\nhttps://blog.csdn.net/weixin_43899069/article/details/110290292?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-4-110290292-blog-114146606.pc_relevant_multi_platform_whitelistv4&spm=1001.2101.3001.4242.3&utm_relevant_index=7\n\n\n\n```cpp\nclass Solution {\npublic:\n    int evalRPN(vector<string>& tokens) {\n        stack<long long> st; \n        for(int i = 0; i < tokens.size(); i++){\n            if (tokens[i] == \"+\" || tokens[i] == \"-\" || tokens[i] == \"*\" || tokens[i] == \"/\") {\n                long long num1 = st.top();\n                st.pop();\n                long long num2 = st.top();\n                st.pop();\n                if (tokens[i] == \"+\") st.push(num2 + num1);\n                if (tokens[i] == \"-\") st.push(num2 - num1);\n                if (tokens[i] == \"*\") st.push(num2 * num1);\n                if (tokens[i] == \"/\") st.push(num2 / num1);\n            }else{\n                st.push(stoll(tokens[i]));\n            }\n            \n        }\n        return st.top();\n    }\n};\n```\n\n## 239. 滑动窗口最大值——单调队列\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/sliding-window-maximum/)\n\n给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。\n\n返回滑动窗口中的最大值。\n\n进阶：\n\n你能在线性时间复杂度内解决此题吗？\n\n<img src=\"栈与队列/image-20230216192400940.png\" alt=\"image-20230216192400940\" style=\"zoom:33%;\" />\n\n**思路：**\n\n<img src=\"https://code-thinking.cdn.bcebos.com/gifs/239.%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%9C%80%E5%A4%A7%E5%80%BC-2.gif\" alt=\"239.滑动窗口最大值-2\" style=\"zoom:67%;\" />\n\n使用**单调队列**:\n\n1. pop(value)：如果窗口移除的元素value等于单调队列的front元素，那么队列弹出元素，否则不用任何操作\n2. push(value)：如果push的元素value大于**back元素**的数值，那么就将队列front的元素弹出，直到push元素的数值小于等于队列入口元素的数值为止\n\n**代码：**\n\n```cpp\nclass Solution {\npublic:\n    class MyQueue{ //单调队列（从大到小），由于需要访问q.back()和pop_back()，需要用双向的\n        public:\n            deque<int> q;\n            void pop(int x){\n                if(!q.empty() && x == q.front()) q.pop_front();\n            }\n        // 判断非空需要放在前面，如果改为x == q.front() && !q.empty()就会报错\n            void push(int x){\n                while(!q.empty() && x > q.back()) {\n                    q.pop_back();\n                }\n                q.push_back(x);\n            }\n            int front(){\n                return q.front();\n            }\n    };\n\n    vector<int> maxSlidingWindow(vector<int>& nums, int k) {\n        MyQueue que;\n        vector<int> results;\n        for(int i = 0; i < k; i++){\n            que.push(nums[i]);\n        }\n        results.push_back(que.front());\n        for(int i = k; i < nums.size(); i++){\n            que.pop(nums[i-k]);\n            que.push(nums[i]);\n            results.push_back(que.front());\n        }\n        return results;\n    }\n};\n```\n\n## 347.前 K 个高频元素——优先级队列\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/top-k-frequent-elements/)\n\n给定一个非空的整数数组，返回其中出现频率前 k 高的元素。\n\n示例 1:\n\n- 输入: nums = [1,1,1,2,2,3], k = 2\n- 输出: [1,2]\n\n思路：\n\n**堆是一棵完全二叉树，树中每个结点的值都不小于（或不大于）其左右孩子的值。** 如果父亲结点是大于等于左右孩子就是大顶堆，小于等于左右孩子就是小顶堆。\n\n使用map（key元素  value频率）和小顶堆\n\n【priority_queue优先队列的基础知识】\n\nhttps://leetcode.cn/problems/top-k-frequent-elements/solutions/1283998/c-xiao-bai-you-hao-you-xian-dui-lie-de-j-53ay/\n\nhttps://blog.csdn.net/qq_43679351/article/details/124825229\n\n```cpp\n// 时间复杂度：O(nlogk)\n// 空间复杂度：O(n)\nclass Solution {\npublic:\n    \n    //小顶堆\n    class mycomparison{\n    public:\n        bool operator()(const pair<int, int>& lhs, const pair<int, int>& rhs) {\n            return lhs.second > rhs.second;\n        }\n    }; //注意这里的分号\n\n    vector<int> topKFrequent(vector<int>& nums, int k) {\n         // 要统计元素出现频率\n        unordered_map<int, int> map; // map<nums[i],对应出现的次数>\n        for (int i = 0; i < nums.size(); i++) {\n            map[nums[i]]++; //key元素  value频率\n        }\n        //上式可以改写为\n        // for (int i : nums) {\n        //    map[i]++; //key元素  value频率\n        //}\n\n        // 对频率排序\n        // 定义一个小顶堆，大小为k\n        priority_queue<pair<int, int>, vector<pair<int, int>>, mycomparison> q;\n\n        // 用固定大小为k的小顶堆，扫描所有频率的数值\n        for(auto& a:map){\n            q.push(a);\n            if(q.size()>k){\n               q.pop(); \n            }\n        }\n\n        // 找出前K个高频元素，因为小顶堆先弹出的是最小的，所以倒序来输出到数组\n        vector<int> result(k);\n        for (int i = k - 1; i >= 0; i--) {\n            result[i] = q.top().first;\n            q.pop();\n        }\n        return result;\n    }\n};\n```\n\n"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/resnet50结构/","content":"ext\next.conv1\next.bn1\next.layer1\next.layer1.0\next.layer1.0.conv1\next.layer1.0.bn1\next.layer1.0.conv2\next.layer1.0.bn2\next.layer1.0.conv3\next.layer1.0.bn3\next.layer1.0.shortcut\next.layer1.0.shortcut.0\next.layer1.0.shortcut.1\next.layer1.1\next.layer1.1.conv1\next.layer1.1.bn1\next.layer1.1.conv2\next.layer1.1.bn2\next.layer1.1.conv3\next.layer1.1.bn3\next.layer1.1.shortcut\next.layer1.2\next.layer1.2.conv1\next.layer1.2.bn1\next.layer1.2.conv2\next.layer1.2.bn2\next.layer1.2.conv3\next.layer1.2.bn3\next.layer1.2.shortcut\next.layer2\next.layer2.0\next.layer2.0.conv1\next.layer2.0.bn1\next.layer2.0.conv2\next.layer2.0.bn2\next.layer2.0.conv3\next.layer2.0.bn3\next.layer2.0.shortcut\next.layer2.0.shortcut.0\next.layer2.0.shortcut.1\next.layer2.1\next.layer2.1.conv1\next.layer2.1.bn1\next.layer2.1.conv2\next.layer2.1.bn2\next.layer2.1.conv3\next.layer2.1.bn3\next.layer2.1.shortcut\next.layer2.2\next.layer2.2.conv1\next.layer2.2.bn1\next.layer2.2.conv2\next.layer2.2.bn2\next.layer2.2.conv3\next.layer2.2.bn3\next.layer2.2.shortcut\next.layer2.3\next.layer2.3.conv1\next.layer2.3.bn1\next.layer2.3.conv2\next.layer2.3.bn2\next.layer2.3.conv3\next.layer2.3.bn3\next.layer2.3.shortcut\next.layer3\next.layer3.0\next.layer3.0.conv1\next.layer3.0.bn1\next.layer3.0.conv2\next.layer3.0.bn2\next.layer3.0.conv3\next.layer3.0.bn3\next.layer3.0.shortcut\next.layer3.0.shortcut.0\next.layer3.0.shortcut.1\next.layer3.1\next.layer3.1.conv1\next.layer3.1.bn1\next.layer3.1.conv2\next.layer3.1.bn2\next.layer3.1.conv3\next.layer3.1.bn3\next.layer3.1.shortcut\next.layer3.2\next.layer3.2.conv1\next.layer3.2.bn1\next.layer3.2.conv2\next.layer3.2.bn2\next.layer3.2.conv3\next.layer3.2.bn3\next.layer3.2.shortcut\next.layer3.3\next.layer3.3.conv1\next.layer3.3.bn1\next.layer3.3.conv2\next.layer3.3.bn2\next.layer3.3.conv3\next.layer3.3.bn3\next.layer3.3.shortcut\next.layer3.4\next.layer3.4.conv1\next.layer3.4.bn1\next.layer3.4.conv2\next.layer3.4.bn2\next.layer3.4.conv3\next.layer3.4.bn3\next.layer3.4.shortcut\next.layer3.5\next.layer3.5.conv1\next.layer3.5.bn1\next.layer3.5.conv2\next.layer3.5.bn2\next.layer3.5.conv3\next.layer3.5.bn3\next.layer3.5.shortcut\next.layer4\next.layer4.0\next.layer4.0.conv1\next.layer4.0.bn1\next.layer4.0.conv2\next.layer4.0.bn2\next.layer4.0.conv3\next.layer4.0.bn3\next.layer4.0.shortcut\next.layer4.0.shortcut.0\next.layer4.0.shortcut.1\next.layer4.1\next.layer4.1.conv1\next.layer4.1.bn1\next.layer4.1.conv2\next.layer4.1.bn2\next.layer4.1.conv3\next.layer4.1.bn3\next.layer4.1.shortcut\next.layer4.2\next.layer4.2.conv1\next.layer4.2.bn1\next.layer4.2.conv2\next.layer4.2.bn2\next.layer4.2.conv3\next.layer4.2.bn3\next.layer4.2.shortcut\next.avgpool\nhead\nhead.fc\n\n\n\n50(没有包括shortcut)\n\n\n\nmodel for adaptation: ExtractorHead(\n  (ext): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (shortcut): Sequential()\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (head): LinearClassifier(\n    (fc): Linear(in_features=2048, out_features=10, bias=True)\n  )\n)"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/resnet26结构/","content":"module.conv1\nmodule.bn1\nmodule.layer1\nmodule.layer1.0\nmodule.layer1.0.conv1\nmodule.layer1.0.bn1\nmodule.layer1.0.conv2\nmodule.layer1.0.bn2\nmodule.layer1.0.shortcut\nmodule.layer1.1\nmodule.layer1.1.conv1\nmodule.layer1.1.bn1\nmodule.layer1.1.conv2\nmodule.layer1.1.bn2\nmodule.layer1.1.shortcut\nmodule.layer1.2\nmodule.layer1.2.conv1\nmodule.layer1.2.bn1\nmodule.layer1.2.conv2\nmodule.layer1.2.bn2\nmodule.layer1.2.shortcut\nmodule.layer1.3\nmodule.layer1.3.conv1\nmodule.layer1.3.bn1\nmodule.layer1.3.conv2\nmodule.layer1.3.bn2\nmodule.layer1.3.shortcut\nmodule.layer2\nmodule.layer2.0\nmodule.layer2.0.conv1\nmodule.layer2.0.bn1\nmodule.layer2.0.conv2\nmodule.layer2.0.bn2\nmodule.layer2.0.shortcut\nmodule.layer2.1\nmodule.layer2.1.conv1\nmodule.layer2.1.bn1\nmodule.layer2.1.conv2\nmodule.layer2.1.bn2\nmodule.layer2.1.shortcut\nmodule.layer2.2\nmodule.layer2.2.conv1\nmodule.layer2.2.bn1\nmodule.layer2.2.conv2\nmodule.layer2.2.bn2\nmodule.layer2.2.shortcut\nmodule.layer2.3\nmodule.layer2.3.conv1\nmodule.layer2.3.bn1\nmodule.layer2.3.conv2\nmodule.layer2.3.bn2\nmodule.layer2.3.shortcut\nmodule.layer3\nmodule.layer3.0\nmodule.layer3.0.conv1\nmodule.layer3.0.bn1\nmodule.layer3.0.conv2\nmodule.layer3.0.bn2\nmodule.layer3.0.shortcut\nmodule.layer3.1\nmodule.layer3.1.conv1\nmodule.layer3.1.bn1\nmodule.layer3.1.conv2\nmodule.layer3.1.bn2\nmodule.layer3.1.shortcut\nmodule.layer3.2\nmodule.layer3.2.conv1\nmodule.layer3.2.bn1\nmodule.layer3.2.conv2\nmodule.layer3.2.bn2\nmodule.layer3.2.shortcut\nmodule.layer3.3\nmodule.layer3.3.conv1\nmodule.layer3.3.bn1\nmodule.layer3.3.conv2\nmodule.layer3.3.bn2\nmodule.layer3.3.shortcut\nmodule.linear\n\n\n\n\n\n```\n\n(module): ResNet(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): LambdaLayer()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): LambdaLayer()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=64, out_features=10, bias=True)\n)\n```\n"},{"url":"/2023/03/11/cpp/链表/","content":"## 链表理论基础\n\n![链表1](链表/20200806194529815.png)\n\n```cpp\n// 单链表\nstruct ListNode {\n    int val;  // 节点上存储的元素\n    ListNode *next;  // 指向下一个节点的指针\n    ListNode(int x) : val(x), next(NULL) {}  // 节点的构造函数，val初始化为x，next初始化为NULL\n};\n```\n\n![image-20221218173554982](链表/image-20221218173554982.png)\n\n\n\n删除节点和添加节点：\n\n![链表-删除节点](链表/20200806195114541.png)\n\n![链表-添加节点](链表/20200806195134331.png)\n\n![链表-链表与数据性能对比](链表/20200806195200276.png)\n\n使用C，C++编程语言的话，不要忘了还要从**内存中删除**被移除的节点\n\n\n\n## 移除链表元素\n\n为了使用一种统一的逻辑来移除链表的节点，我们可以设置**虚拟头节点**\n\n最后在题目中，return 头结点的时候，需要 `return dummyNode->next;`\n\n### 新建虚拟头节点：\n\n```cpp\nListNode* dummyHead = new ListNode(-1);\n```\n\n注：当需要避免对头节点的情况进行特殊处理时，采用虚拟头节点\n\n[(63条消息) C++之new的使用_软硬兼施的程序员的博客-CSDN博客_c++ new](https://blog.csdn.net/qq_54182926/article/details/119193250)\n\n一、new的使用。\nnew+数据类型（初值），返回值为申请空间的对应数据类型的地址。\n1.使用new申请一个对象\n\n```cpp\nint *p = new int(10);//申请了一个初值为10的整型数据\n```\n\n2.使用new申请数组\n\n```cpp\nint *arr = new int[10];//申请了能存放10个整型数据元素的数组，其首地址为arr\n\n```\n\n二、delete运算符的使用。\nnew运算符通常搭配delete元素安抚来使用，new用来动态申请存储空间，delete用于释放new申请的空间。\n语法格式如下：\n\n```cpp\ndelete p；\ndelete[] arr;//注意要删除数组时，需要加[]，以表示arr为数组。\n```\n\n## 翻转链表\n\n双指针法\n\n![img](链表/008eGmZEly1gnrf1oboupg30gy0c44qp.gif)\n\n## 删除链表的倒数第N个节点\n\n双指针法：fast先走n+1步，slow再和fast同时走。注意使用虚拟头节点\n\n<img src=\"https://code-thinking.cdn.bcebos.com/pics/19.%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E5%80%92%E6%95%B0%E7%AC%ACN%E4%B8%AA%E8%8A%82%E7%82%B93.png\" alt=\"img\" style=\"zoom:50%;\" />\n\n\n\n## 链表相交\n\n思路：相交部分长度相同，交点处指针相等。即先将长度对齐，再比较指针是否相同。\n\n注：**判断链表结束是看是否指向NULL，**而不是0。没有使用虚拟头节点的话，判断如下`while(curA != NULL)`,不需要用curA->next\n\n\n\n## 环形链表\n\n题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。\n\n**判断环内相遇**：双指针，慢指针每走一步，快指针走两步，如果两指针相遇，则表明有环。\n\n![141.环形链表](链表/141.环形链表.gif)\n\n**找到环的入口：**（快指针可能走N圈才会和慢的相遇）\n\n![142.环形链表II（求入口）](链表/142.环形链表II（求入口）.gif)\n\n**从头结点出发一个指针，从相遇节点 也出发一个指针，这两个指针每次只走一个节点， 那么当这两个指针相遇的时候就是 环形入口的节点**。\n\n\n\n\n\n"},{"url":"/2023/03/11/cpp/数组/","content":"## 二分查找\n\n- 坚持**左闭右闭or左闭又开**\n\t- while(left <= right)  or  while(left < right)\n- 比较left、middle和right三个值\n\n## 移除元素\n\n数组的元素在内存地址中是连续的，不能单独删除数组中的某个元素，只能覆盖。\n\n**双指针法（快慢指针法**）：通过一个快指针和慢指针在一个for循环下完成两个for循环的工作\n\n- 快指针：寻找不等于有目标元素的值 （通常快指针就是for循环里面的索引）\n- 慢指针：指向新数组最后一个下标的位置\n\n```cpp\n        for (int fast=0;fast<nums.size();fast++){\n            if (nums[fast] != val){\n                nums[slow] = nums[fast];\n                slow++;\n            }\n        }\n```\n\n## 长度最小的子数组\n\n>给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。\n>示例：\n>输入：s = 7, nums = [2,3,1,2,4,3] 输出：2 解释：子数组 [4,3] 是该条件下的长度最小的子数组\n\n**滑动窗口**：不断的调节子序列的起始位置和终止位置。其实算是双指针法的一种，一个指向滑动窗口起始位置，一个指向末端。\n\n滑动窗口主要确定如下三点：\n\n- 窗口内是什么？\n- 如何移动窗口的起始位置？\n- 如何移动窗口的结束位置？\n\n![209.长度最小的子数组](数组/209.长度最小的子数组.gif)\n"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/实验/","content":"## 数据集\n\nCIFAR10-to-CIFAR10C\n\nCIFAR100-to-CIFAR100C\n\nImageNet-to-ImageNetC \n\n\n\n [VisDA](https://github.com/VisionLearningGroup/taskcv-2017-public/tree/master/classification) and [Office-Home](https://www.hemanthdv.org/officeHomeDataset.html)\n\n\n\n语义分割数据集\n\nCityscapses-to-ACDC\n\n\n\n## 模型\n\n可以直接使用robustbench的预训练模型\n\n\n\n\n\n## 对比方法\n\nBN \n\nTENT\n\nSHOT\n\nAdaContrast\n\nCoTTA\n\n\n\nieee signal processing letter\n\n## 实验\n\n### 离线非连续目标域适应实验 （无源无监督域适应 source-free unsupervised domain adaptation）\n\n- 重置模型\n- 还可添加与SHOT对比/结合的实验\n\n![image-20230209144734581](实验/image-20230209144734581.png)\n\n\n\n子空间的mean_entropy下降更慢，error更低\n\n\n\n<img src=\"实验/image-20230209191212551.png\" alt=\"image-20230209191212551\" style=\"zoom:50%;\" /><img src=\"实验/image-20230209191239040.png\" alt=\"image-20230209191239040\" style=\"zoom:50%;\" />\n\n## 离线非连续目标域适应实验\n\n\n\n\n\n### 在线连续目标域适应实验\n\n- 参考cotta论文设置\n- 不重置模型\n\n\t\n\n![image-20230209144535655](实验/image-20230209144535655.png)\n\n![image-20230209144553554](实验/image-20230209144553554.png)\n\n\n\n### 消融实验和分析\n\n\n\n\n\n\n\n\n\n#### t-SNE可视化\n\n例：![image-20230209145917834](实验/image-20230209145917834.png)\n\n\n\n\n\n## 测试仅微调前几层和后几层的效果\n\n### resnet-26(conv + all bn statistics)\n\n![image-20230214161528738](实验/image-20230214161528738.png)\n\n**error rate的变化特点**\n\n仅微调前几层（输入层）的时候，error 一直呈现下降趋势；而仅微调后几层（输出层）时，error 会先下降后上升,很易过拟合\n\n以下结果lr均为0.001\n\n微调第一层时的结果\n\n<img src=\"实验/image-20230217171207376.png\" alt=\"image-20230217171207376\" style=\"zoom:50%;\" />\n\n微调前三层时的结果\n\n<img src=\"实验/image-20230217171240694.png\" alt=\"image-20230217171240694\" style=\"zoom:50%;\" />\n\n微调前九层时的结果\n\n<img src=\"实验/image-20230217171310725.png\" alt=\"image-20230217171310725\" style=\"zoom:50%;\" />\n\n微调最后一层时的结果 \n\n<img src=\"实验/image-20230217171032071.png\" alt=\"image-20230217171032071\" style=\"zoom:50%;\" />\n\n微调后三层时的结果\n\n<img src=\"实验/image-20230217171102632.png\" alt=\"image-20230217171102632\" style=\"zoom:50%;\" />\n\n微调后九层时的结果\n\n<img src=\"实验/image-20230217171137404.png\" alt=\"image-20230217171137404\" style=\"zoom:50%;\" />\n\n微调所有层的结果 \n\n<img src=\"实验/image-20230217171344470.png\" alt=\"image-20230217171344470\" style=\"zoom:50%;\" />\n\ntent\n\n<img src=\"实验/image-20230217170947606.png\" alt=\"image-20230217170947606\" style=\"zoom:50%;\" />\n\nldme\n\n<img src=\"实验/image-20230217171413288.png\" alt=\"image-20230217171413288\" style=\"zoom:50%;\" />\n\n### resnet-26(bn)\n\n![image-20230306100121325](实验/image-20230306100121325.png)\n\n\n\n1\n\n<img src=\"实验/image-20230306135337210.png\" alt=\"image-20230306135337210\" style=\"zoom:50%;\" />\n\n<img src=\"实验/image-20230306135404338.png\" alt=\"image-20230306135404338\" style=\"zoom:50%;\" />\n\n9\n\n<img src=\"实验/image-20230306135622519.png\" alt=\"image-20230306135622519\" style=\"zoom:50%;\" />\n\n17\n\n<img src=\"实验/image-20230306140607421.png\" alt=\"image-20230306140607421\" style=\"zoom:50%;\" />\n\n\n\n23\n\n<img src=\"实验/image-20230306140542547.png\" alt=\"image-20230306140542547\" style=\"zoom:50%;\" />\n\n25(tent)\n\n<img src=\"实验/image-20230306140722961.png\" alt=\"image-20230306140722961\" style=\"zoom:50%;\" />\n\n\n\n单独分析一个域的情况：\n\nimpulse_noise:\n\n只有更新所有BN层时，error才下降，且此时entropy下降缓慢\n\n<img src=\"实验/image-20230306141814141.png\" alt=\"image-20230306141814141\" style=\"zoom:50%;\" />\n\n<img src=\"实验/image-20230306141752113.png\" alt=\"image-20230306141752113\" style=\"zoom:50%;\" />\n\nelastic_transform:\n\n当解冻的BN层越多，entropy下降的越快\n\n<img src=\"实验/image-20230306142548988.png\" alt=\"image-20230306142548988\" style=\"zoom:50%;\" />\n\n<img src=\"实验/image-20230306142630463.png\" alt=\"image-20230306142630463\" style=\"zoom:50%;\" />\n\n\n\n### resnet-26(bn+conv)\n\n![image-20230306100151951](实验/image-20230306100151951.png)\n\n\n\n1\n\n<img src=\"实验/image-20230306104530197.png\" alt=\"image-20230306104530197\" style=\"zoom:50%;\" />\n\n<img src=\"实验/image-20230306104643454.png\" alt=\"image-20230306104643454\" style=\"zoom:50%;\" />\n\n误差持续上升的这些域，熵下降的更迅速，tent在这些域对应的误差也较大；可以解释为分布偏差较大的域，仅更新前几层网络可能会导致网络崩溃，越训越差。\n\n\n\n3（与1的情况相似）\n\n<img src=\"实验/image-20230306105559189.png\" alt=\"image-20230306105559189\" style=\"zoom:50%;\" />\n\n\n\n7（误差上升的域数同1，为6个域）\n\n<img src=\"实验/image-20230306105735052.png\" alt=\"image-20230306105735052\" style=\"zoom:50%;\" />\n\n在误差下降的域，误差值甚至低于tent\n\n<img src=\"实验/image-20230306110018402.png\" alt=\"image-20230306110018402\" style=\"zoom:50%;\" />\n\n\n\n9（误差上升的类别数为2个域，这两个域在tent中的误差都是最高的，接近40%）\n\n<img src=\"实验/image-20230306110306474.png\" alt=\"image-20230306110306474\" style=\"zoom:50%;\" />\n\n<img src=\"实验/image-20230306110505956.png\" alt=\"image-20230306110505956\" style=\"zoom:50%;\" />\n\n17（误差上升的域为8个，其中3个直接上升，另5个先下降后上升）\n\n<img src=\"实验/image-20230306110753164.png\" alt=\"image-20230306110753164\" style=\"zoom:50%;\" />\n\n<img src=\"实验/image-20230306111053139.png\" alt=\"image-20230306111053139\" style=\"zoom:50%;\" />\n\n\n\n23\n\n<img src=\"实验/image-20230306111528991.png\" alt=\"image-20230306111528991\" style=\"zoom:50%;\" />\n\n\n\n25（这里调整完了所有的BN层，误差全部下降）\n\n<img src=\"实验/image-20230306111400245.png\" alt=\"image-20230306111400245\" style=\"zoom:50%;\" />\n\n\n\n### resnet-26(allbn+conv)\n\n![image-20230306100227413](实验/image-20230306100227413.png)\n\n0(tent)\n\n<img src=\"实验/image-20230306103133212.png\" alt=\"image-20230306103133212\" style=\"zoom:50%;\" />\n\n\n\n1\n\n<img src=\"实验/image-20230306103012687.png\" alt=\"image-20230306103012687\" style=\"zoom:50%;\" />\n\n\n\n9\n\n<img src=\"实验/image-20230306102713434.png\" alt=\"image-20230306102713434\" style=\"zoom:50%;\" />\n\n\n\n19\n\n<img src=\"实验/image-20230306102555685.png\" alt=\"image-20230306102555685\" style=\"zoom:50%;\" />\n\n\n\nall\n\n<img src=\"实验/image-20230306102358797.png\" alt=\"image-20230306102358797\" style=\"zoom:50%;\" />\n\n\n\n单个域性能分析：\n\ngaussian（熵严格按照解冻层数越多下降越快）\n\n<img src=\"实验/image-20230306154146818.png\" alt=\"image-20230306154146818\" style=\"zoom:50%;\" />\n\n\n\n\n\n\n\n### resnet-50（复现ttt++）\n\nlr = 0.001\n\n\n\nlayer1\n\n<img src=\"实验/image-20230217104033394.png\" alt=\"image-20230217104033394\" style=\"zoom:50%;\" />\n\n\n\nlayer4\n\n<img src=\"实验/image-20230217104332979.png\" alt=\"image-20230217104332979\" style=\"zoom:50%;\" />\n\nlayer11\n\n<img src=\"实验/image-20230217104210696.png\" alt=\"image-20230217104210696\" style=\"zoom:50%;\" />\n\nlayer-11\n\n<img src=\"实验/image-20230217104051595.png\" alt=\"image-20230217104051595\" style=\"zoom:50%;\" />\n\nlayer-4(0.01)\n\n<img src=\"实验/image-20230217165904272.png\" alt=\"image-20230217165904272\" style=\"zoom:50%;\" />\n\nlayer-1\n\n<img src=\"实验/image-20230217104425260.png\" alt=\"image-20230217104425260\" style=\"zoom:50%;\" />\n\n\n\nall\n\n<img src=\"实验/image-20230217165803214.png\" alt=\"image-20230217165803214\" style=\"zoom:50%;\" />\n\n \n\ntent\n\n<img src=\"实验/image-20230217104504987.png\" alt=\"image-20230217104504987\" style=\"zoom:50%;\" />\n\nldme\n\n<img src=\"实验/image-20230217104609914.png\" alt=\"image-20230217104609914\" style=\"zoom:50%;\" />\n\n"},{"url":"/2023/03/11/cpp/回溯/","content":"## 回溯算法理论基础\n\n回溯是递归的副产品，只要有递归就会有回溯。在下文中，**回溯函数也就是递归函数，指的都是一个函数**\n\n**回溯法解决的问题都可以抽象为树形结构**，因为回溯法解决的都是在集合中递归查找子集，**集合的大小就构成了树的宽度，递归的深度，都构成的树的深度**。\n\n回溯三部曲：\n\n```cpp\nvoid backtracking(参数){\n    if (终止条件) {\n        存放结果;\n        return;\n    }\n    \n    for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) {\n        处理节点;\n        backtracking(路径，选择列表); // 递归\n        回溯，撤销处理结果\n    }\n}\n\n```\n\n## 第77题. 组合\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/combinations/)\n\n给定两个整数 n 和 k，返回 1 ... n 中所有可能的 k 个数的组合。\n\n示例:\n输入: n = 4, k = 2\n输出:\n[\n[2,4],\n[3,4],\n[2,3],\n[1,2],\n[1,3],\n[1,4],\n]\n\n思路：\n\n把组合问题抽象为如下树形结构：\n\n<img src=\"回溯/20201123195223940-16777567027868.png\" alt=\"77.组合\" style=\"zoom:33%;\" />\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n\n    vector<vector<int>> result; // 存放符合条件结果的集合\n    vector<int> path; // 用来存放符合条件结果\n    void backtracking(int n, int k, int startIndex) {\n        if(path.size() == k){\n            result.push_back(path);\n            return;\n        }\n        for(int i = startIndex; i <=n; i++){ //注意此处是<=\n            path.push_back(i);\n            backtracking(n, k, i + 1);\n            path.pop_back();\n        }\n\n    }\n\n    vector<vector<int>> combine(int n, int k) {\n        backtracking(n, k, 1);\n        return result;\n    }\n};\n```\n\n剪枝优化题解：\n\n```cpp\nclass Solution {\npublic:\n\n    vector<vector<int>> result; // 存放符合条件结果的集合\n    vector<int> path; // 用来存放符合条件结果\n    void backtracking(int n, int k, int startIndex) {\n        if(path.size() == k){\n            result.push_back(path);\n            return;\n        }\n        // n-i+1 >= k-path.size() 大多数回溯算法的剪枝操作，都是改这里i的范围\n        for(int i = startIndex; i <= n + 1 - k + path.size(); i++){ //注意此处是<=\n            path.push_back(i);\n            backtracking(n, k, i + 1);\n            path.pop_back();\n        }\n\n    }\n\n    vector<vector<int>> combine(int n, int k) {\n        backtracking(n, k, 1);\n        return result;\n    }\n};\n```\n\n## 216.组合总和III\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/combination-sum-iii/)\n\n找出所有相加之和为 n 的 k 个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。\n\n说明：\n\n- 所有数字都是正整数。\n- 解集不能包含重复的组合。\n\n示例 1: 输入: k = 3, n = 7 输出: [[1,2,4]]\n\n示例 2: 输入: k = 3, n = 9 输出: [[1,2,6], [1,3,5], [2,3,4]]\n\n思路：\n\nk是树的深度，n是树的宽度\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking(int targetSum, int k, int sum, int startIndex){\n        if(path.size() == k){\n            if(sum == targetSum) result.push_back(path);\n            return;\n        }\n        for(int i = startIndex; i <= 9; i++){\n            path.push_back(i);\n            sum = sum + i;\n            backtracking(targetSum, k, sum, i+1);\n            sum = sum - i;\n            path.pop_back();\n        }\n    }\n\n    vector<vector<int>> combinationSum3(int k, int n) {\n        backtracking(n, k, 0, 1);\n        return result;\n    }\n};\n```\n\n剪枝后：\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking(int targetSum, int k, int sum, int startIndex){\n        if(sum > targetSum) return;\n        if(path.size() == k){\n            if(sum == targetSum) result.push_back(path);\n            return;\n        }\n        for(int i = startIndex; i <= 9 + 1 - k + path.size(); i++){\n            path.push_back(i);\n            sum = sum + i;\n            backtracking(targetSum, k, sum, i+1);\n            sum = sum - i;\n            path.pop_back();\n        }\n    }\n\n    vector<vector<int>> combinationSum3(int k, int n) {\n        backtracking(n, k, 0, 1);\n        return result;\n    }\n};\n```\n\n## 17.电话号码的字母组合\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/letter-combinations-of-a-phone-number/)\n\n给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。\n\n![17.电话号码的字母组合](回溯/2020102916424043.png)\n\n思路：\n\n![17. 电话号码的字母组合](回溯/20201123200304469.png)\n\n代码：\n\n```cpp\nclass Solution {\npublic:\n    const string letterMap[10] = {\n        \"\", // 0\n        \"\", // 1\n        \"abc\", // 2\n        \"def\", // 3\n        \"ghi\", // 4\n        \"jkl\", // 5\n        \"mno\", // 6\n        \"pqrs\", // 7\n        \"tuv\", // 8\n        \"wxyz\", // 9\n    };\n\n    vector<string> result;\n    string s;\n    void backtracking(string digits, int index){\n        if (index == digits.size()) {\n            result.push_back(s);\n            return;\n        }\n        \n        int digit = digits[index] - '0'; // 将index指向的数字转为int\n        string letters = letterMap[digit];\n        for (int i = 0; i < letters.size(); i++) {\n            s.push_back(letters[i]);\n            backtracking(digits,index + 1); // 递归\n            s.pop_back();\n        }\n    }\n\n    vector<string> letterCombinations(string digits) {\n        if(digits == \"\") return result;\n        backtracking(digits, 0);\n        return result;\n    }\n};\n```\n\n## 39. 组合总和\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/combination-sum/)\n\n给定一个**无重复元素**的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。\n\n`candidates` 中的 **同一个** 数字可以 **无限制重复被选取**\n\n![39.组合总和](回溯/20201223170730367.png)\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> results;\n    vector<int> path;\n    void backtracking(vector<int> candidates, int target, int index, int sum){\n        if(sum == target){\n            results.push_back(path);\n            return;\n        }\n        if(sum > target){\n            return;\n        }\n        for (int i = index; i < candidates.size(); i++) {\n            path.push_back(candidates[i]);\n            backtracking(candidates, target, i, sum + candidates[i]); // 递归\n            path.pop_back();\n        }\n    }\n\n    vector<vector<int>> combinationSum(vector<int>& candidates, int target) {\n        backtracking(candidates, target, 0, 0);\n        return  results;\n    }\n};\n```\n\n## 40.组合总和II\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/combination-sum-ii/)\n\n给定一个数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。\n\ncandidates 中的每个数字在每个组合中只能使用一次。\n\n注意：数组candidates中会出现重复数字，需要**排序后去重**\n\n```cpp\nclass Solution {\nprivate:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking(vector<int>& candidates, int target, int sum, int startIndex) {\n        if (sum == target) {\n            result.push_back(path);\n            return;\n        }\n        if(sum > target){\n            return;\n        }\n        for (int i = startIndex; i < candidates.size(); i++) {\n            // 要对同一树层使用过的元素进行跳过，这是本题的关键\n            if (i > startIndex && candidates[i] == candidates[i - 1]) {\n                continue;\n            }\n            path.push_back(candidates[i]);\n            backtracking(candidates, target, sum + candidates[i], i + 1); // 和39.组合总和的区别1，这里是i+1，每个数字在每个组合中只能使用一次\n            path.pop_back();\n        }\n    }\n\npublic:\n    vector<vector<int>> combinationSum2(vector<int>& candidates, int target) {\n        path.clear();\n        result.clear();\n        // 首先把给candidates排序，让其相同的元素都挨在一起。\n        sort(candidates.begin(), candidates.end());\n        backtracking(candidates, target, 0, 0);\n        return result;\n    }\n};\n```\n\n##  131.分割回文串\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/palindrome-partitioning/)\n\n给定一个字符串 s，将 s 分割成一些子串，使每个子串都是回文串。\n\n返回 s 所有可能的分割方案。\n\n示例: 输入: \"aab\" 输出: [ [\"aa\",\"b\"], [\"a\",\"a\",\"b\"] ]\n\n```cpp\nclass Solution {\npublic:\n    // 用双指针法，一个指针从前向后，一个指针从后向前，如果前后指针所指向的元素是相等的，就是回文字符串\n    bool isPalindrome(const string& s, int start, int end) {\n        for (int i = start, j = end; i < j; i++, j--) {\n            if (s[i] != s[j]) {\n                return false;\n            }\n        }\n        return true;\n    }\n    vector<vector<string>> result;\n    vector<string> path; // 放已经回文的子串\n    void backtracking (const string& s, int startIndex) {\n        if (startIndex >= s.size()) {\n            result.push_back(path);\n            return;\n        }\n        \n        for (int i = startIndex; i < s.size(); i++) {\n            if (isPalindrome(s, startIndex, i)) { // 是回文子串\n                // 获取[startIndex,i]在s中的子串\n                string str = s.substr(startIndex, i - startIndex + 1);\n                path.push_back(str);\n            } else {                // 如果不是则直接跳过\n                continue;\n            }\n            backtracking(s, i + 1); // 寻找i+1为起始位置的子串\n            path.pop_back();        // 回溯过程，弹出本次已经填在的子串\n        }\n    }\n\n    vector<vector<string>> partition(string s) {\n        backtracking(s, 0);\n        return result;\n    }\n};\n```\n\n\n\n## 93.复原IP地址\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/restore-ip-addresses/)\n\n给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。\n\n有效的 IP 地址 正好由四个整数（每个整数位于 0 到 255 之间组成，且不能含有前导 0），整数之间用 '.' 分隔。\n\n示例 1：\n\n- 输入：s = \"25525511135\"\n- 输出：[\"255.255.11.135\",\"255.255.111.35\"]\n\n```cpp\nclass Solution {\npublic:\n    // 判断字符串s在左闭又闭区间[start, end]所组成的数字是否合法\n    bool isValid(const string& s, int start, int end) {\n        if (start > end) {\n            return false;\n        }\n        if (s[start] == '0' && start != end) { // 0开头的数字不合法\n                return false;\n        }\n        int num = 0;\n        for (int i = start; i <= end; i++) {\n            if (s[i] > '9' || s[i] < '0') { // 遇到非数字字符不合法\n                return false;\n            }\n            num = num * 10 + (s[i] - '0');\n            if (num > 255) { // 如果大于255了不合法\n                return false;\n            }\n        }\n        return true;\n    }\n    vector<string> result;// 记录结果\n    // startIndex: 搜索的起始位置，pointNum:添加逗点的数量\n    void backtracking(string& s, int startIndex, int pointNum){\n        if (pointNum == 3) { // 逗点数量为3时，分隔结束\n            // 判断第四段子字符串是否合法，如果合法就放进result中\n            if (isValid(s, startIndex, s.size() - 1)) {\n                result.push_back(s);\n            }\n            return;\n        }\n        \n        for (int i = startIndex; i < s.size(); i++) {\n            if (isValid(s, startIndex, i)) { // 判断 [startIndex,i] 这个区间的子串是否合法\n                s.insert(s.begin() + i + 1 , '.');  // 在i的后面插入一个逗点\n                pointNum++;\n                backtracking(s, i + 2, pointNum);   // 插入逗点之后下一个子串的起始位置为i+2\n                pointNum--;                         // 回溯\n                s.erase(s.begin() + i + 1);         // 回溯删掉逗点\n            } else break; // 不合法，直接结束本层循环\n        }\n    }\n\n\n    vector<string> restoreIpAddresses(string s) {\n        if (s.size() < 4 || s.size() > 12) return result; // 算是剪枝了\n        backtracking(s, 0, 0);\n        return result;\n    }\n};\n```\n\n##  78.子集\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/subsets/)\n\n给定一组**不含重复元素**的整数数组 nums，返回该数组所有可能的子集（幂集）。\n\n说明：解集不能包含重复的子集。\n\n示例: 输入: nums = [1,2,3] 输出: [ [3],  [1],  [2],  [1,2,3],  [1,3],  [2,3],  [1,2],  [] ]\n\n**子集是收集树形结构中树的所有节点的结果**。**而组合问题、分割问题是收集树形结构中叶子节点的结果**。\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking(vector<int> nums, int index){\n        result.push_back(path);// 重点！不管到没到最后都要收集结果\n        //下面这个可要可不要,因为在for循环中给出了判断\n        if (index >= nums.size()) {\n            //不是在这里才收集结果\n            return;\n        }\n        \n        for (int i = index; i < nums.size(); i++) {\n            path.push_back(nums[i]);\n            backtracking(nums, i + 1); // 递归\n            path.pop_back();\n        }\n    }\n    vector<vector<int>> subsets(vector<int>& nums) {\n        backtracking(nums, 0);\n        return result;\n    }\n};\n```\n\n## 90.子集II\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/subsets-ii/)\n\n给定一个可能包含**重复元素**的整数数组 nums，返回该数组所有可能的子集（幂集）。\n\n说明：解集不能包含重复的子集。\n\n示例:\n\n- 输入: [1,2,2]\n- 输出: [ [2], [1], [1,2,2], [2,2], [1,2], [] ]\n\n思路：\n\n本题就是在上一题的基础上加上了**去重**，通过**排序去重**\n\n```cpp\nclass Solution {\nprivate:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking(vector<int>& nums, int startIndex) {\n        result.push_back(path);\n        //if (startIndex >= nums.size()) {\n        //    return;\n        //}\n        for (int i = startIndex; i < nums.size(); i++) {\n            //跳过重复元素\n            if (i > startIndex && nums[i] == nums[i - 1]) {\n                continue;\n            }\n            path.push_back(nums[i]);\n            backtracking(nums, i + 1);\n            path.pop_back();\n        }\n    }\n\npublic:\n    vector<vector<int>> subsetsWithDup(vector<int>& nums) {\n        sort(nums.begin(), nums.end()); // 去重需要排序\n        backtracking(nums, 0);\n        return result;\n    }\n};\n```\n\n## 491.递增子序列\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/non-decreasing-subsequences/)\n\n给定一个整型数组, 你的任务是找到所有该数组的递增子序列，递增子序列的长度至少是2。\n\n示例:\n\n- 输入: [4, 6, 7, 7]\n- 输出: [[4, 6], [4, 7], [4, 6, 7], [4, 6, 7, 7], [6, 7], [6, 7, 7], [7,7], [4,7,7]]\n\n说明:\n\n- 给定数组的长度不会超过15。\n- 数组中的整数范围是 [-100,100]。\n- 给定数组中可能包含重复数字，相等的数字应该被视为递增的一种情况。\n\n思路：\n\n不可对原数组进行重排，而且重复的数字不一定挨着出现，所以可以使用哈希表来判断数字是否用过\n\n```cpp\nclass Solution {\nprivate:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking(vector<int>& nums, int startIndex) {\n        if (path.size() > 1) {\n            result.push_back(path);\n            // 注意这里不要加return，要取树上的节点\n        }\n        unordered_set<int> uset; // 使用set对本层元素进行去重\n        for (int i = startIndex; i < nums.size(); i++) {\n            //若当前数比上一个小（注意上一个是path.back），或者使用过，则跳过\n            if ((!path.empty() && nums[i] < path.back())\n                    || uset.find(nums[i]) != uset.end()) {\n                    continue;\n            }\n            uset.insert(nums[i]); // 记录这个元素在本层用过了，本层后面不能再用了\n            path.push_back(nums[i]);\n            backtracking(nums, i + 1);\n            path.pop_back();\n        }\n    }\npublic:\n    vector<vector<int>> findSubsequences(vector<int>& nums) {\n        result.clear();\n        path.clear();\n        backtracking(nums, 0);\n        return result;\n    }\n};\n```\n\n## 46.全排列\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/permutations/)\n\n给定一个 没有重复 数字的序列，返回其所有可能的全排列。\n\n示例:\n\n- 输入: [1,2,3]\n- 输出: [ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1] ]\n\n思路：\n\n用used数组，用过的记为1\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking (vector<int>& nums, vector<bool>& used) {\n        // 此时说明找到了一组\n        if (path.size() == nums.size()) {\n            result.push_back(path);\n            return;\n        }\n        for (int i = 0; i < nums.size(); i++) {\n            if (used[i] == true) continue; // path里已经收录的元素，直接跳过\n            used[i] = true;\n            path.push_back(nums[i]);\n            backtracking(nums, used);\n            path.pop_back();\n            used[i] = false;\n        }\n    }\n    vector<vector<int>> permute(vector<int>& nums) {\n        result.clear();\n        path.clear();\n        vector<bool> used(nums.size(), false);//注意vector的初始化方式\n        backtracking(nums, used);\n        return result;\n    }\n};\n```\n\n## 47.全排列 II\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/permutations-ii/)\n\n给定一个**可包含重复**数字的序列 nums ，按任意顺序返回所有**不重复**的全排列。\n\n思路：\n\n排序+used+set\n\n```cpp\nclass Solution {\nprivate:\n    vector<vector<int>> result;\n    vector<int> path;\n    void backtracking (vector<int>& nums, vector<bool>& used) {\n        if (path.size() == nums.size()) {\n            result.push_back(path);\n            return;\n        }\n        unordered_set<int> uset; // 控制某一节点下的同一层元素不能重复\n        for (int i = 0; i < nums.size(); i++) {\n            if (uset.find(nums[i]) != uset.end()) {\n                continue;\n            }\n            if (used[i] == false) {  //控制同一树枝不重复\n                uset.insert(nums[i]); // 记录元素\n                used[i] = true;\n                path.push_back(nums[i]);\n                backtracking(nums, used);\n                path.pop_back();\n                used[i] = false;\n            }\n        }\n    }\npublic:\n    vector<vector<int>> permuteUnique(vector<int>& nums) {\n        result.clear();\n        path.clear();\n        sort(nums.begin(), nums.end()); // 排序\n        vector<bool> used(nums.size(), false);\n        backtracking(nums, used);\n        return result;\n    }\n};\n```\n\n## 51. N皇后*\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/n-queens/)\n\n按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或**同一斜线**！上的棋子。\n\nn 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。\n\n给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。\n\n每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 'Q' 和 '.' 分别代表了皇后和空位。\n\n题解：\n\n重点是 isValid函数的逻辑\n\n```cpp\nclass Solution {\nprivate:\nvector<vector<string>> result;\n// n 为输入的棋盘大小\n// row 是当前递归到棋盘的第几行了\nvoid backtracking(int n, int row, vector<string>& chessboard) {\n    if (row == n) {\n        result.push_back(chessboard);\n        return;\n    }\n    for (int col = 0; col < n; col++) {\n        if (isValid(row, col, chessboard, n)) { // 验证合法就可以放\n            chessboard[row][col] = 'Q'; // 放置皇后\n            backtracking(n, row + 1, chessboard);\n            chessboard[row][col] = '.'; // 回溯，撤销皇后\n        }\n    }\n}\nbool isValid(int row, int col, vector<string>& chessboard, int n) {\n    // 检查列\n    for (int i = 0; i < row; i++) { // 这是一个剪枝\n        if (chessboard[i][col] == 'Q') {\n            return false;\n        }\n    }\n    // 检查 45度角是否有皇后\n    for (int i = row - 1, j = col - 1; i >=0 && j >= 0; i--, j--) {\n        if (chessboard[i][j] == 'Q') {\n            return false;\n        }\n    }\n    // 检查 135度角是否有皇后\n    for(int i = row - 1, j = col + 1; i >= 0 && j < n; i--, j++) {\n        if (chessboard[i][j] == 'Q') {\n            return false;\n        }\n    }\n    return true;\n}\npublic:\n    vector<vector<string>> solveNQueens(int n) {\n        result.clear();\n    \tvector<string> chessboard(n, string(n, '.'));\n        backtracking(n, 0, chessboard);\n        return result;\n    }\n};\n```\n"},{"url":"/2023/03/11/计算机视觉/test_time_adaptation/wideresnet结构/","content":"### WRN-28-10\n\n<img src=\"wideresnet结构/Wide-Residual-Network-WRN-architecture-of-depth-10-and-widen-factor-4-including-7_Q640-16768105930529.jpg\" alt=\"Wide Residual Network (WRN) architecture of depth 10 and widen factor... |  Download Scientific Diagram\" style=\"zoom:33%;\" />\n\nconv1\nblock1\nblock1.layer\nblock1.layer.0\nblock1.layer.0.bn1\nblock1.layer.0.relu1\nblock1.layer.0.conv1\nblock1.layer.0.bn2\nblock1.layer.0.relu2\nblock1.layer.0.conv2\nblock1.layer.0.convShortcut\nblock1.layer.1\nblock1.layer.1.bn1\nblock1.layer.1.relu1\nblock1.layer.1.conv1\nblock1.layer.1.bn2\nblock1.layer.1.relu2\nblock1.layer.1.conv2\nblock1.layer.2\nblock1.layer.2.bn1\nblock1.layer.2.relu1\nblock1.layer.2.conv1\nblock1.layer.2.bn2\nblock1.layer.2.relu2\nblock1.layer.2.conv2\nblock1.layer.3\nblock1.layer.3.bn1\nblock1.layer.3.relu1\nblock1.layer.3.conv1\nblock1.layer.3.bn2\nblock1.layer.3.relu2\nblock1.layer.3.conv2\nblock2\nblock2.layer\nblock2.layer.0\nblock2.layer.0.bn1\nblock2.layer.0.relu1\nblock2.layer.0.conv1\nblock2.layer.0.bn2\nblock2.layer.0.relu2\nblock2.layer.0.conv2\nblock2.layer.0.convShortcut\nblock2.layer.1\nblock2.layer.1.bn1\nblock2.layer.1.relu1\nblock2.layer.1.conv1\nblock2.layer.1.bn2\nblock2.layer.1.relu2\nblock2.layer.1.conv2\nblock2.layer.2\nblock2.layer.2.bn1\nblock2.layer.2.relu1\nblock2.layer.2.conv1\nblock2.layer.2.bn2\nblock2.layer.2.relu2\nblock2.layer.2.conv2\nblock2.layer.3\nblock2.layer.3.bn1\nblock2.layer.3.relu1\nblock2.layer.3.conv1\nblock2.layer.3.bn2\nblock2.layer.3.relu2\nblock2.layer.3.conv2\nblock3\nblock3.layer\nblock3.layer.0\nblock3.layer.0.bn1\nblock3.layer.0.relu1\nblock3.layer.0.conv1\nblock3.layer.0.bn2\nblock3.layer.0.relu2\nblock3.layer.0.conv2\nblock3.layer.0.convShortcut\nblock3.layer.1\nblock3.layer.1.bn1\nblock3.layer.1.relu1\nblock3.layer.1.conv1\nblock3.layer.1.bn2\nblock3.layer.1.relu2\nblock3.layer.1.conv2\nblock3.layer.2\nblock3.layer.2.bn1\nblock3.layer.2.relu1\nblock3.layer.2.conv1\nblock3.layer.2.bn2\nblock3.layer.2.relu2\nblock3.layer.2.conv2\nblock3.layer.3\nblock3.layer.3.bn1\nblock3.layer.3.relu1\nblock3.layer.3.conv1\nblock3.layer.3.bn2\nblock3.layer.3.relu2\nblock3.layer.3.conv2\nbn1\nrelu\nfc\n\n\n\n[23/02/14 17:17:35] [cifar10c_offline.py:  259]: model for adaptation: WideResNet(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (block1): NetworkBlock(\n    (layer): Sequential(\n      (0): BasicBlock(\n        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (1): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (2): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (3): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n  )\n  (block2): NetworkBlock(\n    (layer): Sequential(\n      (0): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      )\n      (1): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (2): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (3): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n  )\n  (block3): NetworkBlock(\n    (layer): Sequential(\n      (0): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      )\n      (1): BasicBlock(\n        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (2): BasicBlock(\n        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (3): BasicBlock(\n        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n  )\n  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n  (relu): ReLU(inplace=True)\n  (fc): Linear(in_features=640, out_features=10, bias=True)\n)\n[23/0"},{"url":"/2023/03/11/work/简历/","content":"## 毕业设计 -  基于目标域自适应的在线学习算法研究\n\n课题简介：针对深度神经网络在分布偏移的目标域中存在性能急剧下降的问题，研究目标域自适应算法，实现在不改变原模型和训练流程的情况下，仅通过目标域的无标签数据对模型参数进行在线更新，从而极大提升模型对动态变化环境的适应能力。 \n\n当前进度：基于最小化预测熵的思想，在只给定测试数据的情况下，通过将损失函数替换为最小化预测熵，来最小化模型的不确定性；同时提取神经网络在目标域参数优化轨迹的低维子空间，并在子空间上进行网络参数的更新，以提高模型的鲁棒性，从而避免灾难性遗忘问题。目前在cifar10/100c上测试结果已超越SOTA水平。\n\n- 在仅给定测试数据和源模型的情况下，通过**最小化预测熵**损失函数来减少模型不确定性。同时在目标域优化轨迹的**低维子空间**上更新模型参数，以提高模型鲁棒性，避免灾难性遗忘。目前在cifar10/100c上测试结果已超SOTA水平。\n\n\n\n## 基于单目视觉的飞机位姿估计系统\n\n根据项目要求采用了传统方法和基于深度学习的方法，传统方法首先采集目标模板图像，使用sfm对目标进行稀疏重建以获得2D-3D点对应，通过orb特征匹配算法将输入图像与模板库图像进行匹配，选择最优匹配图像后通过PnP解算位姿，并通过卡尔曼滤波排除异常值。\n\n深度学习的方法，预先采集并标注目标检测数据集和关键点识别数据集，通过YOLOv5目标检测网络进行识别以及裁剪图像，然后通过自定义的UNet关键点检测网络识别目标上的2D关键点，最终通过PnP算法求解位姿。\n\n最终实现30ms延时内定位精度优于2%的效果。\n\n\n\n![image-20230305114017843](简历/image-20230305114017843.png)\n\n![流程图](简历/流程图.png)\n\n## 技术栈\n\n\n\n#### 工具栈\n\npython、C++、verilog、pytorch、linux、opencv图像处理库\n\n#### 理论栈\n\n域自适应、特征匹配（传统or深度）、目标检测、视觉位姿估计算法、多视图几何？\n\n\n\n\n\n{YePeng Yang} received the B.E. degree in electronic information engineering from Huazhong University of Science and Technology, Wuhan, China, in 2021. Since 2021, he has been with the Brain-inspired Application Technology Center, Shanghai Jiao Tong University, where he is currently pursuing the Master degree. His main research interests include domain adaptation and feature matching."},{"url":"/2023/03/11/cpp/贪心算法/","content":"## 贪心算法理论基础\n\n**贪心的本质是选择每一阶段的局部最优，从而达到全局最优**。\n\n\n\n有同学问了如何验证可不可以用贪心算法呢？\n\n**最好用的策略就是举反例，如果想不到反例，那么就试一试贪心吧**。\n\n## 455.分发饼干\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/assign-cookies/)\n\n假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。\n\n对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] >= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。\n\n思路：排序后遍历，将最大的饼干喂给能喂饱的胃口最大的孩子。\n\n```cpp\nclass Solution {\npublic:\n    int findContentChildren(vector<int>& g, vector<int>& s) {\n        sort(g.begin(), g.end());\n        sort(s.begin(), s.end());\n        int index = s.size() - 1; // 饼干数组的下标\n        int result = 0;\n        for (int i = g.size() - 1; i >= 0; i--) { // 遍历胃口 \n            if (index >= 0 && s[index] >= g[i]) { // 遍历饼干 \n                result++;\n                index--;\n            }\n        }\n        return result;\n    }\n\n};\n```\n\n## 135. 分发糖果\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/candy/)\n\n老师想给孩子们分发糖果，有 N 个孩子站成了一条直线，老师会根据每个孩子的表现，预先给他们评分。\n\n你需要按照以下要求，帮助老师给这些孩子分发糖果：\n\n- 每个孩子至少分配到 1 个糖果。\n- 相邻的孩子中，评分高的孩子必须获得更多的糖果。\n\n那么这样下来，老师至少需要准备多少颗糖果呢？\n\n思路：\n\n这道题目一定是要确定一边之后，再确定另一边，例如比较每一个孩子的左边，然后再比较右边，**如果两边一起考虑一定会顾此失彼**。\n\n先确定右边评分大于左边的情况（也就是从前向后遍历）\n\n此时局部最优：只要右边评分比左边大，右边的孩子就多一个糖果，全局最优：相邻的孩子中，评分高的右孩子获得比左边孩子更多的糖果\n\n局部最优可以推出全局最优。\n\n再确定左孩子大于右孩子的情况（从后向前遍历）\n\n取candyVec[i + 1] + 1 和 candyVec[i] 最大的糖果数量，**candyVec[i]只有取最大的才能既保持对左边candyVec[i - 1]的糖果多，也比右边candyVec[i + 1]的糖果多**。\n\n```cpp\nclass Solution {\npublic:\n    int candy(vector<int>& ratings) {\n        vector<int> candyVec(ratings.size(), 1);\n        // 从前向后\n        for (int i = 1; i < ratings.size(); i++) {\n            if (ratings[i] > ratings[i - 1]) candyVec[i] = candyVec[i - 1] + 1;\n        }\n        // 从后向前\n        for (int i = ratings.size() - 2; i >= 0; i--) {\n            if (ratings[i] > ratings[i + 1] ) {\n                candyVec[i] = max(candyVec[i], candyVec[i + 1] + 1);\n            }\n        }\n        // 统计结果\n        int result = 0;\n        for (int i = 0; i < candyVec.size(); i++) result += candyVec[i];\n        return result;\n    }\n};\n```\n\n### **BM96** **主持人调度（二）**\n\n## 方法二（排序+贪心）\n\n#### 1.解题思路\n\n- 首先建立两个数组分别存储开始时间（记为start）和结束时间（记为end）。\n- 然后分别对start和end数组进行排序。\n- 接着遍历start数组，判断当前开始时间是否大于等于最小的结束时间，如果是，则说明当前主持人就可以搞定（对应当前最小的结束时间的那个活动）；如果否，则需要新增一个主持人，并将end数组下标后移（表示对应的活动已经有人主持）。\n\n#### 2.代码实现\n\n```cpp\npublic class Solution {\n    public int minmumNumberOfHost (int n, int[][] startEnd) {\n        //初始化两个数组，分别记录开始时间和结束时间 ， 用vector也可以\n        int[] start=new int[n];\n        int[] end=new int[n];\n \n        //将活动的开始和结束时间赋值道start和end数组\n        for(int i=0;i<n;i++){\n            start[i]=startEnd[i][0];\n            end[i]=startEnd[i][1];\n        }\n \n        //按从小到大的顺序对start和end数组排序\n        Arrays.sort(start);\n        Arrays.sort(end);\n \n        int res=0,index=0;\n        for(int i=0;i<n;i++){\n            //如果大于等于当前最小的结束时间，说明当前主持人可以搞定 ????why？？？\n            if(start[i]>=end[index]){\n                index++;\n            }\n            //否则，需要新增主持人\n            else{\n                res++;\n            }\n        }\n \n        return res;\n \n    }\n}\n```\n\n#### 3.复杂度分析\n\n- 时间复杂度：需要进行排序，排序接口的时间复杂度是$$O(nlogn)$$，所以时间复杂度为$$O(nlogn)$$\n- 空间复杂度：需要额外大小为n的start和end数组，所以空间复杂度为$$O(n)$$。\n\n"},{"url":"/2023/03/11/cpp/字符串/","content":"## 反转字符串\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/reverse-string/)\n\n编写一个函数，其作用是将输入的字符串反转过来。输入字符串以字符数组 char[] 的形式给出。\n\n不要给另外的数组分配额外的空间，你必须原地修改输入数组、使用 O(1) 的额外空间解决这一问题。\n\n你可以假设数组中的所有字符都是 ASCII 码表中的可打印字符。\n\n示例 1：\n输入：[\"h\",\"e\",\"l\",\"l\",\"o\"]\n输出：[\"o\",\"l\",\"l\",\"e\",\"h\"]\n\n示例 2：\n输入：[\"H\",\"a\",\"n\",\"n\",\"a\",\"h\"]\n输出：[\"h\",\"a\",\"n\",\"n\",\"a\",\"H\"]\n\n```cpp\nclass Solution {\npublic:\n    void reverseString(vector<char>& s) {\n        char temp;\n        int left = 0;\n        int right = s.size() - 1;\n        while(left < right){\n            temp = s[right];\n            s[right] = s[left];\n            s[left] = temp;\n            left++;\n            right--;\n        }\n    }\n};\n```\n\n解析答案：\n\n```cpp\nvoid reverseString(vector<char>& s) {\n    for (int i = 0, j = s.size() - 1; i < s.size()/2; i++, j--) {\n        swap(s[i],s[j]); //注意何时使用库函数\n    }\n}\n```\n\n\n\n## 反转字符串II\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/reverse-string-ii/)\n\n给定一个字符串 s 和一个整数 k，从字符串开头算起, 每计数至 2k 个字符，就反转这 2k 个字符中的前 k 个字符。\n\n如果剩余字符少于 k 个，则将剩余字符全部反转。\n\n如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。\n\n示例:\n\n输入: s = \"abcdefg\", k = 2\n输出: \"bacdfeg\"\n\n```cpp\nclass Solution {\npublic:\n    string reverseStr(string s, int k) {\n        for(int i = 0; i < s.size(); i = i + 2*k){\n            if(i + k > s.size()) reverse(s.begin()+i,s.end());\n            else reverse(s.begin()+i,s.begin()+i+k);\n        }\n        return s;\n    }\n    //也可自己构造reverse,但要记住官方的reverse是左闭右开\n    // void reverse_me(string& s, int start, int end) {\n    //     for (int i = start, j = end+1; i < j; i++, j--) {\n    //         swap(s[i], s[j-1]);\n    //     }\n    // }\n\n};\n```\n\n## 替换空格\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/ti-huan-kong-ge-lcof/)\n\n请实现一个函数，把字符串 s 中的每个空格替换成\"%20\"。\n\n示例 1： 输入：s = \"We are happy.\"\n输出：\"We%20are%20happy.\"\n\n**思路：**\n\n首先扩充数组到每个空格替换成\"%20\"之后的大小。\n\n然后从后向前替换空格，也就是双指针法\n\n**题解：**\n\n```cpp\nclass Solution {\npublic:\n    string replaceSpace(string s) {\n        int length = s.size();\n        for(int i = 0; i < s.size(); i++){\n            if(s[i] == ' '){\n                length = length + 2;\n            }\n        }\n        int left = s.size() - 1;\n        int right = length - 1;\n        s.resize(length);\n        while(left >= 0){\n            if(s[left] == ' '){\n                s[right] = '0';\n                s[right-1] = '2';\n                s[right-2] = '%';\n                right = right - 3;\n            }else{\n                s[right] = s[left];\n                right--; \n            }\n            left--;\n        }\n        return s;\n    }\n};\n```\n\n## 翻转字符串里的单词*\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/reverse-words-in-a-string/)\n\n给定一个字符串，逐个翻转字符串中的每个单词。\n\n示例 1：\n输入: \"the sky is blue\"\n输出: \"blue is sky the\"\n\n示例 2：\n输入: \"  hello world!  \"\n输出: \"world! hello\"\n解释: 输入字符串可以在前面或者后面包含多余的空格，但是反转后的字符不能包括。\n\n示例 3：\n输入: \"a good  example\"\n输出: \"example good a\"\n解释: 如果两个单词间有多余的空格，将反转后单词间的空格减少到只含一个。\n\n**思路：**\n\n若要不额外占用空间，首先去除多余空格，再翻转整个字符串，再用双指针翻转每个单词\n\n**我的：**\n\n```cpp\nclass Solution {\npublic:\n    string reverseWords(string s) {\n\n        //双指针去除多余空格\n        int a = 0,b = 0;\n        while(a == 0 && s[b] == ' ') b++;\n        for(;b < s.size(); b++){\n            if(s[b] == ' ' && (s[b+1] == ' ' || b+1 >= s.size())) continue; \n            else {\n                s[a] = s[b];\n                a++;\n            }\n        }\n\n        s.resize(a);\n        //翻转字符串\n        reverse(s,0,s.size());\n\n        //双指针翻转每个单词\n        for(int c = 0,d = 0; d <= s.size(); d++){\n            if(s[d] == ' '|| d == s.size()){\n                reverse(s,c,d);\n                c = d + 1;\n            }\n        }\n        return s;\n    }\n\n    //左闭右开\n    void reverse(string& s, int start, int end) {\n        for (int i = start, j = end-1; i < j; i++, j--) {\n            swap(s[i], s[j]);\n        }\n    }\n};\n```\n\n**题解：**\n\nhttps://www.cnblogs.com/zou-ma/p/16162731.html \n\n在 for 循环中，`i++`和 `++i`是一样一样的，但是`++i`的循环耗时短。\n\n使用`++i`是一定程度上的系统优化\n\n```cpp\nclass Solution {\npublic:\n    void reverse(string& s, int start, int end){ //翻转，区间写法：左闭右闭 []\n        for (int i = start, j = end; i < j; i++, j--) {\n            swap(s[i], s[j]);\n        }\n    }\n\n    void removeExtraSpaces(string& s) {//去除所有空格并在相邻单词之间添加空格, 快慢指针。\n        int slow = 0;   //整体思想参考https://programmercarl.com/0027.移除元素.html\n        for (int i = 0; i < s.size(); ++i) { //\n            if (s[i] != ' ') { //遇到非空格就处理，即删除所有空格。\n                if (slow != 0) s[slow++] = ' '; //手动控制空格，给单词之间添加空格。slow != 0说明不是第一个单词，需要在单词前添加空格。\n                while (i < s.size() && s[i] != ' ') { //补上该单词，遇到空格说明单词结束。\n                    s[slow++] = s[i++];\n                }\n            }\n        }\n        s.resize(slow); //slow的大小即为去除多余空格后的大小。\n    }\n\n    string reverseWords(string s) {\n        removeExtraSpaces(s); //去除多余空格，保证单词之间之只有一个空格，且字符串首尾没空格。\n        reverse(s, 0, s.size() - 1);\n        int start = 0; //removeExtraSpaces后保证第一个单词的开始下标一定是0。\n        for (int i = 0; i <= s.size(); ++i) {\n            if (i == s.size() || s[i] == ' ') { //到达空格或者串尾，说明一个单词结束。进行翻转。\n                reverse(s, start, i - 1); //翻转，注意是左闭右闭 []的翻转。\n                start = i + 1; //更新下一个单词的开始下标start\n            }\n        }\n        return s;\n    }\n};\n```\n\n## 左旋转字符串\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/)\n\n字符串的左旋转操作是把字符串前面的若干个字符转移到字符串的尾部。请定义一个函数实现字符串左旋转操作的功能。比如，输入字符串\"abcdefg\"和数字2，该函数将返回左旋转两位得到的结果\"cdefgab\"。\n\n示例 1：\n输入: s = \"abcdefg\", k = 2\n输出: \"cdefgab\"\n\n示例 2：\n输入: s = \"lrloseumgh\", k = 6\n输出: \"umghlrlose\"\n\n限制：\n1 <= k < s.length <= 10000\n\n**题解：**\n\n整体反转+局部反转就可以实现反转单词顺序的目的\n\n```cpp\nclass Solution {\npublic:\n    string reverseLeftWords(string s, int n) {\n        reverse(s.begin(),s.end());\n        reverse(s.begin(),s.end()-n);\n        reverse(s.end()-n,s.end());\n        return s;\n    }\n};\n```\n\n\n\n## 实现 strStr()——KMP算法\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/find-the-index-of-the-first-occurrence-in-a-string/)\n\n实现 strStr() 函数。\n\n给定一个 haystack 字符串和一个 needle 字符串，在 haystack 字符串中找出 needle 字符串出现的第一个位置 (从0开始)。如果不存在，则返回 -1。\n\n示例 1: 输入: haystack = \"hello\", needle = \"ll\" 输出: 2\n\n示例 2: 输入: haystack = \"aaaaa\", needle = \"bba\" 输出: -1\n\n说明: 当 needle 是空字符串时，我们应当返回什么值呢？这是一个在面试中很好的问题。 对于本题而言，当 needle 是空字符串时我们应当返回 0 。这与C语言的 strstr() 以及 Java的 indexOf() 定义相符。\n\n\n\n#### kmp算法：\n\n那么使用KMP可以解决两类经典问题：\n\n1. 匹配问题：[28. 实现 strStr()(opens new window)](https://programmercarl.com/0028.实现strStr.html)\n2. 重复子串问题：[459.重复的子字符串](https://programmercarl.com/0459.重复的子字符串.html)\n\nKMP的主要思想是**当出现字符串不匹配时，可以知道一部分之前已经匹配的文本内容，可以利用这些信息避免从头再去做匹配了。**\n\n文本串：aabaabaaf\n\n模式串：aabaaf\n\n\n\nnext数组就是一个前缀表（prefix table），即最长相等前后缀\n\n前缀：包含首字母，不包含尾字母的所有子串\n\n后缀：包含尾字母，不包含首字母的所有子串\n\n**前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。**\n\n![KMP详解1](https://code-thinking.cdn.bcebos.com/gifs/KMP%E7%B2%BE%E8%AE%B21.gif)\n\n前缀表的求法：\n\n<img src=\"字符串/image-20230212143008343.png\" alt=\"image-20230212143008343\" style=\"zoom: 25%;\" />\n\n遇见冲突后找冲突的前一位所对应的前缀表，再从前缀表所对下标处继续匹配\n\n![KMP精讲2](https://code-thinking.cdn.bcebos.com/gifs/KMP%E7%B2%BE%E8%AE%B22.gif)\n\n初始化： j 指向前缀末尾位置（也是当前公共前后缀的长度）   i 指向后缀末尾位置\n\n前后缀不相同\n\n前后缀相同\n\n更新next数组\n\n```cpp\nclass Solution {\npublic:\n    void getNext(int* next, const string& s) {\n        //初始化\n        int j = 0;\n        next[0] = 0;\n        // 相等就加（(i 和 j 都得加，而j加1后刚好等于当前i对应的next)，不相等就回溯;先考虑不相等的情况\n        for(int i = 1; i < s.size(); i++) {\n            while (j > 0 && s[i] != s[j]) {\n                j = next[j - 1];\n            }\n            if (s[i] == s[j]) {\n                j++;\n            }\n            next[i] = j;\n        }\n    }\n    int strStr(string haystack, string needle) {\n        if (needle.size() == 0) {\n            return 0;\n        }\n        int next[needle.size()];\n        getNext(next, needle);\n        int j = 0;\n        for (int i = 0; i < haystack.size(); i++) {\n            while(j > 0 && haystack[i] != needle[j]) {\n                j = next[j - 1];\n            }\n            if (haystack[i] == needle[j]) {\n                j++;\n            }\n            if (j == needle.size() ) {\n                return (i - needle.size() + 1);\n            }\n        }\n        return -1;\n    }\n};\n```\n\n# 重复的子字符串\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/repeated-substring-pattern/)\n\n给定一个非空的字符串，判断它是否可以由它的一个子串重复多次构成。给定的字符串只含有小写英文字母，并且长度不超过10000。\n\n示例 1:\n输入: \"abab\"\n输出: True\n解释: 可由子字符串 \"ab\" 重复两次构成。\n\n示例 2:\n输入: \"aba\"\n输出: False\n\n示例 3:\n输入: \"abcabcabcabc\"\n输出: True\n解释: 可由子字符串 \"abc\" 重复四次构成。 (或者子字符串 \"abcabc\" 重复两次构成。)\n\n\n\n**思路：**\n\n字符串s：abcabc\n\n![图二](https://code-thinking-1253855093.file.myqcloud.com/pics/20220728104931.png)\n\n所以判断字符串s是否由重复子串组成，只要两个s拼接在一起，里面还出现一个s的话，就说明是由重复子串组成。\n\n我的思路：可用上题的strStr()找\n\n```cpp\n    bool repeatedSubstringPattern(string s) {\n        string t = s + s;\n        t.erase(t.begin()); t.erase(t.end() - 1); // 掐头去尾\n        if(strStr(t,s) != -1) return true;\n\n        return false;\n    }\n```\n\n用KMP算法：\n\n<img src=\"https://code-thinking-1253855093.file.myqcloud.com/pics/20220728212157.png\" alt=\"图四\" style=\"zoom:50%;\" />\n\n如果len % (len - (next[len - 1])) == 0 ，则说明数组的长度正好可以被 (数组长度-最长相等前后缀的长度) 整除 ，说明该字符串有重复的子字符串。\n\n**数组长度减去最长相同前后缀的长度相当于是第一个周期的长度，也就是一个周期的长度，如果这个周期可以被整除，就说明整个数组就是这个周期的循环。**\n\n```cpp\nclass Solution {\npublic:\n    void getNext (int* next, const string& s){\n        next[0] = 0;\n        int j = 0;\n        for(int i = 1;i < s.size(); i++){\n            while(j > 0 && s[i] != s[j]) {\n                j = next[j - 1];\n            }\n            if(s[i] == s[j]) {\n                j++;\n            }\n            next[i] = j;\n        }\n    }\n\n    bool repeatedSubstringPattern(string s) {\n        int len = s.size();\n        int next[len];\n        getNext(next, s);\n        if(next[len - 1] != 0 && len % (len - (next[len - 1])) == 0) return true;\n        else return false;\n    }\n};\n```\n\n"},{"url":"/2023/03/11/cpp/二叉树/","content":"## [#](https://www.programmercarl.com/二叉树理论基础.html#二叉树理论基础篇)二叉树理论基础篇\n\n**C++中map、set、multimap，multiset的底层实现都是平衡二叉搜索树**，所以map、set的增删操作时间时间复杂度是logn\n\nunordered_map、unordered_set，unordered_map、unordered_map底层实现是哈希表\n\n\n\n一棵树当中没有子结点（即度为0）的结点称为*叶子*结点，简称“*叶子*”\n\n\n\n**二叉树主要有两种遍历方式：**\n\n1. 深度优先遍历：先往深走，遇到叶子节点再往回走。\n2. 广度优先遍历：一层一层的去遍历。\n\n- 深度优先遍历\n    - 前序遍历（递归法，迭代法）\n    - 中序遍历（递归法，迭代法）\n    - 后序遍历（递归法，迭代法）\n- 广度优先遍历\n    - 层次遍历（迭代法）\n\n前中后是针对中间的节点\n\n<img src=\"二叉树/20200806191109896.png\" alt=\"img\" style=\"zoom:50%;\" />\n\n### [#](https://www.programmercarl.com/二叉树理论基础.html#二叉树的定义)二叉树的定义\n\n刚刚我们说过了二叉树有两种存储方式顺序存储，和链式存储，顺序存储就是用数组来存，这个定义没啥可说的，我们来看看链式存储的二叉树节点的定义方式。\n\nC++代码如下：\n\n```cpp\nstruct TreeNode {\n    int val;\n    TreeNode *left;\n    TreeNode *right;\n    TreeNode(int x) : val(x), left(NULL), right(NULL) {}\n};\n```\n\n## 二叉树的递归遍历\n\n- [144.二叉树的前序遍历(opens new window)](https://leetcode.cn/problems/binary-tree-preorder-traversal/)\n- [145.二叉树的后序遍历(opens new window)](https://leetcode.cn/problems/binary-tree-postorder-traversal/)\n- [94.二叉树的中序遍历](https://leetcode.cn/problems/binary-tree-inorder-traversal/)\n\n#### 递归三要素：\n\n1. **确定递归函数的参数和返回值：** 确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。\n\n2. **确定终止条件：** 写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。\n\n3. **确定单层递归的逻辑：** 确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。\n\n    \n\n**以下以前序遍历为例：**\n\n1. **确定递归函数的参数和返回值**：因为要打印出前序遍历节点的数值，所以参数里需要传入vector来放节点的数值，除了这一点就不需要再处理什么数据了也不需要有返回值，所以递归函数返回类型就是void，代码如下：\n\n```cpp\nvoid traversal(TreeNode* cur, vector<int>& vec)\n```\n\n2. **确定终止条件**：在递归的过程中，如何算是递归结束了呢，当然是当前遍历的节点是空了，那么本层递归就要结束了，所以如果当前遍历的这个节点是空，就直接return，代码如下：\n\n```cpp\nif (cur == NULL) return;\n```\n\n3. **确定单层递归的逻辑**：前序遍历是中左右的循序，所以在单层递归的逻辑，是要先取中节点的数值，代码如下：\n\n```cpp\nvec.push_back(cur->val);    // 中\ntraversal(cur->left, vec);  // 左\ntraversal(cur->right, vec); // 右\n```\n\n\n\n前序遍历：\n\n```cpp\nclass Solution {\npublic:\n    void traversal(TreeNode* cur, vector<int>& vec) {\n        if (cur == NULL) return;\n        vec.push_back(cur->val);    // 中\n        traversal(cur->left, vec);  // 左\n        traversal(cur->right, vec); // 右\n    }\n    vector<int> preorderTraversal(TreeNode* root) {\n        vector<int> result;\n        traversal(root, result);\n        return result;\n    }\n};\n```\n\n那么前序遍历写出来之后，中序和后序遍历就不难理解了，代码如下：\n\n中序遍历：\n\n```cpp\nvoid traversal(TreeNode* cur, vector<int>& vec) {\n    if (cur == NULL) return;\n    traversal(cur->left, vec);  // 左\n    vec.push_back(cur->val);    // 中\n    traversal(cur->right, vec); // 右\n}\n```\n\n后序遍历：\n\n```cpp\nvoid traversal(TreeNode* cur, vector<int>& vec) {\n    if (cur == NULL) return;\n    traversal(cur->left, vec);  // 左\n    traversal(cur->right, vec); // 右\n    vec.push_back(cur->val);    // 中\n}\n```\n\n## 二叉树的迭代遍历\n\n**递归的实现就是：每一次递归调用都会把函数的局部变量、参数值和返回地址等压入调用栈中**\n\n可以用**栈**实现二叉树的前后中序遍历\n\n#### 前序遍历（迭代法）\n\n前序遍历是中左右，每次先处理的是中间节点，那么先将根节点放入栈中，然后将右孩子加入栈，再加入左孩子。\n\n为什么要先加入 右孩子，再加入左孩子呢？ 因为这样出栈的时候才是中左右的顺序。\n\n<img src=\"二叉树/二叉树前序遍历（迭代法）-16766184074375.gif\" alt=\"二叉树前序遍历（迭代法）\" style=\"zoom: 67%;\" />\n\n```cpp\nclass Solution {\npublic:\n    vector<int> preorderTraversal(TreeNode* root) {\n        stack<TreeNode*> st;\n        vector<int> result;\n        if (root == NULL) return result;\n        st.push(root);\n        while (!st.empty()) {\n            TreeNode* node = st.top();                       // 中\n            st.pop();\n            result.push_back(node->val);\n            if (node->right) st.push(node->right);           // 右（空节点不入栈）\n            if (node->left) st.push(node->left);             // 左（空节点不入栈）\n        }\n        return result;\n    }\n};\n```\n\n**中序遍历，可以写出如下代码：**\n\n**需要借用指针的遍历来帮助访问节点，栈则用来处理节点上的元素**\n\n**这是因为前序遍历中访问节点（遍历节点）和处理节点（将元素放进result数组中）可以同步处理，但是中序就无法做到同步！**\n\n```cpp\nclass Solution {\npublic:\n    vector<int> inorderTraversal(TreeNode* root) {\n        vector<int> result;\n        stack<TreeNode*> st;\n        TreeNode* cur = root;\n        while (cur != NULL || !st.empty()) {\n            if (cur != NULL) { // 指针来访问节点，访问到最底层\n                st.push(cur); // 将访问的节点放进栈\n                cur = cur->left;                // 左\n            } else {\n                cur = st.top(); // 从栈里弹出的数据，就是放进result数组里的数据\n                st.pop();\n                result.push_back(cur->val);     // 中\n                cur = cur->right;               // 右\n            }\n        }\n        return result;\n    }\n};\n```\n\n后序：\n\n<img src=\"https://img-blog.csdnimg.cn/20200808200338924.png\" alt=\"前序到后序\" style=\"zoom:50%;\" />\n\n```cpp\nclass Solution {\npublic:\n    vector<int> postorderTraversal(TreeNode* root) {\n        stack<TreeNode*> st;\n        vector<int> result;\n        if (root == NULL) return result;\n        st.push(root);\n        while (!st.empty()) {\n            TreeNode* node = st.top();\n            st.pop();\n            result.push_back(node->val);\n            if (node->left) st.push(node->left); // 相对于前序遍历，这更改一下入栈顺序 （空节点不入栈）\n            if (node->right) st.push(node->right); // 空节点不入栈\n        }\n        reverse(result.begin(), result.end()); // 将结果反转之后就是左右中的顺序了\n        return result;\n    }\n};\n```\n\n## 二叉树的统一迭代法\n\n#### 迭代法中序遍历\n\n中序遍历代码如下：（详细注释）\n\n```cpp\nclass Solution {\npublic:\n    vector<int> inorderTraversal(TreeNode* root) {\n        vector<int> result;\n        stack<TreeNode*> st;\n        if (root != NULL) st.push(root);\n        while (!st.empty()) {\n            TreeNode* node = st.top();\n            if (node != NULL) {\n                st.pop(); // 将该节点弹出，避免重复操作，下面再将右中左节点添加到栈中\n                if (node->right) st.push(node->right);  // 添加右节点（空节点不入栈）\n\n                st.push(node);                          // 添加中节点\n                st.push(NULL); // 中节点访问过，但是还没有处理，加入空节点做为标记。\n\n                if (node->left) st.push(node->left);    // 添加左节点（空节点不入栈）\n            } else { // 只有遇到空节点的时候，才将下一个节点放进结果集\n                st.pop();           // 将空节点弹出\n                node = st.top();    // 重新取出栈中元素\n                st.pop();\n                result.push_back(node->val); // 加入到结果集\n            }\n        }\n        return result;\n    }\n};\n```\n\n**中节点放入栈之后，紧接着放入一个空指针作为标记**\n\n##  102.二叉树的层序遍历——广度优先\n\n广度用队列，深度用栈\n\n```cpp\n//迭代法\nclass Solution {\npublic:\n    vector<vector<int>> levelOrder(TreeNode* root) {\n        queue<TreeNode*> que;\n        if (root != NULL) que.push(root);\n        vector<vector<int>> result;\n        while (!que.empty()) {\n            int size = que.size();\n            vector<int> vec;\n            // 这里一定要使用固定大小size，不要使用que.size()，因为que.size是不断变化的\n            for (int i = 0; i < size; i++) {\n                TreeNode* node = que.front();\n                que.pop();\n                vec.push_back(node->val);\n                if (node->left) que.push(node->left);\n                if (node->right) que.push(node->right);\n            }\n            result.push_back(vec);\n        }\n        return result;\n    }\n};\n```\n\n```cpp\n// 递归法\nclass Solution {\npublic:\n    void order(TreeNode* cur, vector<vector<int>>& result, int depth)\n    {\n        if (cur == nullptr) return;\n        if (result.size() == depth) result.push_back(vector<int>());\n        result[depth].push_back(cur->val);\n        order(cur->left, result, depth + 1);\n        order(cur->right, result, depth + 1);\n    }\n    vector<vector<int>> levelOrder(TreeNode* root) {\n        vector<vector<int>> result;\n        int depth = 0;\n        order(root, result, depth);\n        return result;\n    }\n};\n```\n\n# 226.翻转二叉树\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/invert-binary-tree/)\n\n翻转一棵二叉树。\n\n思路：\n\n采用递归前序的方法。\n\n[题解还有更多其他方法](https://www.programmercarl.com/0226.%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91.html#%E6%8B%93%E5%B1%95)\n\n```cpp\nclass Solution {\npublic:\n    void order(TreeNode* cur)\n    {\n        if (cur == nullptr) return;\n        TreeNode* temp = cur->left;\n        cur->left = cur->right;\n        cur->right = temp;\n        order(cur->left);\n        order(cur->right);\n    }\n\n    TreeNode* invertTree(TreeNode* root) {\n        order(root);\n        return root;\n    }\n};\n```\n\n## 101. 对称二叉树\n\n递归法：\n\n```cpp\nclass Solution {\npublic:\n    bool compare(TreeNode* left, TreeNode* right){\n        if (left == NULL && right != NULL) return false;\n        else if (left != NULL && right == NULL) return false;\n        else if (left == NULL && right == NULL) return true;\n        else if (left->val != right->val) return false; \n        else{\n            bool outside = compare(left->left, right->right);\n            bool inside = compare(left->right, right->left);\n            bool isSame = outside && inside;\n            return isSame;\n        }\n    }\n\n    bool isSymmetric(TreeNode* root) {\n        if(root == NULL) return true;\n        return compare(root->left,root->right);\n    }\n};\n```\n\n迭代法的条件判断和递归的逻辑是一样的。\n\n代码如下：\n\n```cpp\nclass Solution {\npublic:\n    bool isSymmetric(TreeNode* root) {\n        if (root == NULL) return true;\n        queue<TreeNode*> que;\n        que.push(root->left);   // 将左子树头结点加入队列\n        que.push(root->right);  // 将右子树头结点加入队列\n        \n        while (!que.empty()) {  // 接下来就要判断这两个树是否相互翻转\n            TreeNode* leftNode = que.front(); que.pop();\n            TreeNode* rightNode = que.front(); que.pop();\n            if (!leftNode && !rightNode) {  // 左节点为空、右节点为空，此时说明是对称的\n                continue;\n            }\n\n            // 左右一个节点不为空，或者都不为空但数值不相同，返回false\n            if ((!leftNode || !rightNode || (leftNode->val != rightNode->val))) {\n                return false;\n            }\n            que.push(leftNode->left);   // 加入左节点左孩子\n            que.push(rightNode->right); // 加入右节点右孩子\n            que.push(leftNode->right);  // 加入左节点右孩子\n            que.push(rightNode->left);  // 加入右节点左孩子\n        }\n        return true;\n    }\n};\n```\n\n## 104.二叉树的最大深度\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/maximum-depth-of-binary-tree/)\n\n给定一个二叉树，找出其最大深度。\n\n二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。\n\n说明: 叶子节点是指没有子节点的节点。\n\n示例： 给定二叉树 [3,9,20,null,null,15,7]，\n\n![104. 二叉树的最大深度](https://img-blog.csdnimg.cn/20210203153031914.png)\n\n返回它的最大深度 3 。\n\n**我的代码：**\n\n```cpp\nclass Solution {\npublic:\n    //使用递归法\n    int order(TreeNode* cur, int depth){\n        if(cur == NULL) return depth;\n        depth++;\n        int d1 = order(cur->left,depth);\n        int d2 = order(cur->right,depth);\n        depth = max(d1,d2);\n        return depth;\n    }\n\n    int maxDepth(TreeNode* root) {\n        int depth = 0;\n        return order(root, depth);\n    }\n};\n```\n\n```cpp\n//迭代法 层序遍历\nclass Solution {\npublic:\n    int maxDepth(TreeNode* root) {\n        if(root == NULL) return 0;\n        int depth = 0;\n        queue<TreeNode*> que;\n        que.push(root);\n        while(!que.empty()){\n            int size = que.size();\n            for(int i = 0; i < size; i++){\n                TreeNode* node = que.front();\n                que.pop();\n                if(node->left) que.push(node->left);\n                if(node->right) que.push(node->right);\n            }\n            depth++;\n        }\n\n        return depth;\n    }\n};\n```\n\n我的n叉树求最大深度代码：\n\n```cpp\nclass Solution {\npublic:\n    //使用递归法\n    int order(Node* cur, int depth){\n        if(cur == NULL) return depth;\n        depth++;\n        int maxDepth = depth;\n        for(int i = 0; i < cur->children.size(); i++){\n            int d1 = order(cur->children[i],depth);\n            maxDepth = max(d1,maxDepth);\n        }\n        return maxDepth;\n    }\n\n    int maxDepth(Node* root) {\n        int depth = 0;\n        return order(root, depth);\n    }\n};\n```\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    int maxDepth(Node* root) {\n        if (root == 0) return 0;\n        int depth = 0;\n        for (auto child : root->children){\n            depth = max(depth, maxDepth(child));\n        }\n        return depth + 1;\n    }\n};\n```\n\n## 111.二叉树的最小深度\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/minimum-depth-of-binary-tree/)\n\n给定一个二叉树，找出其最小深度。\n\n最小深度是从**根节点到最近叶子节点**的最短路径上的节点数量。\n\n说明: 叶子节点是指没有子节点的节点。\n\n示例:\n\n给定二叉树 [3,9,20,null,null,15,7],\n\n![111.二叉树的最小深度1](https://img-blog.csdnimg.cn/2021020315582586.png)\n\n返回它的最小深度 2\n\n<img src=\"https://img-blog.csdnimg.cn/20210203155800503.png\" alt=\"111.二叉树的最小深度\" style=\"zoom:33%;\" />\n\n\n\n我的代码 ：\n\n本题核心就是找到根节点\n\n```cpp\n// 后续遍历\nclass Solution {\npublic:\n// 如果左子树为空，右子树不为空，说明最小深度是 1 + 右子树的深度。\n// 反之，右子树为空，左子树不为空，最小深度是 1 + 左子树的深度。\n// 最后如果左右子树都不为空，返回左右子树深度最小值 + 1 \n    int getDepth(TreeNode* node){\n        int depth = 0;\n        if(node == NULL) return 0;\n        //上式不等价于node->left == NULL && node->right == NULL 的情况，而是表明这个节点为空，不会增加深度。\n        int leftDepth = getDepth(node->left);//左\n        int rightDepth = getDepth(node->right);//右\n        // 中  但是没有要处理的\n        if(node->left == NULL && node->right != NULL) return rightDepth + 1;\n        if(node->left != NULL && node->right == NULL) return leftDepth + 1;\n        // if(node->left != NULL && node->right != NULL) return min(leftDepth,rightDepth) + 1;\n        // if(node->left == NULL && node->right == NULL) return 0 + 1;  叶子节点 深度加1\n        return min(leftDepth,rightDepth) + 1;  //最好还是在所有分支都给返回值，防止编译器判断为无返回值\n    }\n\n    int minDepth(TreeNode* root) {\n        return getDepth(root);\n    }\n};\n```\n\n```cpp\n//题解：前序\nclass Solution {\nprivate:\n    int result;\n    void getdepth(TreeNode* node, int depth) {\n        if (node->left == NULL && node->right == NULL) {\n            result = min(depth, result);  \n            return;\n        }\n        // 中 只不过中没有处理的逻辑\n        if (node->left) { // 左\n            getdepth(node->left, depth + 1);\n        }\n        if (node->right) { // 右\n            getdepth(node->right, depth + 1);\n        }\n        return ;\n    }\n\npublic:\n    int minDepth(TreeNode* root) {\n        if (root == NULL) return 0;\n        result = INT_MAX;\n        getdepth(root, 1);\n        return result;\n    }\n};\n```\n\n```cpp\n//用队列进行层序遍历，逻辑比较清晰\nclass Solution {\npublic:\n\n    int minDepth(TreeNode* root) {\n        if (root == NULL) return 0;\n        int depth = 0;\n        queue<TreeNode*> que;\n        que.push(root);\n        while(!que.empty()) {\n            int size = que.size();\n            depth++; // 记录最小深度\n            for (int i = 0; i < size; i++) {\n                TreeNode* node = que.front();\n                que.pop();\n                if (node->left) que.push(node->left);\n                if (node->right) que.push(node->right);\n                if (!node->left && !node->right) { // 当左右孩子都为空的时候，说明是最低点的一层了，退出\n                    return depth;\n                }\n            }\n        }\n        return depth;\n    }\n};\n```\n\n## 222.完全二叉树的节点个数\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/count-complete-tree-nodes/)\n\n给出一个完全二叉树，求出该树的节点个数。\n\n<img src=\"二叉树/20200920221638903.png\" alt=\"img\" style=\"zoom: 33%;\" />\n\n我的代码（没有针对完全二叉树做优化）：\n\n```cpp\nclass Solution {\npublic:\n    int countNodes(TreeNode* root) {\n        queue<TreeNode*> que;\n        if(root != NULL) que.push(root);\n        int num = 0;\n        while(!que.empty()){\n            int size = que.size();\n            for(int i = 0; i < size; i++){\n                TreeNode* node = que.front();\n                que.pop();\n                num++;\n                if(node->left) que.push(node->left);\n                if(node->right) que.push(node->right);\n            }\n        }\n        return num;\n    }\n};\n```\n\n题解：\n\n这个就不是满二叉树\n\n<img src=\"https://code-thinking-1253855093.file.myqcloud.com/pics/20220829163709.png\" alt=\"img\" style=\"zoom:33%;\" />\n\n判断其子树是不是满二叉树(一直向左遍历的深度==一直向右遍历的深度)，如果是则利用公式（2^树深度 - 1 ，注意这里根节点深度为1）计算这个子树（满二叉树）的节点数量，如果不是则继续递归\n\n```cpp\nclass Solution {\npublic:\n    int countNodes(TreeNode* root) {\n        if (root == nullptr) return 0;\n        TreeNode* left = root->left;\n        TreeNode* right = root->right;\n        int leftDepth = 0, rightDepth = 0; // 这里初始为0是有目的的，为了下面求指数方便\n        while (left) {  // 求左子树深度\n            left = left->left;\n            leftDepth++;\n        }\n        while (right) { // 求右子树深度\n            right = right->right;\n            rightDepth++;\n        }\n        if (leftDepth == rightDepth) {\n            return (2 << leftDepth) - 1; // 注意(2<<1) 相当于2^2，所以leftDepth初始为0\n        }\n        //上面是返回条件\n        return countNodes(root->left) + countNodes(root->right) + 1;\n    }\n};\n```\n\n## 110.平衡二叉树\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/balanced-binary-tree/)\n\n给定一个二叉树，判断它是否是高度平衡的二叉树。\n\n本题中，一棵高度平衡二叉树定义为：一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。\n\n这里强调一波概念：\n\n- 二叉树节点的深度：指从根节点到该节点的最长简单路径边的条数。\n- 二叉树节点的高度：指从该节点到叶子节点的最长简单路径边的条数。\n\n<img src=\"https://img-blog.csdnimg.cn/20210203155515650.png\" alt=\"110.平衡二叉树2\" style=\"zoom:50%;\" />\n\n采用递归：\n\n分别求出其左右子树的高度，然后如果差值小于等于1，则返回当前二叉树的高度，否则返回-1，表示已经不是二叉平衡树了。\n\n```cpp\nclass Solution {\npublic:\n    int getHeight(TreeNode* node){\n        if(node == NULL) return 0;\n        int leftHeight = getHeight(node->left);\n        if(leftHeight == -1) return -1;\n        int rightHeight = getHeight(node->right);\n        if(rightHeight == -1) return -1;\n        if(abs(leftHeight-rightHeight)>1) return -1;\n        else return 1 + max(leftHeight, rightHeight); // 以当前节点为根节点的树的最大高度\n    }\n\n    bool isBalanced(TreeNode* root) {\n        int result = getHeight(root);\n        if(result != -1) return true;\n        else return false;\n    }\n};\n```\n\n## 257. 二叉树的所有路径\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/binary-tree-paths/)\n\n给定一个二叉树，返回所有从根节点到叶子节点的路径。\n\n题解：\n\nto_string() 将数字转为string\n\n```cpp\nclass Solution {\nprivate:\n\n    void traversal(TreeNode* cur, string path, vector<string>& result) {\n        path += to_string(cur->val); // 中\n        if (cur->left == NULL && cur->right == NULL) {\n            result.push_back(path);\n            return;\n        }\n        if (cur->left) traversal(cur->left, path + \"->\", result); // 左\n        if (cur->right) traversal(cur->right, path + \"->\", result); // 右\n    }\n\npublic:\n    vector<string> binaryTreePaths(TreeNode* root) {\n        vector<string> result;\n        string path;\n        if (root == NULL) return result;\n        traversal(root, path, result);\n        return result;\n\n    }\n};\n```\n\n## 404.左叶子之和\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/sum-of-left-leaves/)\n\n计算给定二叉树的所有左叶子之和。\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    int sumleft(TreeNode* root , int leftfig, int sum){\n        // int leftfig = 0;\n        // int sum = 0;\n        if(root->left == NULL && root->right == NULL && leftfig == 1){\n            return sum + root->val;\n        }\n        int sum1 = 0;\n        int sum2 = 0;\n        if(root->left) sum1 = sumleft(root->left, 1 ,sum);\n        if(root->right) sum2 = sumleft(root->right, 0 ,sum);\n        return sum1 + sum2;\n    }\n    int sumOfLeftLeaves(TreeNode* root) {\n        int leftfig = 0;\n        int sum = 0;\n        if(root == NULL) return 0;\n        return sumleft(root, leftfig, sum);\n    }\n};\n```\n\n## 513.找树左下角的值\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/find-bottom-left-tree-value/)\n\n给定一个二叉树，在树的最后一行找到最左边的值。\n\n我的代码：\n\n层序遍历\n\n```cpp\nclass Solution {\npublic:\n    int findBottomLeftValue(TreeNode* root) {\n        queue <TreeNode*> que;\n        que.push(root);\n        int result;\n        while(!que.empty()){\n            int size = que.size();\n            for(int i = 0; i < size; i++){\n                TreeNode* node = que.front();\n                que.pop();\n                if(i == 0) result = node->val; //关键是这一行，记录最后一层的第一个元素\n                if(node->left) que.push(node->left);\n                if(node->right) que.push(node->right);\n            }\n        }\n        return result;\n    }\n};\n```\n\n## 112. 路径总和\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/path-sum/)\n\n给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。\n\n我的代码：参考**257. 二叉树的所有路径**\n\n当不用遍历所有二叉树时，需要返回值，如本题，一旦找到路径就返回。\n\n```cpp\nclass Solution {\npublic:\n    bool traversal(TreeNode* cur, int pathsum, int targetSum) {\n        pathsum += cur->val; // 中\n        if (cur->left == NULL && cur->right == NULL) {\n            if(pathsum == targetSum) return true;\n            else return false;\n        }\n        bool leftsig;\n        bool rightsig;\n        if (cur->left) leftsig = traversal(cur->left, pathsum, targetSum); // 左\n        if (cur->right) rightsig = traversal(cur->right, pathsum, targetSum); // 右\n        return leftsig || rightsig; //这里也可以优化一下，只要leftsig=1，就返回，right同理\n    }\n\n    bool hasPathSum(TreeNode* root, int targetSum) {\n        if(root == NULL) return false;\n        return traversal(root, 0, targetSum);\n    }\n};\n```\n\n## 113. 路径总和ii\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/path-sum-ii/)\n\n给定一个二叉树和一个目标和，找到所有从根节点到叶子节点路径总和等于给定目标和的路径。\n\n说明: 叶子节点是指没有子节点的节点。\n\n我的代码：参考**257. 二叉树的所有路径**\n\n```cpp\nclass Solution {\npublic:\n    void traversal(TreeNode* cur, vector<int> path, int pathsum, int targetSum, vector<vector<int>>& path_results) {\n        path.push_back(cur->val);\n        pathsum += cur->val; // 中\n        if (cur->left == NULL && cur->right == NULL) {\n            if(pathsum == targetSum){\n                path_results.push_back(path);\n                return ;\n            } \n            else return;\n        }\n        if (cur->left) traversal(cur->left, path, pathsum, targetSum, path_results); // 左\n        if (cur->right) traversal(cur->right, path, pathsum, targetSum, path_results); // 右\n        return ;\n    }\n    vector<vector<int>> pathSum(TreeNode* root, int targetSum) {\n        vector<vector<int>> path_results;\n        vector<int> path;\n        if(root == NULL) return path_results;\n        traversal(root, path, 0, targetSum, path_results);\n        return path_results;\n    }\n};\n```\n\n## 106.从中序与后序遍历序列构造二叉树*\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/construct-binary-tree-from-inorder-and-postorder-traversal/)\n\n根据一棵树的中序遍历与后序遍历构造二叉树。\n\n注意: 你可以假设树中没有重复的元素。\n\n例如，给出\n\n中序遍历 inorder = [9,3,15,20,7] 后序遍历 postorder = [9,15,7,20,3] 返回如下的二叉树：\n\n<img src=\"二叉树/20210203154316774-16775032045302.png\" alt=\"106. 从中序与后序遍历序列构造二叉树1\" style=\"zoom:100%;\" />\n\n思路：\n\n以 后序数组的最后一个元素为切割点，先切中序数组，根据中序数组，反过来再切后序数组。一层一层切下去，每次后序数组最后一个元素就是节点元素\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* traversal(vector<int>& inorder, vector<int>& postorder) {    \n        // 第一步 终止条件\n        if (postorder.size() == 0) return NULL;\n        int rootValue = postorder[postorder.size() - 1];\n        TreeNode* root = new TreeNode(rootValue);\n        if (postorder.size() == 1) return root;// 叶子节点\n\n        // 找到中序遍历的切割点\n        int delimiterIndex;\n        for (delimiterIndex = 0; delimiterIndex < inorder.size(); delimiterIndex++) {\n            if (inorder[delimiterIndex] == rootValue) break;\n        }\n        // 切割中序数组  这题关键：这里切割的写法\n        // 左闭右开区间：[0, delimiterIndex)\n        vector<int> leftInorder(inorder.begin(), inorder.begin() + delimiterIndex);\n        // [delimiterIndex + 1, end)\n        vector<int> rightInorder(inorder.begin() + delimiterIndex + 1, inorder.end() );\n\t\t// 切割后序\n        vector<int> leftPostorder(postorder.begin(), postorder.begin() + leftInorder.size());\n        // [leftInorder.size(), end)\n        vector<int> rightPostorder(postorder.begin() + leftInorder.size(), postorder.end()- 1);\n        \n        root->left = traversal(leftInorder, leftPostorder);\n        root->right = traversal(rightInorder, rightPostorder);\n\n        return root;    \n\n    }\n\n    TreeNode* buildTree(vector<int>& inorder, vector<int>& postorder) {\n        if (inorder.size() == 0 || postorder.size() == 0) return NULL;\n        return traversal(inorder, postorder);\n    }\n};\n```\n\n\n\n## 654.最大二叉树\n\n[力扣题目地址(opens new window)](https://leetcode.cn/problems/maximum-binary-tree/)\n\n给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下：\n\n- 二叉树的根是数组中的最大元素。\n- 左子树是通过数组中最大值左边部分构造出的最大二叉树。\n- 右子树是通过数组中最大值右边部分构造出的最大二叉树。\n\n<img src=\"https://assets.leetcode.com/uploads/2020/12/24/tree1.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* recursion(vector<int> nums) {\n        if(nums.size() == 0) return NULL;\n        int rootValue = 0;\n        int rootValueIndex = 0;\n        for(int i = 0; i < nums.size(); i++){\n            if(nums[i]>rootValue){\n                rootValue = nums[i];\n                rootValueIndex = i;\n            }\n        }\n        TreeNode* root = new TreeNode(rootValue);\n        vector<int> leftnums(nums.begin(), nums.begin() + rootValueIndex);\n        vector<int> rightnums(nums.begin() + rootValueIndex + 1, nums.end());\n\n        root->left = recursion(leftnums);\n        root->right = recursion(rightnums);\n        return root;\n    }\n\n    TreeNode* constructMaximumBinaryTree(vector<int>& nums) {\n        return recursion(nums);\n    }\n};\n```\n\n## 617.合并二叉树\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/merge-two-binary-trees/)\n\n给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。\n\n你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。\n\n我的代码：\n\n也可以不new，在root1上改\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* mergeTrees(TreeNode* root1, TreeNode* root2) {\n        TreeNode* root = new TreeNode();\n        if(root1 == NULL) return root2;\n        if(root2 == NULL) return root1;\n        root->val = root1->val + root2->val;\n        root->left = mergeTrees(root1->left,root2->left);\n        root->right = mergeTrees(root1->right,root2->right);\n        return root; \n    }\n};\n```\n\n## 700.二叉搜索树中的搜索\n\n[力扣题目地址(opens new window)](https://leetcode.cn/problems/search-in-a-binary-search-tree/)\n\n给定二叉搜索树（BST）的根节点和一个值。 你需要在BST中找到节点值等于给定值的节点。 返回以该节点为根的子树。 如果节点不存在，则返回 NULL。\n\n思路：\n\n二叉搜索树是一个有序树：\n\n- 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值；\n- 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值；\n- 它的左、右子树也分别为二叉搜索树\n\n我的代码：\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* searchBST(TreeNode* root, int val) {\n        if(root == NULL) return NULL;\n        if(root->val == val){\n            return root;\n        }\n        if(root->val < val){\n            return searchBST(root->right, val);\n        }\n        if(root->val > val){\n            return searchBST(root->left, val);\n        }\n        return NULL;\n    }\n};\n```\n\n## 98.验证二叉搜索树*\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/validate-binary-search-tree/)\n\n给定一个二叉树，判断其是否是一个有效的二叉搜索树。\n\n假设一个二叉搜索树具有如下特征：\n\n- 节点的左子树只包含小于当前节点的数。\n- 节点的右子树只包含大于当前节点的数。\n- 所有左子树和右子树自身必须也是二叉搜索树。\n\n思路：\n\n这道题目比较容易陷入两个陷阱：\n\n- 陷阱1\n\n**不能单纯的比较左节点小于中间节点，右节点大于中间节点就完事了**。**我们要比较的是 左子树所有节点小于中间节点，右子树所有节点大于中间节点**。\n\n例如： [10,5,15,null,null,6,20] 这个case：\n\n<img src=\"https://img-blog.csdnimg.cn/20200812191501419.png\" alt=\"二叉搜索树\" style=\"zoom: 67%;\" />\n\n节点10大于左节点5，小于右节点15，但右子树里出现了一个6 这就不符合了！\n\n- 陷阱2\n\n样例中最小节点 可能是int的最小值，如果这样使用最小的int来比较也是不行的。\n\n此时可以初始化比较元素为longlong的最小值`long long maxVal = LONG_MIN;`\n\n\n\n**核心思想：二叉搜索树如果按左中右（中序）顺序遍历，则数据是递增的**\n\n题解：\n\n```cpp\n//先变成数组\nclass Solution {\nprivate:\n    vector<int> vec;\n    void traversal(TreeNode* root) {\n        if (root == NULL) return;\n        traversal(root->left);\n        vec.push_back(root->val); // 将二叉搜索树转换为有序数组\n        traversal(root->right);\n    }\npublic:\n    bool isValidBST(TreeNode* root) {\n        vec.clear(); // 不加这句在leetcode上也可以过，但最好加上\n        traversal(root);\n        for (int i = 1; i < vec.size(); i++) {\n            // 注意要小于等于，搜索树里不能有相同元素\n            if (vec[i] <= vec[i - 1]) return false;\n        }\n        return true;\n    }\n};\n```\n\n## 530.二叉搜索树的最小绝对差\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/minimum-absolute-difference-in-bst/)\n\n给你一棵所有节点为非负值的二叉搜索树，请你计算树中任意两节点的差的绝对值的最小值。\n\n我的代码（参考98）：\n\n```cpp\nclass Solution {\npublic:\n    vector<int> vec;\n    void traversal(TreeNode* root){\n        if(root == NULL) return;\n        traversal(root->left);\n        vec.push_back(root->val); // 将二叉搜索树转换为有序数组\n        traversal(root->right);\n    }\n\n    int getMinimumDifference(TreeNode* root) {\n        traversal(root);\n        int min_sub = 100000;\n        for (int i = 1; i < vec.size(); i++) {\n            min_sub = min(min_sub,vec[i] - vec[i - 1]);\n        }\n        return min_sub;\n    }\n};\n```\n\n## 501.二叉搜索树中的众数\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/find-mode-in-binary-search-tree/)\n\n给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。\n\n假定 BST 有如下定义：\n\n- 结点左子树中所含结点的值小于等于当前结点的值\n- 结点右子树中所含结点的值大于等于当前结点的值\n- 左子树和右子树都是二叉搜索树\n\n思路：\n\n针对任意二叉树，可以使用map（有序，key是次数，value是节点值）\n\n这里是二叉搜索树，是递增的，可以只比较相邻元素\n\n```cpp\nclass Solution {\npublic:\n    vector<int> vec;\n    void traversal(TreeNode* root){\n        if(root == NULL) return;\n        traversal(root->left);\n        vec.push_back(root->val); // 将二叉搜索树转换为有序数组\n        traversal(root->right);\n    }\n\n    vector<int> findMode(TreeNode* root) {\n        traversal(root);\n        int left = 0;\n        int maxnum = 0;\n        vector<int> results;\n        for (int right = 0; right < vec.size(); right++) {\n            if(vec[left] != vec[right]) left = right;\n            if(maxnum < right - left){\n                maxnum = right - left;\n                results.clear(); //这里很关键\n                results.push_back(vec[right]);\n            }else if(maxnum == right - left){\n                results.push_back(vec[right]);\n            }\n\n        }\n        return results;\n    }\n};\n```\n\n## 236. 二叉树的最近公共祖先\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/)\n\n给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n\n我的代码：\n\n返回子树中找到目标值的个数\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* result = NULL;\n    int traversal(TreeNode* node, TreeNode* p, TreeNode* q) {\n        if(result != NULL) return 2;\n        int leftfig = 0;\n        int rightfig = 0;\n        if(node->left) leftfig = traversal(node->left, p, q);\n        else leftfig = 0;\n        if(node->right) rightfig = traversal(node->right, p, q);\n        else rightfig = 0;\n        if(leftfig == 1 && rightfig == 1) {\n            result = node;\n            return 2;\n          \n        }else if((leftfig == 1 && rightfig == 0)or(leftfig == 0 && rightfig == 1)){\n            if(node == p || node == q) {\n                result = node;\n                return 2;\n            }\n            else return 1;\n        } \n        else if(leftfig == 0 && rightfig == 0) {\n            if(node == p || node == q) return 1;\n            else return 0;\n        }\n        else return 2;\n    }\n\n    TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) {\n        traversal(root, p, q);\n        return result;\n    }\n};\n```\n\n题解：\n\n主要是理清这张图的逻辑\n\n<img src=\"二叉树/202102041512582.png\" alt=\"236.二叉树的最近公共祖先2\" style=\"zoom:50%;\" />\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) {\n        if (root == q || root == p || root == NULL) return root;\n        TreeNode* left = lowestCommonAncestor(root->left, p, q);\n        TreeNode* right = lowestCommonAncestor(root->right, p, q);\n        if (left != NULL && right != NULL) return root;\n\n        if (left == NULL && right != NULL) return right;\n        else if (left != NULL && right == NULL) return left;\n        else  { //  (left == NULL && right == NULL)\n            return NULL;\n        }\n    }\n};\n```\n\n1. 求最小公共祖先，需要**从底向上遍历**，那么二叉树，只能通过**后序遍历（**即：回溯）实现从底向上的遍历方式。\n2. 在回溯的过程中，必然要遍历整棵二叉树，即使已经找到结果了，依然要把其他节点遍历完，因为要使用递归函数的返回值（也就是代码中的left和right）做逻辑判断。\n3. 要理解如果返回值left为空，right不为空为什么要返回right，为什么可以用返回right传给上一层结果。\n\n```cpp\n//后序遍历\nleft = 递归函数(root->left);  // 左\nright = 递归函数(root->right); // 右\nleft与right的逻辑处理;         // 中 \n```\n\n搜索一条边的写法（本题，下题）：\n\n```text\nif (递归函数(root->left)) return ;\nif (递归函数(root->right)) return ;\n```\n\n搜索整个树写法：\n\n```text\nleft = 递归函数(root->left);\nright = 递归函数(root->right);\nleft与right的逻辑处理;\n```\n\n \n\n## 235. 二叉搜索树的最近公共祖先\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-search-tree/)\n\n给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。\n\n题解：\n\n```cpp\n//对于二叉搜索树，从上向下去递归遍历，第一次遇到 cur节点是数值在[p, q]区间中，那么cur就是 p和q的最近公共祖先\nclass Solution {\npublic:\n    TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) {\n        while(root) {\n            if (root->val > p->val && root->val > q->val) {\n                root = root->left;\n            } else if (root->val < p->val && root->val < q->val) {\n                root = root->right;\n            } else return root;\n        }\n        return NULL;\n    }\n};\n```\n\n## 701.二叉搜索树中的插入操作\n\n[力扣题目链接](https://leetcode.cn/problems/insert-into-a-binary-search-tree/)\n\n<img src=\"二叉树/insertbst-16777246991064.jpg\" alt=\"img\" style=\"zoom:50%;\" />\n\n我的代码：\n\n迭代法\n\n找到空的就插入新节点\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* insertIntoBST(TreeNode* root, int val) {\n        TreeNode* node = root; \n        TreeNode* insert = new TreeNode(val);//注意新插入节点的创建方式\n        if(node == NULL) return insert;\n        while(node != NULL){\n            if(val > node->val) {\n                if(node->right) node = node->right;\n                else {\n                    node->right = insert;\n                    break;\n                }\n            }\n            if(val < node->val) {\n                if(node->left) node = node->left;\n                else {\n                    node->left = insert;\n                    break;\n                }\n            }\n        }\n        return root;\n    }\n};\n```\n\n题解：\n\n记录了父节点\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* insertIntoBST(TreeNode* root, int val) {\n        if (root == NULL) {\n            TreeNode* node = new TreeNode(val);\n            return node;\n        }\n        TreeNode* cur = root;\n        TreeNode* parent = root; // 这个很重要，需要记录上一个节点，否则无法赋值新节点\n        while (cur != NULL) {\n            parent = cur;\n            if (cur->val > val) cur = cur->left;\n            else cur = cur->right;\n        }\n        TreeNode* node = new TreeNode(val);\n        if (val < parent->val) parent->left = node;// 此时是用parent节点的进行赋值\n        else parent->right = node;\n        return root;\n    }\n};\n```\n\n## 450.删除二叉搜索树中的节点*\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/delete-node-in-a-bst/)\n\n给定一个二叉搜索树的根节点 root 和一个值 key，删除二叉搜索树中的 key 对应的节点，并保证二叉搜索树的性质不变。返回二叉搜索树（有可能被更新）的根节点的引用。\n\n一般来说，删除节点可分为两个步骤：\n\n首先找到需要删除的节点； 如果找到了，删除它。 说明： 要求算法时间复杂度为 $O(h)$，h 为树的高度。\n\n思路：\n\n1. 暴力：中序遍历后变成数组再处理\n\n2. **分析各种可能的情况**\n\n    有以下五种情况：\n\n    - 第一种情况：没找到删除的节点，遍历到空节点直接返回了\n    - 找到删除的节点\n        - 第二种情况：左右孩子都为空（叶子节点），直接删除节点， 返回NULL为根节点\n        - 第三种情况：删除节点的左孩子为空，右孩子不为空，删除节点，右孩子补位，返回右孩子为该节点\n        - 第四种情况：删除节点的右孩子为空，左孩子不为空，删除节点，左孩子补位，返回左孩子为该节点\n        - 第五种情况：左右孩子节点都不为空，则将删除节点的左子树头结点（左孩子）放到删除节点的右子树的最左面节点的左孩子上，返回删除节点右孩子为该节点。\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* deleteNode(TreeNode* root, int key) {\n        if (root == nullptr) return nullptr; //1\n        if (root->val == key) {\n            if(root->left == nullptr && root->right == nullptr) return nullptr; //2\n            if(root->left == nullptr) return root->right; //3\n            if(root->right == nullptr) return root->left; //4\n            if(root->left != nullptr && root->right != nullptr){ //5\n                // 找右子树最左面的节点\n                TreeNode* cur = root->right; \n                while(cur->left != nullptr) {\n                    cur = cur->left;\n                }\n                cur->left = root->left; // 把要删除的节点（root）左子树放在cur的左孩子的位置\n                root = root->right; \n                return root;\n            } \n        }\n   //上面相当于把新的节点返回给上一层，上一层就要用 root->left 或者 root->right接住，代码如下：\n        if (root->val > key) root->left = deleteNode(root->left, key);\n        if (root->val < key) root->right = deleteNode(root->right, key);\n        return root;\n    }\n};\n```\n\n## 669. 修剪二叉搜索树*\n\n[力扣题目链接](https://leetcode.cn/problems/trim-a-binary-search-tree/)\n\n<img src=\"二叉树/trim2.jpg\" alt=\"img\" style=\"zoom:33%;\" />\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    TreeNode* trimBST(TreeNode* root, int low, int high) {\n        if (root == nullptr ) return nullptr;\n        if (root->val < low) {\n            TreeNode* right = trimBST(root->right, low, high); // 寻找符合区间[low, high]的节点\n            return right;\n        }\n        if (root->val > high) {\n            TreeNode* left = trimBST(root->left, low, high); // 寻找符合区间[low, high]的节点\n            return left;\n        }\n        root->left = trimBST(root->left, low, high); // root->left接入符合条件的左孩子\n        root->right = trimBST(root->right, low, high); // root->right接入符合条件的右孩子\n        return root;\n    }\n};\n```\n\n## 108.将有序数组转换为二叉搜索树\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/convert-sorted-array-to-binary-search-tree/)\n\n将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。\n\n本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。\n\n思路：\n\n1. 选取数组中间的节点为根节点，分割数组为左右两部分\n2. 分别在两部分中间再找中间节点\n\n题解：\n\n```cpp\nclass Solution {\nprivate:\n    TreeNode* traversal(vector<int>& nums, int left, int right) {\n        if (left > right) return nullptr;\n        int mid = left + ((right - left) / 2); // 防止溢出 \n        // 等价于int mid = (left + right) / 2;\n        TreeNode* root = new TreeNode(nums[mid]);\n        root->left = traversal(nums, left, mid - 1);\n        root->right = traversal(nums, mid + 1, right);\n        return root;\n    }\npublic:\n    TreeNode* sortedArrayToBST(vector<int>& nums) {\n        TreeNode* root = traversal(nums, 0, nums.size() - 1);\n        return root;\n    }\n};\n```\n\n## 538.把二叉搜索树转换为累加树\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/convert-bst-to-greater-tree/)\n\n给出二叉 搜索 树的根节点，该树的节点值各不相同，请你将其转换为累加树（Greater Sum Tree），使每个节点 node 的新值等于原树中大于或等于 node.val 的值之和。\n\n思路：\n\n右中左序遍历，需要记录前一个节点的数值\n\n```cpp\nclass Solution {\nprivate:\n    int pre = 0; // 记录前一个节点的数值\n    void traversal(TreeNode* cur) { // 右中左遍历\n        if (cur == NULL) return;\n        traversal(cur->right);\n        cur->val += pre;\n        pre = cur->val;\n        traversal(cur->left);\n    }\npublic:\n    TreeNode* convertBST(TreeNode* root) {\n        pre = 0;\n        traversal(root);\n        return root;\n    }\n};\t\t\n```\n\n"},{"url":"/2023/02/09/cpp/哈希表/","content":"## 哈希表理论基础\n\n哈希表是根据关键码的值而直接进行访问的数据结构。\n\n当我们需要**查询一个元素是否出现过，或者一个元素是否在集合里的时候**，就要第一时间想到哈希法。\n\n**拉链法**\n\n刚刚小李和小王在索引1的位置发生了冲突，发生冲突的元素都被存储在链表中。 这样我们就可以通过索引找到小李和小王了\n\n![哈希表4](哈希表/20210104235015226.png)\n\n（数据规模是dataSize， 哈希表的大小为tableSize）\n\n其实拉链法就是要选择适当的哈希表的大小，这样既不会因为数组空值而浪费大量内存，也不会因为链表太长而在查找上浪费太多时间。\n\n**线性探测法**\n\n使用线性探测法，一定要保证tableSize大于dataSize。 我们需要依靠哈希表中的空位来解决碰撞问题。\n\n例如冲突的位置，放了小李，那么就向下找一个空位放置小王的信息。所以要求tableSize一定要大于dataSize ，要不然哈希表上就没有空置的位置来存放 冲突的数据了。如图所示：\n\n![哈希表5](哈希表/20210104235109950.png)\n\n常见的哈希结构：数组、set（集合）、map（映射）\n\n> std::unordered_set底层实现为哈希表，std::set 和std::multiset 的底层实现是红黑树，红黑树是一种平衡二叉搜索树，所以key值是有序的，但key不可以修改，改动key值会导致整棵树的错乱，所以只能删除和增加。\n\n> std::unordered_map 底层实现为哈希表，std::map 和std::multimap 的底层实现是红黑树。同理，std::map 和std::multimap 的key也是有序的（这个问题也经常作为面试题，考察对语言容器底层的理解）。\n\n**当我们要使用集合来解决哈希问题的时候，优先使用unordered_set，因为它的查询和增删效率是最优的，如果需要集合是有序的，那么就用set，如果要求不仅有序还要有重复数据的话，那么就用multiset**\n\n虽然std::set、std::multiset 的底层实现是红黑树，不是哈希表，不过给我们的使用方式，还是哈希法的使用方式，即key和value。map同理。\n\n\n\n## 有效的字母异位词\n\n题目：给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。\n\n示例 1: 输入: s = \"anagram\", t = \"nagaram\" 输出: true\n\n示例 2: 输入: s = \"rat\", t = \"car\" 输出: false\n\n**说明:** 你可以假设字符串只包含小写字母。\n\n> 数组就是简单的哈希表，但是数组的大小可不是无限开辟的\n\n本题使用数组模拟哈希表，数组大小为26，元素值代表字母出现的次数\n\n![242.有效的字母异位词](哈希表/242.有效的字母异位词.gif)\n\n\n\n## 两个数组的交集\n\n![349. 两个数组的交集](哈希表/20200818193523911.png)\n\n本题结果是无重复的，且没有限制数组的长度，且无需排序，因此使用**unordered_set**\n\n> **使用数组来做哈希的题目，是因为题目都限制了数值的长度。**而这道题目没有限制数值的长度，就无法使用数组来做哈希表了。\n>\n> （用vector代替数组是否能解决限制长度的问题）\n>\n> **而且如果哈希值比较少、特别分散、跨度非常大，使用数组就造成空间的极大浪费。**\n\n[c++ unordered_set详细操作_好人好事代表nxx的博客-CSDN博客_unordered_set](https://blog.csdn.net/qq_40286920/article/details/124731777)\n\n**使用迭代器构造**\n\n```cpp\nunordered_set<int> set3(set1.begin(), set1.end());\n```\n\n**find()函数——查找**\n//查找2，找到返回迭代器，失败返回end()\n\n```cpp\nset1.find(2);\n```\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    vector<int> intersection(vector<int>& nums1, vector<int>& nums2) {\n        unordered_set<int> result_set;\n        unordered_set<int> nums_set(nums1.begin(),nums1.end());\n        for(int num:nums2){\n            if (nums_set.find(num) != nums_set.end()) {\n                result_set.insert(num);\n            }\n        }\n        return vector<int>(result_set.begin(), result_set.end()); //注意返回值类型\n    }\n};\n```\n\n注：\n\n那有同学可能问了，遇到哈希问题我直接都用set不就得了，用什么数组啊。\n\n直接使用set 不仅占用空间比数组大，而且速度要比数组慢，set把数值映射到key上都要做hash计算的。\n\n不要小瞧 这个耗时，在数据量大的情况，差距是很明显的。\n\n\n\n## 快乐数\n\n编写一个算法来判断一个数 n 是不是快乐数。\n\n「快乐数」定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 **无限循环** 但始终变不到 1。如果 可以变为 1，那么这个数就是快乐数。\n\n如果 n 是快乐数就返回 True ；不是，则返回 False 。\n\n**示例：**\n\n输入：19\n输出：true\n解释：\n1^2 + 9^2 = 82\n8^2 + 2^2 = 68\n6^2 + 8^2 = 100\n1^2 + 0^2 + 0^2 = 1\n\n题解：\n\n```cpp\nclass Solution {\npublic:\n    int getSum(int n){\n        int sum = 0;\n        //取数值各个位上的单数操作\n        while(n){\n            sum += (n % 10) * (n % 10); //不可使用^2表示平方\n            n = n/10;\n        }\n        return sum;\n    }\n    bool isHappy(int n) {\n        unordered_set<int> sums_set;\n        while(1){\n            int sum = getSum(n);\n            if(sum == 1) return true;\n            if(sums_set.find(sum) != sums_set.end()) return false;\n            sums_set.insert(sum);\n            n = sum;\n        }\n    }\n};\n```\n\n## 两数之和\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/two-sum/)\n\n给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。\n\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。\n\n**示例:**\n\n给定 nums = [2, 7, 11, 15], target = 9\n\n因为 nums[0] + nums[1] = 2 + 7 = 9\n\n所以返回 [0, 1]\n\n**思路：**\n\n每次遍历时要看这个**元素**之前是否出现过–>考虑用哈希\n\n同时要存元素和下表两个值–>用map–>元素对应key，下标对应value\n\n**题解：**\n\n[(73条消息) C++之auto的使用_Waldeinsamkeit_的博客-CSDN博客_auto iter](https://blog.csdn.net/zyc2018/article/details/93591189)\n\n[(73条消息) C++中pair的用法_淼淼1111的博客-CSDN博客_pair](https://blog.csdn.net/u010112268/article/details/81153034)\n\n```c++\nclass Solution {\npublic:\n    vector<int> twoSum(vector<int>& nums, int target) {\n        std::unordered_map <int,int> map;\n        for(int i = 0; i < nums.size(); i++){\n            auto iter = map.find(target-nums[i]);\n            if(iter != map.end()){\n                return {iter->second,i};\n            }else{\n                map.insert(pair<int, int>(nums[i], i));//可用此用法\n            }\n        }\n        return {};\n    }\n};\n```\n\n```cpp\nclass Solution {\npublic:\n    vector<int> twoSum(vector<int>& nums, int target) {\n        unordered_map <int,int> map;\n        for(int i = 0; i < nums.size(); i++){\n            auto iter = map.find(target-nums[i]);\n            if(iter != map.end()){\n                return {iter->second,i};\n            }else{\n                map[nums[i]]=i;  //可用此用法\n            }\n        }\n        return {};\n    }\n};\n```\n\n\n\n## 四数相加\n\n[力扣题目链接](https://leetcode.cn/problems/4sum-ii/)\n\n给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。\n\n为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -2^28 到 2^28 - 1 之间，最终结果不会超过 2^31 - 1 。\n\n**例如:**\n\n输入:\n\n- A = [ 1, 2]\n- B = [-2,-1]\n- C = [-1, 2]\n- D = [ 0, 2]\n\n输出:\n\n2\n\n\n\n**思路：**\n\nHashMap 存一个数组，如 A。然后计算三个数组之和，如 BCD。时间复杂度为：O(n)+O(n^3)，得到 O(n^3).\nHashMap 存三个数组之和，如 ABC。然后计算一个数组，如 D。时间复杂度为：O(n^3)+O(n)，得到 O(n^3).\nHashMap 存两个数组之和，如AB。然后计算两个数组之和，如 CD。时间复杂度为：O(n^2) + O(n^2)，得到 O(n^2).\n\n\n\n要存a+b的所有可能，以及出现的次数\n\n**题解：**\n\n```cpp\nclass Solution {\npublic:\n    int fourSumCount(vector<int>& nums1, vector<int>& nums2, vector<int>& nums3, vector<int>& nums4) {\n        std::unordered_map <int,int> map12;\n        int sum12;\n        int sum34;\n        int num=0;\n        for(int i = 0; i < nums1.size(); i++){\n            for(int j = 0; j < nums2.size(); j++){\n                map12[nums1[i]+nums2[j]]++;\n            }\n        }\n        \n        for(int i = 0; i < nums3.size(); i++){\n            for(int j = 0; j < nums4.size(); j++){\n                sum34 = nums3[i]+nums4[j];\n                if(map12.find(-sum34) != map12.end()){\n                    num = num + map12[-sum34];\n                }\n            }\n        }\n        return num;\n\n    }\n};\n```\n\n```cpp\nclass Solution {\npublic:\n    int fourSumCount(vector<int>& A, vector<int>& B, vector<int>& C, vector<int>& D) {\n        unordered_map<int, int> umap; //key:a+b的数值，value:a+b数值出现的次数\n        // 遍历大A和大B数组，统计两个数组元素之和，和出现的次数，放到map中\n        for (int a : A) {   // 注意这种特殊用法\n            for (int b : B) {\n                umap[a + b]++;\n            }\n        }\n        int count = 0; // 统计a+b+c+d = 0 出现的次数\n        // 在遍历大C和大D数组，找到如果 0-(c+d) 在map中出现过的话，就把map中key对应的value也就是出现次数统计出来。\n        for (int c : C) {\n            for (int d : D) {\n                if (umap.find(0 - (c + d)) != umap.end()) {\n                    count += umap[0 - (c + d)];\n                }\n            }\n        }\n        return count;\n    }\n};\n```\n\n后面大概还剩150道题\n\n\n\n## 赎金信\n\n[力扣题目链接](https://leetcode.cn/problems/ransom-note/)\n\n给定一个赎金信 (ransom) 字符串和一个杂志(magazine)字符串，判断第一个字符串 ransom 能不能由第二个字符串 magazines 里面的字符构成。如果可以构成，返回 true ；否则返回 false。\n\n(题目说明：为了不暴露赎金信字迹，要从杂志上搜索各个需要的字母，组成单词来表达意思。杂志字符串中的每个字符只能在赎金信字符串中使用一次。)\n\n**注意：**\n\n你可以假设两个字符串均只含有小写字母。\n\ncanConstruct(\"a\", \"b\") -> false\ncanConstruct(\"aa\", \"ab\") -> false\ncanConstruct(\"aa\", \"aab\") -> true\n\n**题解：**\n\n```cpp\nclass Solution {\npublic:\n    bool canConstruct(string ransomNote, string magazine) {\n        unordered_map <int,int> map;\n        for(int i = 0; i < magazine.size(); i++){\n            map[magazine[i]]++;\n        }\n        for(int i = 0; i < ransomNote.size(); i++){\n            if(map.find(ransomNote[i]) != map.end()){\n                if(map[ransomNote[i]]>0){\n                    map[ransomNote[i]]--;\n                }else{\n                    return false;\n                }\n            }else{\n                return false;\n            }\n        }\n        return true;\n    }\n};\n```\n\n**其实在本题的情况下，使用map的空间消耗要比数组大一些的，因为map要维护红黑树或者哈希表，而且还要做哈希函数，是费时的！数据量大的话就能体现出来差别了。 所以数组更加简单直接有效！**\n\n```cpp\n// 时间复杂度: O(n)\n// 空间复杂度：O(1)\nclass Solution {\npublic:\n    bool canConstruct(string ransomNote, string magazine) {\n        int record[26] = {0};\n        //add\n        if (ransomNote.size() > magazine.size()) {\n            return false;\n        }\n        for (int i = 0; i < magazine.length(); i++) {\n            // 通过recode数据记录 magazine里各个字符出现次数\n            record[magazine[i]-'a'] ++;\n        }\n        for (int j = 0; j < ransomNote.length(); j++) {\n            // 遍历ransomNote，在record里对应的字符个数做--操作\n            record[ransomNote[j]-'a']--;\n            // 如果小于零说明ransomNote里出现的字符，magazine没有\n            if(record[ransomNote[j]-'a'] < 0) {\n                return false;\n            }\n        }\n        return true;\n    }\n};\n```\n\n## 三数之和\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/3sum/)\n\n给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。\n\n**注意：** 答案中**不可以包含重复**的三元组。\n\n示例：\n\n给定数组 nums = [-1, 0, 1, 2, -1, -4]，\n\n满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ]\n\n\n\n**思考：**\n\n哈希法C++代码:（此题不宜使用哈希法，去重比较复杂）\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> threeSum(vector<int>& nums) {\n        vector<vector<int>> result;\n        sort(nums.begin(), nums.end());\n        // 找出a + b + c = 0\n        // a = nums[i], b = nums[j], c = -(a + b)\n        for (int i = 0; i < nums.size(); i++) {\n            // 排序之后如果第一个元素已经大于零，那么不可能凑成三元组\n            if (nums[i] > 0) {\n                break;\n            }\n            if (i > 0 && nums[i] == nums[i - 1]) { //三元组元素a去重\n                continue;\n            }\n            unordered_set<int> set;\n            for (int j = i + 1; j < nums.size(); j++) {\n                if (j > i + 2\n                        && nums[j] == nums[j-1]\n                        && nums[j-1] == nums[j-2]) { // 三元组元素b去重\n                    continue;\n                }\n                int c = 0 - (nums[i] + nums[j]);\n                if (set.find(c) != set.end()) {\n                    result.push_back({nums[i], nums[j], c});\n                    set.erase(c);// 三元组元素c去重\n                } else {\n                    set.insert(nums[j]);\n                }\n            }\n        }\n        return result;\n    }\n};\n```\n\n\n\n使用双指针法更高效：\n\n![15.三数之和](哈希表/15.三数之和.gif)\n\n遇事不决先排序，两边逼近双指针\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> threeSum(vector<int>& nums) {\n        vector<vector<int>> results;\n        sort(nums.begin(),nums.end());\n        for(int i = 0; i < nums.size(); i++){\n            if(nums[i] > 0) break;\n            //a去重\n            if(i>0 && nums[i] == nums[i-1]) continue;\n            //定义双指针\n            int left = i + 1;\n            int right = nums.size() - 1;\n\n            while(right > left){\n                int sum = nums[i]+nums[left]+nums[right];\n                if(sum > 0) right--;\n                else if(sum <0) left++;\n                else{\n                    results.push_back(vector<int>{nums[i],nums[left],nums[right]});\n                    //去重\n                    while (right > left && nums[right] == nums[right - 1]) right--;\n                    while (right > left && nums[left] == nums[left + 1]) left++;\n                    right--;\n                    left++;\n                }\n            }\n\n        }\n\n        return results;\n    }\n};\n```\n\n## 四数之和\n\n[力扣题目链接(opens new window)](https://leetcode.cn/problems/4sum/)\n\n题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。\n\n**注意：**\n\n答案中不可以包含重复的四元组。\n\n示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ]\n\n```cpp\nclass Solution {\npublic:\n    vector<vector<int>> fourSum(vector<int>& nums, int target) {\n        vector<vector<int>> results;\n        sort(nums.begin(),nums.end());\n        for(int j = 0; j < nums.size(); j++){\n            if(j>0 && nums[j] == nums[j-1]) continue;\n            for(int i = j + 1; i < nums.size(); i++){\n                //a去重\n                if(i>j+1 && nums[i] == nums[i-1]) continue;\n                //定义双指针\n                int left = i + 1;\n                int right = nums.size() - 1;\n\n                while(right > left){\n                    //此处需要强制转换为long，否则会溢出\n                    long sum =(long) nums[j]+nums[i]+nums[left]+nums[right];\n                    if(sum > target) right--;\n                    else if(sum <target) left++;\n                    else{\n                        results.push_back(vector<int>{nums[j],nums[i],nums[left],nums[right]});\n                        //去重\n                        while (right > left && nums[right] == nums[right - 1]) right--;\n                        while (right > left && nums[left] == nums[left + 1]) left++;\n                        right--;\n                        left++;\n                    }\n                }\n\n            }\n        }\n\n\n        return results;\n    }\n};\n```\n\n\n\n## 总结\n\n在两数之和中map正式登场。\n\n来说一说：使用数组和set来做哈希法的局限。\n\n- 数组的大小是受限制的，而且如果元素很少，而哈希值太大会造成内存空间的浪费。\n- set是一个集合，里面放的元素只能是一个key，而两数之和这道题目，不仅要判断y是否存在而且还要记录y的下标位置，因为要返回x 和 y的下标。所以set 也不能用。\n\nmap是一种`<key, value>`的结构，两数之和可以用key保存数值，用value在保存数值所在的下标。所以使用map最为合适。"},{"url":"/2023/01/17/音乐和运动/增重/","content":"注意事项：\n\n- 光吃不练是不能增重的\n- 优先大肌群，胸背腿，肩部也可以优先\n- 优先大重量，一口气能做5-10个即可\n- 跑步、游泳、打球都很难增重，增加的是I型肌纤维，体积小；举铁增加的是II型肌纤维，体积大\n- 新手阶段要以复合动作为主，能够练到多块肌肉\n- 优先自由重量，杠铃和哑铃\n\n![image-20230117221545630](增重/image-20230117221545630.png)\n\n![image-20230117221730315](增重/image-20230117221730315.png)\n\n瘦子推荐吃增肌粉\n\n\n\n不饱和脂肪酸\n\n![image-20230117222019710](增重/image-20230117222019710.png)\n\n\n\n![image-20230117224247590](增重/image-20230117224247590.png)\n\n![image-20230117225403064](增重/image-20230117225403064.png)"},{"url":"/2023/01/06/计算机视觉/目标检测/yolo/","content":"训练：\n\npython train.py --data WANG/data.yaml --weights yolov5s.pt --img 640\n\n\n\n报错：\n\n[(71条消息) yolov5训练时出现WARNING: Ignoring corrupted image and/or label_早睡早起吧的博客-CSDN博客](https://blog.csdn.net/weixin_41859890/article/details/119022689)\n\n\n\n加油口检测结果：\n\n![image-20230106105445743](yolo/image-20230106105445743.png)"},{"url":"/2022/12/11/计算机视觉/test_time_adaptation/泛读/","content":"## SITA: Single Image Test-time Adaptation\n\n**SITA协议：**\n\n•不需要访问源训练数据。\n\n•在推理过程中几乎和原始模型一样快。\n\n•可以适应单个测试实例且不需要批处理。\n\n•适应于一个测试实例的模型不应该在后续实例上使用。\n\n•在测试时不需要超参数调优。\n\n\n\n类似于BN [ 28 ]，我们的方法校准了批范数**统计量**。然而，由于从单个测试实例中估计它们是不可靠的(如[ 28 ]中所做的)，我们利用单个测试实例的某些**增强**来获得批范数统计量的稳健估计。此外，我们提出了一种基于**熵**的方法来估计每个测试实例的校准参数（无需像BN那样手动设置超参）\n\n与TENT [ 31 ]和TTT [ 30 ]需要至少一个后向通道不同，我们的自适应方法只使用一个前向通道，因此(与直接使用源模型相比具有可比性)非常快\n\n\n\n**方法：即对单张输入图像做数据增强，求多张图像的平均统计参数，然后再类似BN的方法动量式的更新**\n\n\n\n（SITA: Single Image Test-time Adaptation）\n\n最近的文献中出现了训练稳健模型[ 9,22,27]的工作。虽然这是一个可行的研究方向，但它需要修改培训过程。这可能并不总是可行的，因为出于隐私/存储方面的考虑，训练数据可能不再可用。所有可用的都是先前训练好的模型。因此，人们对测试时间自适应( Test Time Adaptive，TTA )产生了越来越大的兴趣，在不改变训练过程或要求访问原始训练数据的情况下，模型可以在测试时间进行自适应。\n\nTTT [ 30 ]和TENT [ 31 ]是最近的一些工作，在预测时适应模型方面非常有效。TTT [ 30 ]使用辅助的自监督任务来训练源模型。然后对每个测试实例微调(通过自监督子网络)，进行多次迭代。使用TTT来适应一个新的模型，需要修改训练过程(添加自监督子网络)，因此需要访问源训练数据，而这些数据可能并不总是可用的。此外，多次后退需要相当长的时间，在延迟不可接受的情况下可能无法使用。另一方面，TENT [ 31 ]在不访问源数据的情况下，适应给定的训练模型。TENT假设数据以批次形式出现，批次规模通常比一大很多。\n\n它考虑一个在线设置，其中适应当前实例(批处理)的模型用于适应后续实例(批处理)，这意味着模型具有关于直到某一点所看到的所有测试实例的信息。受TTT [ 30 ]和TENT [ 31 ]的成功和局限性的启发，我们列举了为现实的、具有挑战性的SITA协议所开发的算法的以下理想特性\n\n•不需要访问源训练数据。•在推理过程中几乎和原始模型一样快。•可以适应单个测试实例且不需要批处理。•适应于测试实例的模型不应该在后续实例上使用。•在测试时不需要超参数调优。\n\n第一个属性不仅与隐私/存储问题有关，还与速度有关，因为对源数据的任何重新训练都会使该方法变得缓慢。第三种属性背后的动机是潜伏期和隐私性。对于大批量的样本，我们不得不等待一定数量的样本，导致延迟，或者来自多个用户的俱乐部样本，这可能存在隐私问题。第四个特性是由于不同的测试实例/批次可能来自非常不同的分布，这会对模型的性能产生不利影响。图1比较了SITA设置与文献中其他TTA设置。进一步地，我们不能期望验证实例在测试时刻调整超参数。使用大批量的在线方法，如TENT [ 31 ]，通过一次评估一个腐败类型(单一分布)来提高性能，即重置下一个腐败类型的模型。通过混合CIFAR - 10 - C中的所有15种腐败类型进行评估[ 8 ]\n\n除了TTT和TENT之外，最近的另一项工作BN [ 28 ]分析了校准神经网络批归一化统计量的能力，以使预测对对抗腐败具有鲁棒性。虽然不是特别针对SITA设置而设计，但BN仍然可以应用于SITA设置。BN的工作方式是将源统计量替换为源和目标统计量的加权组合，其中SITA设置中的目标统计量将根据给定的单个测试实例进行估计。这有两个挑战- -第一，单一的图像统计可能不够可靠，第二，校准权重对性能有很大的影响，不同的测试样本可能会有所不同。在文献[ 28 ]中，参数是经验设定的，这对于SITA是不实际的。\n\n在这项工作中，我们提出了AugBN，它克服了上述限制，满足了单幅图像测试时间自适应( SITA )设置的各种需求。该方法不需要对源数据进行任何访问，一次适应一个测试实例，并将模型重置为给定的源模型以适应每一个新的测试实例。与BN [ 28 ]类似，我们的方法校准了批范数统计量。然而，由于从单个测试实例估计它们是不可靠的(如[ 28 ]中所做的)，我们利用单个测试实例的某些增强来获得批量范数统计量的稳健估计。此外，我们提出了一种基于熵的方法来估计每个测试实例的校准参数，而不是像文献[ 28 ]中那样将其视为一种设计选择。我们提出的无超参数方法在各种数据集的分类和分割中表现出一致的性能提升。\n\n与TENT [ 31 ]和TTT [ 30 ]需要至少一个后向通道不同，我们的自适应方法只使用一个前向通道，因此具有相当快的(与直接使用源模型相比具有可比性)。本文的主要贡献总结如下：1 .我们形式化了单幅图像测试时间自适应( SITA )设置。2 .本文提出的无超参数方法在稠密和稀疏预测任务中都表现出快速自适应，仅有一个前向传递。3 .我们在分类和分割任务上都取得了SITA的最新性能。\n\n\n\n**BatchNorm自适应。**最近关于领域适应的文献提出，仅调整批次标准化层的统计信息，以适应测试分布。尽管这些工作并非针对TTA任务，但它们表明，调整标准化统计数据可以提供显著的性能提升，而不需要手工制作的损失函数。预测时间归一化（PTN）[21]使用当前测试批次的均值和方差作为批次规范层中的统计，而不是使用源数据的累积统计。如果测试批大小足够大，可以提供一个很好的估计值，那么这就相当有效。BN[28]使用了类似的方法，重点是提高对腐败的鲁棒性。源模型的累积统计信息与所有可用测试图像上累积的统计信息相结合，以在两种设置中达到可靠的估计：（i）在整个测试集可用的情况下完全自适应，以及（ii）在测试集的子集可用的情况下部分自适应。部分适配设置的特殊情况（一次只有一个图像可用）可被视为测试时间适配。我们的AugBN算法是对BN的显著改进。AugBN使用增强从单个图像获得批处理范数统计的鲁棒估计。此外，所提出的方法自动为每个单独的测试实例找到最佳校准参数，而无需任何验证数据集。这些改进使我们的方法更适用于具有挑战性的SITA设置，在各种数据集和任务中显示出一致的性能提高。最近在网上出现了一些与TTA相关的并行作品。You等人[34]使用具有CORE[13]损失的BN[28]来调整批次范数层的仿射参数。他们的方法需要反向传播和大批量，不符合SITA。Zhang等人[35]对每个测试样本使用32\\/64个扩增样本来获得边际输出分布，并使用类似于TENT[31]的熵损失对其进行优化，这需要对每个样本进行昂贵的优化。Hu等人[11]对传入测试数据的统计数据以及扩增数据进行了在线估计。相比之下，我们的贡献是独一无二的，因为我们根据更难的SITA设置提出了一种轻量级的适应技术。此外，这些方法中的大多数都涉及超参数调整以获得最佳结果，而论文没有在测试时对其进行调整，因此在SITA设置中不现实。\n\n\n\n## Continual Test-Time Domain Adaptation\n\n现实世界中的机器感知系统运行在非平稳且不断变化的环境中，目标域分布会随时间发生变化。现有的方法大多基于自训练和熵正则化，这些方法会受到这些非平稳环境的影响。\n\n为了解决这些问题，我们提出了一种连续测试时间自适应方法( CoTTA )。首先，我们提出使用**权重平均和增强平均**的预测来减少误差累积，这两种方法往往更准确。另一方面，**为了避免灾难性遗忘**，我们提出在每次迭代过程中随机地将一小部分神经元恢复为源预训练的权重，以帮助长期保存源知识\n\n<img src=\"泛读/image-20221205152155979.png\" alt=\"image-20221205152155979\" style=\"zoom:30%;\" />\n\n## Contrastive Test-Time Adaptation\n\n我们提出了一种利用**自监督对比学习**来促进目标特征学习的新方法，以及一种具有显着去噪伪标签的改进的**在线伪标签**方案。对比学习任务与伪标签联合应用，对比与 MoCo 类似但使用源初始化编码器构造的正负对，并排除由伪标签指示的同类负对。同时，我们在线生成伪标签，并通过在目标特征空间中最近的邻居之间进行软投票来细化它们，这是通过维护内存队列来实现的。\n\n\n\n## Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation\n\n域适应( DA )旨在缓解源域和目标域之间的域偏移。大多数DA方法需要访问源数据，但这往往是不可能的(例如,由于数据隐私或知识产权)。在本文中，我们解决了具有挑战性的**无源域适应( SFDA )**问题，在没有源数据的情况下，源预训练模型被适应到目标域。我们的方法是基于这样的观察：目标数据可能不再与源域分类器对齐，仍然形成清晰的簇。我们通过定义目标数据的局部亲和度来捕获这种内在结构，并鼓励具有高局部亲和度的数据之间的标签一致性。我们观察到应该为互惠邻居分配更高的亲和力，并提出一种自正则化损失来减少噪声邻居的负面影响。此外，为了聚合更多上下文的信息，我们考虑了具有较小亲和力值的扩展邻域。在实验结果中我们验证了目标特征的内在结构是领域自适应的重要信息来源。我们证明了这种局部结构可以通过考虑局部邻居、互惠邻居和扩展邻居来有效地捕获。最后，我们在多个2D图像和3D点云识别数据集上实现了最先进的性能。\n\na source-free domain adaptation (SFDA) method 也就相当于TTA\n\n\n\n\n\n## Improving robustness against common corruptions by covariate shift adaptation（BN）\n\n当今最先进的机器视觉模型容易受到图像损坏（如模糊或压缩伪影）的影响，从而限制了其在许多现实应用中的性能。我们在这里认为，衡量模型对常见损坏的健壮性的流行基准（如ImageNet-C）低估了许多（但不是所有）应用场景中的模型健壮性。关键的见解是，在许多场景中，有多个未标记的腐败示例可用，可以用于无监督的在线适应。用损坏图像的统计信息替换通过训练集上的批量归一化估计的激活统计信息，一致地提高了25个不同流行计算机视觉模型的鲁棒性。使用校正后的统计数据，ResNet-50在ImageNet-C上达到62.2%的mCE，而在不进行调整的情况下达到76.7%。凭借更强大的DeepAugment+AugMix模型，我们将最新ResNet50模型的技术水平从53.6%mCE提高到45.4%mCE。即使对单个样本进行调整，也可以提高ResNet-50和AugMix模型的鲁棒性，32个样本足以改善ResNet50架构的现有技术水平。我们认为，无论何时报告腐败基准和其他分布外一般化设置中的分数，都应包括经过调整的统计结果\n\n已知当测试和训练数据从相同分布中采样时，深度神经网络（DNN）在独立和相同分布（i.i.d.）设置中表现良好。\n然而，对于许多应用，这一假设并不成立。在医学成像中，如果使用不同的采集系统，X射线图像或组织切片将与训练数据不同。\n在质量评估中，如果照明条件发生变化或相机上积聚了灰尘颗粒，则图像可能与训练数据不同。自动驾驶汽车可能面临罕见的天气条件，如沙尘暴或大冰雹。虽然人类视觉对这些偏差非常鲁棒[1]，但现代机器视觉模型通常对此类图像损坏敏感。\n我们认为，当前对模型鲁棒性的评估低估了许多（但不是所有）真实场景中的性能。到目前为止，像ImageNet-C[IN-C；2]这样的流行图像损坏基准只关注临时场景，在这种场景中，测试模型对测试期间遇到的损坏没有任何先验知识，即使它多次遇到相同的损坏。在医学图像或质量保证的示例中，图像损坏不会随着样本而改变，而是在潜在的大量样本上持续存在。类似地，在相同的沙尘暴或冰雹期间，自动驾驶汽车将在连续的输入流中面临相同的天气条件。这些（未标记的）观察结果可以允许识别模型适应输入分布的变化。\n这种无监督的自适应机制在域自适应（DA）领域进行了研究，该领域涉及将在一个域（源，这里是干净的图像）上训练的模型自适应到仅存在未标记样本的另一个域中（目标，这里是损坏的图像）。因此，来自域自适应的工具和方法可直接用于提高模型对常见损坏的鲁棒性，但迄今为止尚未报告关于流行基准的结果。这项工作的总体目标是鼓励当前不相交的领域适应和对常见腐败的鲁棒性之间的更强交互。\n我们在此关注DA中的一种流行技术，即调整批处理归一化[BN；3]统计[4-6]。在计算机视觉中，BN是一种用于加速训练的流行技术，几乎存在于所有当前最先进的图像识别模型中。BN估计训练数据集的激活统计，并使用它们来规范网络中的中间激活。\n通过设计，在训练时间期间获得的激活统计信息不反映在分布外设置（如损坏的图像）中测试时测试分布的统计信息。我们调查并证实了这样一个假设，即从干净图像到损坏图像的高水平分布变化在很大程度上表现为深度网络内部表示中的一阶矩和二阶矩的差异，这可以通过调整BN统计信息来缓解，即通过估计损坏图像上的BN统计信息。我们证明，这种简单的自适应可以大大提高对损坏图像的识别性能。\n\n我们的贡献可以总结如下：•我们建议使用两个额外的性能度量来增强常见损坏的当前基准，这两个性能度量在对损坏的图像进行部分和完全无监督的适应之后测量鲁棒性。\n•我们得出了与域适应的联系，并表明即使适应单个损坏的样本，也可以将在IN上训练的ResNet-50模型的基线性能从76:7%mCE提高到71:4%。鲁棒性随着适应样本的增加而增加，并收敛到62:2%的mCE。\n•我们表明，在ImageNet[IN；7，8]上训练的各种普通模型的鲁棒性在适应后显著提高，有时在不适应的情况下接近IN-C上的当前最先进性能。\n•同样，我们表明，当使用经过调整的统计数据时，IN-C上最先进的ResNet-50模型的鲁棒性会不断提高。我们超越了最佳的非适应性模型（52:3%mCE）近7%。\n•我们展示了几个流行图像数据集的结果，并讨论了我们方法的通用性和局限性。\n•我们证明，根据源和目标统计数据之间的Wasserstein距离，可以很好地预测非自适应模型的性能退化。我们提出了一个简单的理论模型，用于基于自适应参数来界定Wasserstein距离。\n\n**纠正批量归一化统计数据，作为减少常见损坏引起的协变移位的有力基线**\n\n我们建议使用域自适应自适应批量归一化统计的一个众所周知的工具[5，6]-作为一个简单的基线，以提高自适应评估场景中对图像损坏的鲁棒性。IN训练的模型通常使用批归一化[BN；3]，以便在训练期间更快地收敛和提高稳定性。在BN层内，第一和第二阶统计量c；在每个特征图c的空间维度和样本上估计激活张量zc的2c。然后通过减去平均值c并除以2c来归一化激活。在训练期间，每个批次估计c和2c。在评估期间，通常使用指数平均法对整个训练数据集的c和2c进行估计[10]。\n使用训练期间获得的BN统计数据进行测试使模型决策具有确定性，但如果输入分布发生变化，则也存在问题。如果激活统计c；对于来自测试域的样本，2c改变，则特征图c的激活不再归一化为零均值和单位方差，打破了所有下游层所依赖的关键假设。\n数学上，这个协变移位2可以形式化如下：定义1（协变移位，参见12，13）。具有密度ps:X Y！R+和密度为pt:X Y！R+，写为ps（x；y）=ps（x）ps（yjx）和pt（x；y）=pt（x）pt（yjx），如果ps（yj x）=pt（yj x）和ps（x（x）6=pt（x），其中y 2 y表示类标签。\n去除协变移位。如果协变移位（Def.1）仅导致特征激活z＝f（x）的一阶矩和二阶矩的差异，则可以通过应用归一化来去除：\n\n通过批处理归一化来减少模型中的协变移位特别简单：估计BN统计量t就足够了；从可用于自适应的测试数据中获取2 t（未标记）样本。如果可用样本的数量n太小，估计的统计数据将太不可靠。因此，我们利用统计数据；2s已经作为先验在训练数据集上计算，并且如下推断每个测试批次的测试统计，\n\n\n\n\n\n## Training on Test Data with Bayesian Adaptation for Covariate Shift\n\n熵最小化。熵最小化已被用作许多环境中的自我监督目标，包括领域适应[Saito等人，2019，Carlucci等人，2017]、半监督学习[Grand代客和Bengio，2004，Bertherot等人，2019年，Lee和Lee，2013]和小样本学习[Dhillon等人，2015]。格兰德代和Bengio【2004】提出了一种概率模型，该模型结合了半监督学习的熵最小化（无分布转移），但仅使用概率模型来激励熵最小化作为MAP解决方案的正则化器，以提高准确性，这不会捕获任何认识上的不确定性。相反，我们关注的是分布转移下的测试时间自适应，这需要引入单独的训练时间和测试时间输入分布模型，并提供可靠的认知不确定性估计，这是我们通过贝叶斯边缘化获得的。我们还设计了一种近似推理方案，以允许在不访问训练数据的情况下进行有效的自适应。测试时间熵最小化（TENT）[Wang等人，2020a]使用熵最小化作为适应测试数据时的唯一目标（尽管没有明确的贝叶斯解释），并在不进一步访问训练数据的情况下进行调整，但仅旨在提高准确性，而不是不确定性估计。与格兰德代和Bengio[2004]相似，TENT仅使用熵最小化学习单个模型，而我们表明，对多个模型显式执行贝叶斯推断和边缘化对于有效的不确定性估计至关重要。TENT还启发性地提出，出于稳定性原因，仅在测试时调整网络中的特定参数，而我们使用学习的后验密度来解释训练集，允许我们调整整个网络，提高某些设置中的性能，并消除启发式设计决策的需要。\n\n\n\n## **Dataset Shift类型**\n\n[(63条消息) Covariate Shift(协变量偏移)_lihe2021的博客-CSDN博客_covariate shift](https://blog.csdn.net/lihe4151021/article/details/123763402)\n\ndataset shift的类型一般分为三种（以下的协变量因变量可以理解为数据中的x及对应标签y）：\n\n- 协变量偏移(Covariate Shift)\n\n协变量的变化，比如模型应用场景中环境、位置的变化等\n\n> 注：这里解释下此处的协变量，假设我们要拟合方程 y=wx，对于一个数据对（x，y）:\n> y为因变量，w为自变量，x为协变量。\n\n- 先验概率偏移(Prior probability shift)\n\n因变量的变化，比如根据月份预测销售额的模型，用平时月份训练的模型预测销售高峰月份的销售额。输入仍然为月份没有变化，但训练集和实际场景中的因变量完全不一样（一般月份和销售高峰月份的销售额本就不一样）。\n\n- 概念偏移\n\n字面理解就是自变量和因变量之间的关系发生了改变。\n\n\n\n\n\n21 \n\nREVISITING BATCH NORMALIZATION FOR\n\nPRACTICAL DOMAIN ADAPTATION\n\n28\n\n"},{"url":"/2022/12/11/计算机视觉/test_time_adaptation/改进方案(杨业鹏-20230207144511)/","content":"相关代码链接：\n\n[DequanWang/tent: ICLR21 Tent: Fully Test-Time Adaptation by Entropy Minimization (github.com)](https://github.com/DequanWang/tent)\n\n[nblt/DLDR: Low Dimensional Landscape Hypothesis is True: DNNs can be Trained in Tiny Subspaces (github.com)](https://github.com/nblt/DLDR)\n\n## 改进和测试方案\n\n- 训练集采用CIFAR10；测试集和验证集采用CIFAR10-C\n\n- 先在CIFAR10数据集上进行常规训练(sgd进行更新)，且同时使用DLDR进行网络权重采样得到低维度优化轨迹子空间（40维）\n\n    ```python\n    python -u train_sgd.py --datasets CIFAR10 --lr 0.1 --corrupt 0 --arch=resnet20 --epochs=150  --save-dir=save_labelnoise$c\\_$model |& tee -a log_$model\n    ```\n\n- 然后在测试集上进行无监督的test time adaptation：\n\t- 优化算法采用P-BFGS（投影到优化轨迹子空间的二阶优化算法）\n\t- 损失函数改为ENTROPY\n\t\n- 最后比较性能：\n\t- source\n\t- 原tent算法\n\t- 原pbfgs\n\t- 本方案改进的算法\n\n\n\n\n\n\n\n```\n\n\npython train_sgd.py --datasets CIFAR10 --lr 0.1 --corrupt 0 --arch=resnet20 --epochs=150  --save-dir=test_save_labelnoise0_resnet20 \n    \npython train_pbfgs.py --epochs 20 --datasets CIFAR10 --corrupt 0. --params_start 0 --params_end 81  --batch-size 1024   --n_components 40 --arch=resnet20  --save-dir=test_save_labelnoise0_resnet20\n\npython train_psgd.py --epochs 40 --datasets CIFAR10 --lr 1 --corrupt 0 --params_start 0 --params_end 81  --batch-size 128  --n_components 40 --arch=resnet20  --save-dir=test_save_labelnoise0_resnet20\n\n\npython -u my_train_pbfgs.py --epochs 20 --datasets CIFAR10 --corrupt 0. --params_start 0 --params_end 81  --batch-size 1024   --n_components 40 --arch=resnet20  --save-dir=my_save_labelnoise0_resnet20 |& tee -a my_log_resnet20\n\n```\n\n\n\ntrain_sgd 150epoch精度为91.25   test.err 为0.0875  testloss 0.3794586459159851\n\n   best  prec为91.71   test err 0.0831  test loss 0.284\n\n\n\npbfgs  20 epoch prec 90.26 test loss 0.303\n\n\n\n​\tbest prec 90.35  \n\n\n\n先在train_sgd.py训练150个epoch的基础上进行熵最小化微调，优化方式为pbfgs(采样81个epoch 的w后降维到40), 只使用测试数据\n\n​\t\n\n\n\n采用150 训练集\n\ntta环境\n\npython my_train_pbfgs --epochs 200 --datasets CIFAR10 --corrupt 0.3 --params_start 0 --params_end 81  --batch-size 64   --n_components 40 --arch=resnet20 --evaluate --save-dir=save_labelnoise0_resnet20\n\n\n\n\n\n## 实验结果\n\n为什么在train和test一样的时候每个epoch的结果相差很多？\n\n​\tmodel.train()和model.eval()的效果不一样吗？bn的问题 bn原理？\n\n```\nseverity = 5\ncorruption_type = 'gaussian_noise'\n```\n\n![image-20221119223520669](改进方案/image-20221119223520669.png)\n\n 到后面精度能提升很多![image-20221119223808215](改进方案/image-20221119223808215.png)\n\n\n\n```\nseverity = 1\ncorruption_type = 'gaussian_noise'\n```\n\n效果明显（batchsize 2000  lr 1e-4）\n\n![image-20221119223946577](改进方案/image-20221119223946577.png)\n\n\n\n\n\n(batch size = 64  lr=1e-4)第一个epoch就已经收敛\n\n![image-20221119224347854](改进方案/image-20221119224347854.png)\n\n\n\n\n\n\n\n降维后是只更新40个参数吗？\n\n\n\ntest sdg\n\n![image-20221121094323553](改进方案/image-20221121094323553.png)\n\n![image-20221121095907461](改进方案/image-20221121095907461.png)\n\n\n\n\n\ntest pbfgs 效果还没sgd好\n\n![image-20221121094603074](改进方案/image-20221121094603074.png)\n\n\n\n有监督 pbfgs\n\n![image-20221121111427513](改进方案/image-20221121111427513.png)\n\n无监督 pbfgs![image-20221121111756185](改进方案/image-20221121111756185.png)\n\n\n\n\n\n测试方案：\n\n1. cifar10上sgd有监督训练  \n\n    选用111epoch的参数\n\n    ![image-20221122134215747](改进方案/image-20221122134215747.png)\n\n2. cifar10c上sgd+tent微调  同时保存epoch参数，便于寻找子空间\n\n    每个小batch存一个参数\n\n    每个epoch5个batch\n\n    如下图所示，选取0-7个epoch即0-34个batch的参数\n\n![image-20221122143317850](改进方案/image-20221122143317850.png)\n\n3. cifart10c上子空间中无监督训练\n\n    在采用相同参数设置的情况下，loss曲线基本相同\n\n    lr =0.0000001\n\n    batch_size=2048\n\n![image-20221122145357279](改进方案/image-20221122145357279.png)\n\n![image-20221122151826425](改进方案/image-20221122151826425.png)\n\n子空间维数为1也是一样的效果\n\n\n\n**采用梯度提取子空间**\n\n并设置lr为1 (小lr的情况下性能反而下降)\n\n有明显提升\n\n![image-20221123140532916](改进方案/image-20221123140532916.png)\n\n![image-20221123140601909](改进方案/image-20221123140601909.png)\n\n子空间为2时性能基本不变\n\n子空间改为1时性能略微下降\n\n![image-20221123141519070](改进方案/image-20221123141519070.png)\n\n子空间改为5时，性能最好\n\n![image-20221123142355847](改进方案/image-20221123142355847.png)\n\n\n\n子空间不变改lr\n\nlr=0.001\n\n![image-20221123141032911](改进方案/image-20221123141032911.png)\n\nlr = 0.1\n\n![image-20221123142618091](改进方案/image-20221123142618091.png)\n\nlr=0.1 采用权重寻找子空间,结果也能很好\n\n![image-20221123143720492](改进方案/image-20221123143720492.png)\n\n\n\n直接将P等于预训练的w\n\n![image-20221123144306888](改进方案/image-20221123144306888.png)\n\n\n\n\n\nw参数存储方式   \n\nclean数据集上的权重\n\n0-35个batch  存为 mat文件\n\n\n\nsgd+entropy 也能训练，只要100个样本就可以收敛\n\n![image-20221130150412389](改进方案/image-20221130150412389.png)\n\n\n\n\n\n可选参数：\n\n- batch_size\n- source_subspace/target_subspace\n- n_components\n\n\n\n\n\n\n"},{"url":"/2022/12/11/计算机视觉/test_time_adaptation/开题/","content":"## 题目：**在线自适应学习算法研究**\n\n### 研究背景\n\nDA:[(59条消息) 域适应（domain adaptation）_点PY的博客-CSDN博客_domain adaptation](https://blog.csdn.net/weixin_42990464/article/details/117149006)\n\n[(59条消息) 迁移学习——Domain Adaptation_Peanut_范的博客-CSDN博客_domain adaptation](https://fancp.blog.csdn.net/article/details/80956828?spm=1001.2101.3001.6650.10&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-10-80956828-blog-117149006.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-10-80956828-blog-117149006.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=16)\n\n随着深度学习的不断发展，深度神经网络在各种应用中表现出良好的性能，特别是当测试样本来自与训练数据相同的分布时，网络预测的准确率可以达到非常高。然而，深度神经网络对于来自不同域数据的概括能力是有限的。当训练（源域）数据与测试（目标域）数据的分布不同时，模型预测的准确率会受到影响，这种情况被称为数据集偏移。例如场景的明暗变化、天气环境的变化以及传感器的退化。正因为存在数据集的偏移，模型在测试过程中可能对训练过程没有见过的未知变化敏感，当源域与目标域的差异过大时，会造成性能的急剧下降，在一些现实场景中，这样的性能下降可能会导致模型无法使用。\n\n在真实场景下的数据分布通常是与训练集的数据分布不同的，为了能在不同的数据分布上部署模型，我们需要模型能够对目标域进行自适应的学习。\n\n在深度学习领域中，可将深度学习算法分为在线学习和离线学习。离线学习也通常称为批学习，是指所有的源数据以批量的形式输入到网络中进行训练，然后将训练所得的模型用于预测任务中。一旦我们要对模型进行更改，只能通过再训练的方式，这将导致效率低下的问题。\n\n离线学习的缺点总结如下：\n\n\\1.    模型训练过程低效\n\n\\2.    训练过程不易拓展于大数据场景。\n\n\\3.    模型无法适应动态变化的环境\n\n在线学习也称为增量学习或适应性学习，是指在一定顺序下接收数据，每接收一个数据，模型会对它进行预测并对当前模型进行更新，然后处理下一个数据。离线学习通常假设训练数据和测试数据是独立同分布的，但是在在线学习的设置中，对训练数据的假设是宽松或几乎没有的。\n\n在线学习的特点总结如下：\n\n\\1.   无需源域数据\n\n\\2.   无需目标域标签\n\n\\3.   无需改变原训练流程\n\n\\4.   无需批处理\n\n随着最近深度神经网络模型的参数量呈指数不断上升，网络的训练成本也急剧增加，普通的用户难以承担如此高额的训练成本。同时，由于数据安全性问题，模型可能在没有源数据的情况下分发，而且在测试期间重新去处理源数据会导致效率十分低下。这意味着在测试阶段，模型必须仅根据其参数和目标域数据在线地对网络进行更新，也就是说，我们需要研究基于目标域自适应的在线学习以解决离线学习的缺点，扩大模型的适用场景，提升模型对动态变化环境的适应能力。\n\n### 研究现状\n\n#### fine-tuning\n\n#### DA\n\n深度学习是机器学习的一个子领域，旨在通过分层架构发现输入数据的多个层次的分布式表示[73]。在过去几年中，基于深度学习的方法出现了爆炸式增长，其中深度学习极大地改进了各种机器学习问题和应用的最先进方法[123]。特别是，深度学习已经将传统的信号处理方法转变为以端到端的方式同时学习特征和预测模型[7]。尽管有监督的深度学习是各种任务中最普遍和最成功的方法，但其成功取决于（i）大量标记的训练数据和（ii）独立且相同分布（i.i.d.）的训练和测试数据集的假设[99]。由于对各种应用领域的海量数据集进行可靠标记通常是昂贵且令人望而却步的，因此对于目标领域中没有足够标记数据集的任务，强烈需要通过利用源领域的丰富标记数据来应用经训练的模型[286]。然而，这种学习策略受到数据分布变化的影响，即源域和目标域之间的域变化[302]。因此，当遇到分布外（OOD）数据时，即源分布与目标分布不同时，训练模型的性能可能会严重下降[25]。\n例如，无法保证应用于目标域中不同于源域中人群的疾病诊断系统的性能。\n\n为了解决这一问题，提出了无监督域自适应（UDA）作为一种可行的解决方案，将从标记的源域学习到的知识迁移到不可见、异构和未标记的目标域[155167]，如图1所示。UDA旨在减轻源域和目标域之间的域转移[118]。UDA的解决方案主要分为统计矩匹配（例如，最大平均差异（MMD）[174]）、领域风格转移[234]、自我训练[148165321]和特征级对抗性学习[66，81，82，171]\n\n域自适应可以被视为转移学习的一种特殊情况，假设标记数据仅在源域中可用[204]。\n\n本文旨在从理论和实践的角度为UDA提供广泛的模型和算法。本综述还涉及新兴方法，特别是最近开发的方法，提供了不同技术的彻底比较，并讨论了独特组件和方法与无监督深度域自适应的联系。UDA，特别是基于深度学习的UDA，在一般转移学习审查中的覆盖面有限。许多先前的领域适应审查没有纳入深度学习方法；然而，基于深度学习的方法已经成为UDA的主流。此外，一些综述没有深入触及域映射[49，118，119]、基于归一化统计[49，119，315，316]、基于集成[49，119270，316]或基于自训练的方法[275]。此外，其中一些只关注有限的应用领域，如视觉数据分析[49202270]或自然语言处理（NLP）[215]。在这篇综述中，我们对这一前景广阔的技术在广泛的应用领域提供了全面的看法，包括自然图像处理、视频分析、NLP、时间序列数据分析、医学图像分析以及气候和地球科学。表1总结了其他审查文件所涉及的主题。\n\n\n\n泛化是观察有限数量的样本并对所有可能的样本进行陈述的过程。在机器学习和模式识别的情况下，样本被用来训练分类器，以对未来的样本进行预测。然而，如果观察到的标记样本不是学习者应该操作的潜在分布的准确反映，系统将不能很好地推广到新样本。在实践中，收集的数据几乎从来不是操作环境的完全公正的表示。\n如果系统地观察到某些结果比均匀随机抽样程序更频繁，则数据是有偏差的。例如，由于当地患者群体的生活条件不同，从一家医院采样的数据可能会相对于全球人群产生偏差。长期以来，统计学家一直在研究样本选择偏差[1]、[2]术语下的抽样偏差。\n修正的基础是估计--或者在有实验设计控制的情况下，知道--选择一个实例进行观察的概率[3]、[4]、[5]。但许多现代数据收集程序，如互联网爬虫，比起从医院患者身上采样，结构化程度较低。因此，即使不是不可能，也很难估计样本被选择进行观察的概率，进而估计有偏差的样本与一般总体的不同之处。另一方面，可能没有必要将其推广到所有人口。更重要的可能是将其推广到特定的目标人群。例如，在欧洲医院收集的数据能否用于培训非洲医院的智能预测系统？\n\n为了针对样本空间（以下称为域）上的特定分布，我们至少需要一些信息。通常可以收集来自目标域的未标记数据，但标签更难获得。\n然而，未标记的数据提供了源域和目标域彼此不同的方式。该信息可用于使分类器适应，即改变其决策，使其更好地向目标领域推广。\n要问的重要问题是：分类器如何从源域学习并推广到目标域？我们将方法分类为三个部分，每个部分都包含更精细的子分类。\n首先，有基于样本的方法，其基于通过单个样本纠正数据采样过程中的偏差。此类别中的方法侧重于数据重要性加权[6]、[7]或类重要性加权[8]。其次，有一些基于特征的方法，它们专注于重塑特征空间，以便在转换的源数据上训练的分类器将推广到目标数据。\n进一步的区别可以在寻找子空间映射[9]、[10]、最优运输技术[11]、学习域不变表示[12]或构造相应的特征[13]中进行。第三，我们考虑我们所称的基于推理的方法。这些方法侧重于将自适应纳入参数估计过程。它是一个多样化的类别，包含算法鲁棒性[14]、极小极大估计量[15]、自学习[16]、经验贝叶斯[17]和PAC贝叶斯[18]。显然，上述分类不一定相互排斥，但我们认为它提供了一个可理解的概述。\n我们的分类揭示了允许域自适应分类器的性能保证的少量条件。在实践中，我们必须假设一个条件成立，这意味着对于任何自适应分类器，都存在一个问题设置，该设置失败。我们讨论了假设检验和因果信息对领域自适应分类器选择的重要性。\n\n\n\n在机器学习和模式识别中，样本被用来训练分类器，以对未来的样本进行预测。然而，如果训练标记样本的分布和测试样本有较大差异，系统将不能很好地推广到新的测试样本。然而实际上，训练数据几乎从来都不能完全涵盖所有的测试场景以及相应的分布。\n\n为了针对目标域上的特定分布，我们至少需要收集一些信息，通常可以收集来自目标域的未标记数据，但标签更难获得。\n\n然而，未标记的数据提供了源域和目标域彼此不同的方式。该信息可用于使分类器适应，即改变其决策，使其更好地向目标领域推广。\n\n要问的重要问题是：分类器如何从源域学习并推广到目标域？我们将方法分类为三个部分，每个部分都包含更精细的子分类。\n\n首先，有基于样本的方法，其基于通过单个样本纠正数据采样过程中的偏差。此类别中的方法侧重于数据重要性加权[6]、[7]或类重要性加权[8]。其次，有一些基于特征的方法，它们专注于重塑特征空间，以便在转换的源数据上训练的分类器将推广到目标数据。\n\n进一步的区别可以在寻找子空间映射[9]、[10]、最优运输技术[11]、学习域不变表示[12]或构造相应的特征[13]中进行。第三，我们考虑我们所称的基于推理的方法。这些方法侧重于将自适应纳入参数估计过程。它是一个多样化的类别，包含算法鲁棒性[14]、极小极大估计量[15]、自学习[16]、经验贝叶斯[17]和PAC贝叶斯[18]。显然，上述分类不一定相互排斥，但我们认为它提供了一个可理解的概述。\n\n我们的分类揭示了允许域自适应分类器的性能保证的少量条件。在实践中，我们必须假设一个条件成立，这意味着对于任何自适应分类器，都存在一个问题设置，该设置失败。我们讨论了假设检验和因果信息对领域自适应分类器选择的重要性。\n\n\n\n近年来提出了许多方法来解决视域自适应问题，该问题也通常被框架为视觉数据集偏差问题[31]。所有人都认识到，源数据表示和目标数据表示的分布发生了变化。事实上，域移动的大小通常由源和目标子空间表示[5，11，21，25，27]之间的距离来衡量。大量方法试图通过学习特征空间变换来对齐源和目标表示[28，23，11，15]来克服这种差异。对于有监督的自适应场景，当目标域中有有限数量的标记数据可用时，已经提出了一些方法来学习对源分类器进行正则化的目标分类器[32，2，1]。其他人试图同时学习特征变换和正则化目标分类器[18，10]。\n最近，基于有监督的CNN的特征表示被证明对于各种视觉识别任务非常有效[22，9，14，29]。特别是，使用深度表示显著减少了分辨率和光照对域移位的影响[9，19]。并行CNN结构，如暹罗网络，已被证明是学习不变表示的有效方法[6，8]。\n然而，训练这些网络需要每个训练实例的标签，因此不清楚如何将这些方法扩展到非监督或半监督设置。还探索了多模式深度学习体系结构，以学习不同输入模式不变的表示[26]。然而，这种方法主要是在生成性的背景下运作的，因此没有充分利用监督的CNN陈述的代表性力量。\n训练源和目标CNN的联合架构是由[7]提出的，但限于两层，因此显著优于使用更深架构的方法[22]，在大型辅助数据源(例如：ImageNet[4])上进行预训练。[13]提出先用去噪自动编码器进行预训练，然后同时训练一个两层网络，同时进行MMD域混淆损失。\n这有效地学习了领域不变表示，但同样，因为学习的网络相对较浅，它缺乏通过使用受监督的深度CNN直接优化分类目标而学习的强语义表示。\n在模型压缩或蒸馏的背景下，在训练期间使用分类器输出分布而不是类别标签已被探索[3，16]。然而，我们是第一个在域自适应设置中应用这一技术的，以便在域之间转移类别相关性。\n其他工作同时探索了直接优化区域不变性表示的想法[12，24]。然而，它们要么使用较弱的域不变性度量，要么使用比我们提出的方法健壮性较差的优化方法，并且它们不试图解决半监督环境下的任务转移问题。\n\n\n\n#### TTT\n\n\n\n在分布变化的情况下，监督学习在泛化方面仍然是出了名的薄弱环节。除非训练和测试数据来自相同的分布，否则即使看似微小的差异也会击败最先进的模型（Recht等人，2018）。对抗鲁棒性和领域适应性只是一些现有的范例，它们试图通过拓扑结构或训练期间可用的测试分布数据来预测训练和测试分布之间的差异。我们探索了一种新的概括方法，它不预测分布变化，而是在测试时从中学习。\n我们从一个简单的观察开始。测试时呈现的未标记测试样本x为我们提供了关于其绘制分布的提示。我们建议通过允许模型参数取决于测试样本x，而不是其未知标签y，利用测试分布的这一提示。可变决策边界（x）的概念在理论上是强大的，因为它脱离了固定模型容量的限制（见第A1节中的附加讨论），但是从x到（x）的反馈机制的设计在实践中提出了新的挑战，我们在这里才开始讨论。\n我们提出的测试时间训练方法基于该单个测试样本x创建了一个自监督学习问题，在进行预测之前在测试时间进行更新。自我监督学习使用一个辅助任务，该任务从未标记的输入中自动创建标签。在我们的实验中，我们使用将每个输入图像旋转90度的倍数并预测其角度的任务（Gidaris等人，2018）。\n这种方法也可以很容易地修改为在标准监督学习环境之外工作。如果一批中有几个测试样本，我们可以使用整个批次进行测试时间培训。如果样本到达在线流，我们通过保持参数的状态来获得进一步的改进。毕竟，预测很少是单一事件。在线版本可以是自然的部署模式，前提是测试样本是由相同或平稳变化的分布变化产生的。\n我们在多个标准基准上的对象识别环境中对我们的方法进行了实验验证。其中包括具有不同级别的不同类型腐败的图像（Hendrycks&amp;Dieterich，2019）、移动对象的视频帧（Shankar等人，2019），以及由收集的一组未知位移的新测试集（Recht等人，2018）。我们的算法在分布变化的情况下进行了实质性改进，同时保持了与原始分布相同的性能。\n在我们的实验中，我们与一个强大的基线（标记为联合训练）进行了比较，该基线在训练时使用监督和自我监督学习，但在测试时保持模型不变。最近的研究表明，训练时间自我监督提高了鲁棒性（Hendrycks等人，2019a）；我们的联合训练基线对应于这项工作的改进实施。第5节对相关工作进行了全面审查。\n我们用第4节中的理论研究补充了实证结果，并在测试时间训练有帮助的凸模型上建立了直观的充分条件；粗略地说，这个条件是两个任务的损失函数之间具有相关的梯度。\n\n#### TTA：\n\n测试时间适应（TTA）测试时间适应方法在部署期间利用当前可用的测试数据来适应预先训练的模型。由于测试样本还提供了分布变化的见解，[38]表明，在测试期间简单地调整批处理归一化（BN）统计数据已经可以显著提高损坏数据的性能。这与[20]的精神一致，[20]先前建议在UDA的设置中更新BN统计数据。虽然该策略只需要前向传递，但TTA中的当前方法进一步执行后向传递，其中更新了模型权重。例如，[49]使BN参数的熵最小化。[57]最小化了所有参数的熵，并使用测试时间增加[16]来人为地增加批量大小。其他方法应用对比学习[4]，甚至在源预训练期间引入额外的自我监督损失，该损失随后被用于在测试期间执行自适应[1，2，24，41]。多样性正则化[21，33]是一种最近的方法，可以避免由置信最大化可能导致的平凡解的崩溃。虽然许多方法都假设有一批可用的测试数据，但其中一项工作集中于单个样本TTA[2，10，31，57]\n\n\n\n\n\n\n\n"},{"url":"/2022/12/11/计算机视觉/test_time_adaptation/TENT/","content":"\n\n## TENT: FULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION\n\n### INTRODUCTION\n\n在测试过程中，模型必须适应新的和不同的数据。\n在这种完全测试时间自适应的设置中，模型只有测试数据和自己的参数。我们建议通过测试熵最小化（**test entropy minimization - tent**）进行调整：我们优化了模型的置信度，通过其预测的熵来衡量。\n\n深度网络可以在相同分布的训练和测试数据上实现高精度，然而，对新的和不同的数据的概括是有限的。当训练（源）数据与测试（目标）数据不同时，准确性会受到影响，这种情况被称为**数据集偏移**。模型在测试过程中可能对训练过程中未知的变化敏感，无论是自然变化还是腐蚀，如意外天气或传感器退化。然而，可能需要在不同的数据分布上部署模型，因此需要进行调整（**adaptation**）。\n在测试期间，模型必须仅根据其参数和目标数据进行调整。这种完全测试时间自适应设置不能依赖于源数据或监督。因为源数据可能很庞大且无法及时获取当前测试数据的标签信息。\n\n数据量、计算量和任务需求这些实际问题引出了测试时间适应性的重要性：\n\n1.可用性。为了带宽、隐私或利润，模型可能在没有源数据的情况下分发。\n2.效率。在测试期间（重新）处理源数据在计算上可能不实用。\n3.准确性。如果不进行调整，模型可能太不准确，无法达到其目的。\n\n为了在测试期间进行调整，我们将模型预测的熵最小化。我们将这个目标称为测试熵，并以其命名我们的方法tent。我们选择熵作为它与误差和偏移（domain shift）的联系。熵与误差有关，因为更有信心的预测更准确（图1）。\n熵与损坏导致的变化有关，因为损坏越多，熵就越多，随着损坏程度的增加，熵与图像分类的损失有很强的等级相关性（图2）。\n\n**我们的贡献：**\n\n•我们强调了只使用目标数据而不使用源数据的完全测试时间自适应设置。为了强调推理过程中的实际适应性，我们使用离线和在线更新进行基准测试。\n•我们将熵作为自适应目标，并提出了tent：一种测试时间熵最小化方案，通过减少测试数据上模型预测的熵来减少泛化误差。\n•对于损坏的鲁棒性，在ImageNet-C上，tent达到44:0%的错误，优于鲁棒训练的现状（50:2%）和测试时间标准化的强大基线（49:9%）。\n•对于领域适应，tent能够在线和无源地适应数字分类和语义分割，甚至可以与使用源数据和更多优化的方法相媲美。\n\n### SETTING: FULLY TEST-TIME ADAPTATION\n\n适应解决了从源到目标的泛化问题。具有在源数据和标签xs上训练的参数的模型f（x）；当对移位的目标数据xt进行测试时，ys可能不会泛化。表1总结了自适应设置、所需数据和损失类型。**我们的完全测试时间自适应设置仅需要模型参数f和未标记的目标数据xt**，以便在推理过程中进行自适应。\n\n![image-20221117152108373](Untitled/image-20221117152108373.png)\n\n现有的适应设置扩展了训练过程，使用了更多的数据和监督。通过微调进行转移学习（Donahue等人，2014；Yosinski等人，2014）需要目标标签来（重新）训练监督损失L（xt；yt）。如果没有目标标签，就无法进行这种有监督的训练。域自适应（DA）（Quionero Candela等人，2009年；Saenko等人，2010年；Ganin&amp;Lempitsky，2015年；Tzeng等人，2015年）需要源数据和目标数据来训练跨域损失L（xs；xt）。\n测试时间训练（TTT）（Sun等人，2019b）在测试期间进行调整，但首先改变训练以联合优化其监督损失L（xs；ys）和自我监督损失L（xs）。如果没有source data，将无法使用跨域联合训练（DA）或损失（TTT）。\n测试期间意外的目标数据需要测试时间调整（TTA）。TTT和我们的设置通过**优化测试L（xt）期间的无监督损失**来调整模型。在训练期间，TTT**联合优化了源数据L（xs）上的无监督损失和监督损失L（xs；ys），以确保在损失之间共享参数**，以与L（xt）的自适应兼容。）（测试时间训练（TTT）联合训练目标上的受监督和自监督任务，然后在测试过程中继续训练目标上自监督任务）。\n\n完全测试时间自适应与给定参数的训练数据和训练损失无关。通过不改变训练，我们的设置有可能需要更少的数据和计算来适应。\n\n\n\n### METHOD: TEST ENTROPY MINIMIZATION VIA FEATURE MODULATION\n\n![image-20221117154043795](Untitled/image-20221117154043795.png)\n\n*方法概述。Tent不改变训练阶段（a），但在给定参数和目标数据xt的约束调制下，在测试（b）期间最小化预测的熵*\n\n模型要求：针对的是监督任务，所以模型需要经过有监督的训练；模型必须是概率模型，因为需要计算预测的熵；快速迭代优化需要梯度，所以模型必须可微。\n\n#### ENTROPY OBJECTIVE\n\n我们的测试时间目标L（xt）是最小化模型预测的熵H（^y）^y=f（xt）。特别地，我们测量了香农熵（香农，1948），<img src=\"Untitled/image-20221117160701962.png\" alt=\"image-20221117160701962\" style=\"zoom: 33%;\" />。注意，优化单个预测有一个退化解：即将所有概率分配给最可能的类。我们通过在批处理中共享的参数上联合优化批处理预测来防止这种情况。**（优化单个预测会得到退化解，因此联合优化一个批的预测概率）**\n**熵是一个无监督的目标，因为它只依赖于预测而不是注释。然而，作为预测的度量，它与监督任务和模型直接相关**。\n相比之下，用于自我监督学习的代理任务与监督任务没有直接关系。\n代理任务从没有任务标签y的输入xt导出自监督标签y0。这些代理的示例包括旋转预测（Gidaris等人，2018）、上下文预测（Doersch等人，2015）和跨信道自动编码（Zhang等人，2017）。代理任务上的过多进展可能会干扰受监督任务的性能，自我监督的自适应方法必须相应地限制或混合更新（Sun等人，2019b；a）。因此，需要谨慎地选择与域和任务兼容的代理，为代理模型设计架构，并平衡任务和代理目标之间的优化。我们的熵目标不需要这样的努力**（自监督方法选择一个域与任务相兼容的proxy很重要）**\n\n#### MODULATION PARAMETERS\n\n模型参数是测试时间优化的自然选择，这些是列车时间熵最小化的先前工作的选择（Grand代客和Bengio，2005；Dhillon等人，2020；Carlucci等人，2017）。然而，模型参数是我们设置中训练/源数据的唯一表示，改变可能会导致模型偏离其训练。此外，f可以是非线性的，并且可以是高维的，使得优化对于测试时间的使用过于**敏感**和低效。\n\n![image-20221117154513119](Untitled/image-20221117154513119.png)\n\n*图4：Tent通过估计归一化统计数据来调制测试期间的特征；优化变换参数；\f. 标准化和变换将按通道缩放和移动应用于特征。在不使用源数据的情况下，根据目标数据更新统计信息和参数。在实践中，适应；是有效的，因为它们占模型参数的&lt;1%*\n\n\n\n为了稳定性和效率，我们只更新线性（尺度和位移）和低维（信道方向）的特征调制。图4显示了我们调制的两个步骤：统计归一化和参数转换。标准化将输入x集中并标准化\n\n![image-20221117162250401](Untitled/image-20221117162250401.png)\n\n为了实现，我们只需重新调整源模型的**normalization layers**的用途。我们在测试期间更新了所有层和通道的归一化统计和仿射参数。\n\n#### ALGORITHM\n\n**初始化** 优化器收集源模型中每个归一化层l和通道k的仿射变换参数{γl，k，βl，k}。其余参数θ｛γl，k，βl，k｝是固定的。源数据的归一化统计{μl，k，σl，k}被丢弃。\n**迭代 **每个步骤都会更新一批数据的规范化统计信息和转换参数。在正向过程中，依次估计每个层的归一化统计。在反向传播过程中，通过预测熵的梯度更新变换参数γ、β。变换参数更新遵循当前批次的预测，因此它只影响下一批次（除非重复正向）。这只需要每个额外计算点一个梯度，因此我们默认使用此方案以提高效率。\n**终止 **对于在线适应，不需要终止，只要有测试数据，迭代就会继续。对于离线适应，首先更新模型，然后重复推理。\n当然，适应可以通过多个时期的更新来继续\n\n### EXPERIMENTS\n\n#### ROBUSTNESS TO CORRUPTIONS\n\n为了对损坏的健壮性进行基准测试，我们使用了常见的图像损坏（示例参见附录A）。通过复制CIFAR-10\\/100和ImageNet数据集的**测试/验证集**，并在五个严重级别应用15种类型的损坏，将CIFAR-10/100-C和ImageNet-C数据集转换为CIFAR-10/10-C和ImageNet C损坏基准。\n\n<img src=\"TTA调研/image-20221118152903397.png\" alt=\"image-20221118152903397\" style=\"zoom:43%;\" />\n\nTent在不增加原始数据错误的情况下达到了大多数损坏类型的最小错误。（original的错误率基本没变）\n\n<img src=\"TTA调研/image-20221118153002103.png\" alt=\"image-20221118153002103\" style=\"zoom:50%;\" />\n\n\n\n我们的网络配备了批量标准化（Ioffe&amp;Szegedy，2015）。**对于没有自适应的源模型，在对源数据进行训练期间估计归一化统计**。对于所有测试时间自适应方法，我们在对目标数据进行测试期间估计这些统计数据，正如在标准化自适应的并行工作中所做的那样（Schneider等人，2020；Nado等人，2020）。\n\n测试时间归一化（BN）更新测试期间目标数据的批次归一化统计\n\n（BN) updates batch normalization statistics \n\n<img src=\"Untitled/image-20221117171421188.png\" alt=\"image-20221117171421188\" style=\"zoom: 50%;\" />\n\n### RELATED WORK\n\n我们涉及现有的自适应、熵最小化和特征调制方法。\n训练时间自适应域自适应通过跨域损失L（xs；xt）在源和目标上联合优化，以减轻偏移。这些损失优化了特征对齐（Gretton等人，2009；Sun等人，2017）、对抗不变性（Ganin&amp;Lempitsky，2015；Tzeng等人，2017年）或共享代理任务（Sun等人，2019a）。转导（Gammerman等人，1998年；Joachims，1999年；Zhou等人，2004年）联合优化列车和测试，以更好地适应特定测试实例。尽管在其设置中有效，但当源/训练和目标/测试的联合使用被拒绝时，两者都不适用。帐篷只能适应目标。\n最近的“无源”方法（Li等人，2020；Kundu等人，2020年；Liang等人，2020）也适用于无源数据的情况。Li等人（2020）；Kundu等人（2020）依赖于生成建模，并优化具有多重损失的多个模型。Kundu等人（2020）；Liang等人（2020）也改变了训练。Tent不需要生成建模，也不改变训练，因此它可以更广泛地部署，以更高的计算效率在线适应。SHOT（Liang等人，2020）通过信息最大化（熵最小化和多样性正则化）进行调整，但在其他损失和参数化方面有所不同。这些无源方法在多个时间段内以多个损失离线优化，这需要比帐篷更多的调整和计算，但可以通过更多的计算实现更高的精度。Tent在线优化，仅需一次损失和有效的调制参数化，以强调推理过程中的充分测试时间自适应。我们鼓励在没有原始数据的情况下，在适应的前沿对这些作品进行审查。\nChidlovskii等人（2016）是第一个出**于法律、商业或技术考虑**而在没有源数据的情况下激励适应的人。他们通过应用去噪自动编码器来调整预测，而我们通过熵最小化来调整模型。我们分享他们的动机，但方法和实验不同。\n测试时间适配帐篷通过测试时间优化和标准化进行适配以更新模型。\n预测的测试时间适应，通过这种适应，基于更简单和特定的情况来调整更困难和不确定的情况（Jain&amp;Learned Miller，2011），为我们自己的基于确定性的模型适应方案提供了灵感。\n测试时间训练（TTT）（Sun等人，2019b）也在测试期间优化，但其损失不同，必须改变训练。TTT依赖于代理任务，例如识别图像的旋转，因此其损失取决于代理的选择。（事实上，其作者警告说，代理必须“在新域中既定义明确又不平凡”）。TTT改变训练以在适应目标之前优化源上的代理丢失。帐篷无需代理任务，也无需改变训练即可适应。\n标准化特征统计对于域自适应是常见的（Gretton等人，2009；Sun等人，2017）。\n对于批量归一化，Li等人（2017）；Carlucci等人（2017）在培训期间分离了来源和目标统计数据。Schneider等人（2020）；Nado等人（2020）在测试期间估计目标统计，以提高泛化能力。Tent建立在测试时间标准化的基础上，以进一步减少泛化误差。\n熵最小化熵最小化是域自适应（Carlucci等人，2017；Shu等人，2018；Saito等人，2019；Roy等人，2019）、半监督学习（Grand代客和Bengio，2005；Lee，2013；Berhelot等人，2019年）和少镜头学习（Dhillon等人，2020）的关键正则化器。\n正则化熵惩罚数据分布中高密度的决策，以提高不同类别的准确性（Grand代客和Bengio，2005）。这些方法将训练期间的熵与其他有监督和无监督的额外数据损失协调起来。帐篷是第一个将8\n\n### DISCUSSION\n\nTent通过测试时间熵最小化来减少移位数据的泛化误差。在最小化熵的过程中，模型会根据自身预测的反馈进行调整。这是真正自我监督的自我提升。这种类型的自我监督完全由受监督的任务定义，与设计用于从数据中提取更多监督的代理任务不同，但它仍然显著地减少了错误。\n尽管如此，由于腐败和其他变化导致的错误仍然存在，因此需要进行更多的调整。下一步应通过更有效和更有效的损失，在更一般的参数上，对更多更难的换档类型进行测试时间调整。\nShifts Tent减少了各种移位的误差，包括图像损坏、数字外观的简单变化以及模拟真实差异。这些变化作为标准化基准很受欢迎，但其他现实世界的变化也存在。例如，通过复制数据集收集过程制作的CIFAR 10.1和ImageNetV2测试集（Recht等人，2018；2019）包含自然但未知的变化。尽管这两个集合的误差都较高，表明存在移位，但帐篷并不能改善泛化。对抗性转变（Szegedy等人，2014）也威胁着现实世界的使用，攻击者不断适应防御。尽管对抗性训练（Madry et al.，2018）有所不同，但测试时间适应可以帮助对抗此类测试时间攻击。\n参数Tent通过标准化和变换来调节模型，但大部分模型保持不变。测试时间调整可能会更新更多的模型，但问题是要确定既有表现力又可靠的参数，这可能会与损失的选择相互影响。TTT适应受监督和自我监督模型共享的多个特征层，SHOT适应模型的除最后一层以外的所有层。这些选择取决于模型架构、损失和调优。对于帐篷调制是可靠的，但通过SHOT参数化可以更好地解决VisDA上的较大偏移。联合调整投入可能是一种更普遍的选择。如果一个模型能够适应目标，那么它的输入梯度可能会优化空间转换或图像转换，以减少没有源数据的移动。\n损失帐篷使熵最小化。对于更多的适应，一般的但不定期的测试时间优化是否存在有效的损失？熵在任务中是通用的，但范围有限。**它需要批量优化，并且不能一次在一个点上偶尔更新**。TTT可以这样做，但必须使用正确的代理任务。对于较少的计算，是否存在更多局部优化的有效损失？Tent和TTT都需要对模型进行完整（重新）计算以进行更新，因为它们依赖于模型的预测。如果损失是在表示上定义的，那么更新将需要更少的正向和反向计算。具体回到熵，这种损失可能与校准相互作用（Guo等人，2017），因为更好的不确定性估计可以推动更好的适应。\n我们希望，完全测试时间自适应设置可以促进新的方法，使模型能够自适应，就像帐篷每次更新都会生成新模型一样\n\n\n\n\n\n## 实验设置\n\nWe evaluate tent for **corruption robustness on CIFAR-10/CIFAR-100 and ImageNet**, and for **domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS**\n\nFor **corruption** we use residual networks (He et al., 2016) with 26 layers (**R-26**) on CIFAR10/100 and 50 layers (**R-50**) on ImageNet. For domain adaptation we use the **R-26** architecture. For fair comparison, all methods in each experimental condition share the same architecture.\n\nOn ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam\n\nOn ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.\n\nBaselines We compare to **domain adaptation, self-supervision, normalization, and pseudo-labeling**:\n\n- source\n\n- adversarial domain adaptation (RG):\n\n\t- 论文题目：Unsupervised Domain Adaptation by Backpropagation（2015）\n\t- ![image-20221204190413618](TENT/image-20221204190413618.png)\n\t- 需要修改训练流程且无法在测试时更新\n\n- self-supervised domain adaptation (UDA-SS):\n\n  - 联合训练源和目标上的自监督旋转和定位任务，以优化共享表示，需要**同时有源数据和测试数据**\n  - 论文题目：Unsupervised domain adaptation through self-supervision（2019）\n  - 本文讨论了无监督域自适应，即源域上有标记的训练数据，但目标是在目标域上只有未标记数据，希望能够通过一些方法将源域上分类器迁移到目标域上，并能在目标域上有良好的性能。与之前的许多工作一样，我们试图在保持可辨别性的同时，对源域和目标域的学习表示进行对齐。我们实现对齐的方法是同时在两个领域学习**辅助自监督任务**。每一个自监督的任务都使这两个领域沿着与该任务相关的方向走得更近。在源域上与主任务分类器联合训练，可以成功地推广到未标记的目标域。\n  - 代理任务的选择：\n    - 旋转预测：输入图像依次旋转90，180，270度，该任务是该任务是预测旋转角度作为一个四维分类问题。\n    \t翻转预测：输入图像垂直随机翻转;任务是预测它是否翻转。\n    \t贴片位置预测：从输入图像中随机裁剪出小块;任务是预测这些补丁来自哪里。\n\n- test-time training（TTT）:\n\n\t- 在源端对有监督和自监督任务进行联合训练，在测试时继续对目标端自监督任务进行训练\n\t- 论文题目：Test-Time Training with Self-Supervision for Generalization under Distribution Shifts（2020）\n\t- 无监督域自适应研究的是当训练时除了来自训练分布(源域)的有标签数据集外，还有来自测试分布(目标域)的无标签数据集时，分布偏移的问题。然而，**问题设置的局限性在于，泛化性可能只针对这个特定的测试分布而得到改善**，这可能很难提前预料。Test-Time Training不预期任何测试分布，通过改变无监督域适应的设置，同时从其算法中得到启发。我们的论文是对Sun et al . ( 2019 )的一个后续，我们对此进行了解释和实证\n\t\n- test-time normalization (BN)（2020）：\n\n\t- 根据测试数据的分布，动量式的更新BN层的统计数据（均值和方差），能够实现单样本输入\n\n- pseudo-labeling (PL)（2013）：\n\n\t- 调整一个置信度阈值，将超过该阈值的预测赋值为标签，然后在测试之前将模型优化到这些伪标注。\n\t- 在给无标签样本添加标签后，然后选择置信度高的样本扩充到有标签的样本中，再训练模型，不断的更新模型\n\n\t\n\n\t注：**只有BN、PL、TENT算TTA**\n\n\t\n\n### ROBUSTNESS TO CORRUPTIONS\n\n为了衡量腐败的稳健性，我们使用常见的图像腐败(见附录A)。通过复制CIFAR - 10 / 100和ImageNet数据集的测试/验证集，并在5个严重等级( Hendrycks & Dietterich , 2019)上应用15种腐败类型，将CIFAR - 10 / 100 - C和ImageNet - C数据集转换为腐败基准测试集。\n\n\n\n不同程度的不同类型的损坏  每次都会reset model\n\nmodel：resnet 26\n\n难以完全复现原论文数据，需要看看其他论文里面是如何是如何实现的\n\n\n\n\n\n#### github issue\n\n- tent有时会随着训练epoch的轮数增加而效果下降[Performance on CIFAR-10-C deteriorates with more epochs · Issue #7 · DequanWang/tent (github.com)](https://github.com/DequanWang/tent/issues/7) \n\t- 测试时间适应方法如何决定**何时更新或不更新**，特别是何时停止？Tent 在这方面很简单，并且总是在每批上更新，但也许更复杂的规则可以在结果恶化之前停止优化。\n\n"},{"url":"/2022/11/30/类脑实验室/开题答辩记录/","content":"我的：\n\n具体应用：用到白天黑夜\n\n后续调研，整理现有的方法\n\n在大规模数据集上进行测试\n\n\n\n张艺：\n\n\n\n\n\n\n\n框图集中点，其实只研究了其中一部分\n\n\n\n之前做的都是局部避障，用在建图中可能需要很大的修改，难以直接应用，后面还需要考虑怎么能够更加好落地\n\n\n\n比较方面ans 需要更多的调研，主动建图方面还需要更多的调研，需要找到更多方法对比，排除不行的方案\n\n\n\n文：\n\n强化学习建图和之前的传统方法不同\n\n\n\n刘：\n\n开题的题目是什么\n\n主动和自主的区别？\n\n前提条件是啥？我们系统运行的限定是什么？\n\n初始条件是什么？是随便扔到哪都行   还是说有一个目标     未知起点的基于拓扑地图？  \n\n​\t\t\t（希望没有任何先验知识）\n\n探索能力怎么评估？怎么知道探索能力强不强\n\n​\t（短路程，短时间，误差阈值，避免碰撞）\n\n结论非常好，少了时效性\n\n题目可以构成一个博士生论文，和小马哥的研究有什么区别？\n\n你的目标等于主要是构建地图\n\nans的指标是你对标将要完成的？\n\n要明确我们要实现的指标，需要定量分析，\n\n如果你有别人没有，那么证明我们还是蛮新的\n\n能够体现自主的方法就只有通过强化学习吗？\n\n（当然还有其他算法，比如传统启发式的搜索，但是强化学习有一些优势）\n\n我们的自主是通过什么来实现\n\n（就是强化学习）\n\n解决强化学习的什么问题能够提高自主性？\n\n对方法本身没有创新吗？\n\n加哪些辅助模块提升探索能力？\n\n（小马哥帮忙回答一下：1.外在奖励驱动，2. 不确定估计，求方差，求熵的方法，这些方法在固定场景效果好，如遥感导航）\n\n像小马哥这种解释就比较具体\n\n你这里写的问题都很抽象  不够充分   要解决什么问题就能提升鲁棒性  \n\n前面说探索能力低，我就要提升探索能力，这说了等于没说\n\n每个人家方法的具体方法哪里解决的不好，你要去梳理一下，我们针对这些问题去解决，梳理一下，有助于写论文\n\n你的应用场景肯定是我们的机器人，是要满足室内场景还是室外？\n\n可以先是室内，最终的应用场景应该是室外\n\n应老师：如果能超过室内的扫地机器人，那就可以，如果不能超过还是要室外。室内场景很简单，用一些启发式算法就可以了，室外考虑的因素会更多，室外如果用启发式算法要建立复杂的评价模型，而强化学习能够比用启发式算法更好\n\n室内我直接用一个cad地图就可以解决了\n\n仿真场景也要说清楚，你这个用的是什么数据集？kitti是固定路线， 你可以用街景路线，\n\n最后实验场景是啥要清楚，仿真场景是什么，实际的应用场景\n\n最终我们要对标的方法，指标定义要清楚，我们可以和sota一样，但是其他方面可能要更好\n\n\n\n\n\n总结：\n\n要知道本领域大家都做了什么，\n\n要认真填写excel表，特别推荐，大家互相交流\n\ntent出来就有上百篇引用，要整理总结，要看现在的sota到了什么水平？\n\n综述体现了对系统的了解\n\n文：提高效率，要全面，要去追踪，不能仅靠老师，还是要靠自己，要解决哪些问题，我们能不能提出新的？追踪最新进展，在这之上再去进行下一步工作。\n\n至少一星期能看多少篇？集中调研应该不只一周五篇\n\n"},{"url":"/2022/11/30/计算机视觉/test_time_adaptation/Low Dimensional Trajectory Hypothesis is True/","content":"## Low Dimensional Trajectory Hypothesis is True:DNNs can be Trained in Tiny Subspaces\n\n[用最直观的方式告诉你：什么是主成分分析PCA_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1E5411E71z/?spm_id_from=333.337.search-card.all.click&vd_source=0bb4d13f4bd26eaf7f64c37f29dc9f27)\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221117201516681.png\" alt=\"image-20221117201516681\" style=\"zoom:33%;\" />\n\n[【学长小课堂】什么是奇异值分解SVD--SVD如何分解时空矩阵_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV16A411T7zX/?spm_id_from=333.788&vd_source=0bb4d13f4bd26eaf7f64c37f29dc9f27)\n\n![image-20221117201821493](Low Dimensional Trajectory Hypothesis is True/image-20221117201821493.png)\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221117203502974.png\" alt=\"image-20221117203502974\" style=\"zoom:33%;\" />\n\n**摘要：**深度神经网络（DNN）通常包含大量参数，但存在冗余，因此可以猜测它们可以在低维子空间中训练。在本文中，我们提出了一种基于训练轨迹的低维特性的动态线性降维（DLDR）。简化方法是有效的，并得到了综合实验的支持：在40维空间中优化DNN可以在数千甚至数百万个参数上实现与常规训练相当的性能。由于只有几个变量需要优化，我们开发了一种有效的基于准牛顿的算法，获得了对标记噪声的鲁棒性，并提高了训练有素的模型的性能，三个后续实验，可以显示找到这种低维子空间的优势\n\n### INTRODUCTION\n\n深度神经网络（DNN）在各个领域取得了前所未有的成功[1]，[2]。在DNN中，参数的数量通常非常大，例如，VGG11[3]中的28.5M，MobileNet[4]中的3.3M，Xception[5]中的21.0M。然而，简单地将DNN的每个参数视为独立变量太粗糙了。事实上，这些参数具有很强的相互关系。例如，梯度从深层传播到浅层，因此不同层之间的参数梯度密切相关。同一层中的参数也具有协同相关性。因此，独立优化变量的数量可能没有我们想象的那么多。换句话说，DNN似乎可以在相对低维的子空间中得到很好的训练，正如[6]首次提出的那样。\n简而言之，DNN参数在训练中的依赖性和冗余性可以正式描述为以下假设。\n\n**低维轨迹假说**\n对于具有n个参数的神经网络，参数在训练过程中的轨迹可以近似地被具有d维的空间覆盖d≪ n。如果这一假设成立，那么学习在实践和理论方面都会有很大的好处。\n假说是否成立的最佳标准是在这样的低维空间中的优化是否可以实现与优化原始空间中的所有参数相同或相似的性能。\n在开创性工作[7]中，作者将SGD训练在全参数上的90%准确度设置为标准，并发现所需的内在维度远小于参数的数量。例如，在CIFAR-10[8]上，具有62006个参数的LeNet[9]可以在2900维子空间中进行优化，获得的精度为常规训练的90%。尽管通过随机投影提取子空间的方法是初步的，但其性能非常有希望。随后，[10]考虑了网络的不同部分，并在每一步重新绘制随机基，进一步将所需维度减少到数百，但精度降级仍然类似。\n但现有的工作部分不完全支持低维轨迹假设，因为与常规的全参数训练相比仍有很大差距。在本文中，我们建议通过分析动态轨迹来提取子空间，而不是[7]，[10]中的随机投影。通过所提出的方法，许多标准的神经网络结构可以**仅由40个独立变量很好地训练，并且性能几乎与全参数上的常规训练相同**，这表明**DNN可以在低维子空间中训练**，我们确实可以有效地找到这样的子空间。\n\n为了直观地说明假设和我们的目标，我们可以考虑参数为w∈Rn的DNN f(x，w)。\n它的训练序列，即训练轨迹，可以表示为{wi}i=0，…，t，其中wi是指训练步骤i的w值。这个假设意味着我们可以找到一个子空间(实际上，它是一个仿射集，但我们稍后将进行集中化；因此，我们在本文中没有严格区分这两个概念)来近似覆盖优化轨迹{wi}i=0，...，t。这种现象与神经切核(NTK)[11]，[12]的低阶性质和顶子空间[6]的集中梯度有关，这将在3.1节中讨论。\n请注意，提取子空间，即**寻找独立变量（自变量），不同于在模型简化方法中选择参数**，例如参见[13]、[14]。考虑图1中的一个玩具示例，其中包含三个需要优化的变量。如图所示，优化轨迹在e1和e2跨越的子空间中，即优化轨迹的维度为2，但没有单个参数可以减少。这个简单的例子显示了我们的关注点：我们的目标是找到合适的参数组合来构造低维子空间中的自变量。\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221117210137300.png\" alt=\"image-20221117210137300\" style=\"zoom:33%;\" />\n\n*图1：有三个参数w(1)、w(2)、w(3)需要优化。*\n*但是训练轨迹{wi}i=0，...，t可以在由e1和e2跨越的二维子空间中。如果是这样的话，在低维空间中的训练可以具有与在高维空间中的训练相当的性能。*\n\n为了找到覆盖训练轨迹的子空间，人们应该关注训练轨迹。因此，我们将所提出的子空间提取方法命名为动态线性降维(DLDR)。利用DLDR得到的独立变量（自变量），我们可以很好地刻画只有几个基(自变量)的子空间中的训练轨迹。在第五节中，大量的数值实验将表明，许多标准的DNN结构只需40个自变量就可以很好地进行训练。同时，测试精度可以保持与在原始空间中对全参数进行常规训练几乎相同的精度。\n从理论上讲，即使在训练集不是很大的情况下，将数百万个参数减少到几个自变量就可以解释DNN良好的泛化性能。在实际应用中，由于只需要对几个自变量进行优化，我们可以**使用二阶方法而不是一阶优化方法（?）**，如SGD[15]，以克服一些固有的缺点，如尺度敏感性和收敛速度慢。在现有的工作中，引入了**动量等部分二阶信息**，极大地提高了DNN的性能，导致了目前流行的自适应策略，如Adam[16]，RMSprop[17]等。由于DLDR发现的低维子空间，拟牛顿方法，如DFP和BFGS[18]，[19]，变得适用于DNN的训练。本文提出了一种投影子空间上的BFGS算法，称为P-BFGS算法，**比SGD算法节省了约30%的时间**。\n低维轨迹假设的另一个后续应用是**提高模型对标签噪声的稳健性**。由于DNN是在过度参数化的制度下工作，它们可以很容易地适应任何标签，甚至是不正确或无意义的标签[20]。因此，当训练标签被噪声破坏时，DNN很容易被破坏。现在，由于我们已经发现了非常低维的子空间，随着训练自由度的显著降低，在这样的子空间中训练DNN有望对标签噪声具有更强的鲁棒性。在5.4节中，我们将发现，在没有任何其他稳健性增强技术的情况下，低维子空间的训练在CIFAR-10上可以达到**50%以上的测试精度，即使90%的训练标签是随机设置的**。\n本文最重要的贡献是验证了低维轨迹假设，这表明在低维子空间中优化DNN可以在所有参数上获得与训练相似的性能。\n具体贡献包括：子空间提取方法和提取低维子空间的三个后续优势：\n\n·动态线性降维技术，以有效地找到低维子空间；\n\n·通过在低维子空间中使用基于准牛顿的算法来节省训练时间；\n\n·通过在低维子空间中进行训练来对标签噪声具有鲁棒性；\n\n·通过子空间训练来提高训练有素的模型的性能。\n本文的其余部分组织如下。我们首先回顾了第二节中的相关工作。然后在第三节中提出并验证了低维轨迹假设的DLDR算法。在第四节中，我们设计了一个基于DLDR的拟牛顿算法。然后在第五节中我们对降维性能进行了评估。第六节进行了简要的讨论。代码已发布1https://github.com/nblt/DLDR。\n\n### RELATED WORKS\n\n分析和理解DNN优化目标的前景非常重要。例如，Li等人[21]使用一系列可视化方法可视化DNN的损失情况。他等人[22]观察到，在局部最优点存在许多不对称方向，沿着这些方向，损耗在一侧急剧增加，在另一侧缓慢增加。一个重要方面是衡量DNN景观的内在维度。在开创性工作[7]中，发现使用随机投影，在缩减子空间中的优化可以达到常规SGD训练的90%性能。在此基础上，提出了内在维数远小于参数数量的观点。\n下面的工作[10]通过考虑网络的不同部分并在每个步骤重新绘制随机基来提高随机基训练性能。\n与之前的工作不同，我们通过分析DNN的训练动态来提取子空间，然后得到了显著的改进：固有维数降低了一个数量级，并且精度提高到与常规训练几乎相同。\n验证低维轨迹假设并找到可以很好地训练DNN的微小子空间非常重要，不仅对于理解DNN的学习，而且对于设计强大的优化方法也非常重要。这与[6]中的发现一致，即在短时间的训练之后，DNN的梯度可以收敛到一个非常小的子空间，该子空间由Hessian矩阵的几个顶部特征向量覆盖。在实践中，低维轨迹假设可能会激发更强大的优化方法，并带来更多潜力来克服学习中的一些现有障碍。由于在极小的子空间中，优化变量的数量大大减少，因此可以以相对简单的方式利用高阶信息。在[23]、[24]等中，设计了精细的方法来使用曲率信息，同时保持计算效率，然而，这是一个不相容的矛盾，除非可以有效减少优化变量的数量。另一个密切相关的方向是低级训练[25]，[26]或更新[27]，[28]。例如，LoRA[28]通过学习低秩参数矩阵来更新预训练的模型，提高了微调大型语言模型的效率。虽然这些方法侧重于参数矩阵的静态低秩结构或它们的自适应，但我们利用了训练动力学的低秩特性，并且是这些方法的补充，这可以带来进一步的可能效率\n\n### DYNAMIC LINEAR DIMENSIONALITY REDUCTION\n\n#### Motivation\n\n**本文的主要目标是指出并验证DNN中参数的自由度相当低**。这可以简洁地表示为参数的轨迹在低维空间中的假设。\n在一项开创性的工作[6]中，作者发现在低维随机选择的空间中进行训练仍然可以产生有意义的神经网络。具体而言，原始空间中的参数w∈ Rn在d维空间v∈ Rd中更新:\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118101647594.png\" alt=\"image-20221118101647594\" style=\"zoom:35%;\" />\n\n其中P∈ Rn×d是一个随机逆投影矩阵。很明显，（1）中只有d个自由度。如[23]中的数值验证，即使在d≪ n、 （1）可以输出有意义的结果。这里，我们说“有意义”是指**性能比初始化好得多，但比完全训练后的网络效果更差。**[10]中通过用启发式方法替换随机投影，提高了性能。在这些低维空间中的训练性能可以在表1中观察到，表1部分但不完全支持DNN可以在低维空间训练的假设。\n为了研究训练轨迹和低维景观，我们可以使用神经切线核（NTK），它理想地将单输出神经网络的梯度流公式化如下\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118102107485.png\" alt=\"image-20221118102107485\" style=\"zoom:33%;\" />\n\n（公式看不懂。。。先跳过）\n\n基于上述有趣的观察，我们正式提出了第1节中给出的**低维轨迹假设说**。数学上，对于参数的轨迹w0，w1，…，wt，…，低维轨迹假说声称存在低维轨迹v1，v2，…，vt，…和投影P∈ Rn×d使得\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118102632722.png\" alt=\"image-20221118102632722\" style=\"zoom:33%;\" />\n\n该假设侧重于神经网络的训练，其验证是直接的，即，**如果神经网络可以在d维空间中很好地训练，那么低维轨迹假设是正确的。**\n\n如前所述，现有的工作现在只能部分验证该假设，因为在他们提取的子空间上进行训练，神经网络可以被训练为有意义的解决方案，但结果仍然比在整个参数空间上的训练差得多。在下文中，我们将提出一种新的方法来找到神经网络可以很好地训练的低维空间。\n该结果验证了低维轨迹假设，有望为神经网络带来新的理解，例如泛化能力、隐式正则化、高效学习算法等。\n\n#### Methodology\n\n降维的关键问题是找到近似覆盖参数轨迹的低维子空间。**代替处理连续轨迹，我们实际上使用其离散化，即离散采样点来表征轨迹**。基本操作包括:\n\n- 首先，对训练过程中神经网络参数的t步进行采样，即{w1，w2，.。。。，wt}。\n- 第二，中心化样本，即求样本集参数的均值，并且每个参数减去均值得到W\n- 第三，找出由d维子空间P=[e1，e2，.。。。，ed]来涵盖W.请注意，在DNN中，参数n的数目通常显著大于t和d。\n\n第三步是找到一个子空间，该子空间最小化从W的列向量到该子空间的距离之和。利用l2范数，可以将其公式化为最大化W的投影方差，即。\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118112111041.png\" alt=\"image-20221118112111041\" style=\"zoom:33%;\" />\n\n这是一个标准的PCA问题，但计算维度太大，可以使用SVD分解来减少计算量：\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118112323474.png\" alt=\"image-20221118112323474\" style=\"zoom:33%;\" />\n\nDLDR算法总流程：\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118112351688.png\" alt=\"image-20221118112351688\" style=\"zoom:33%;\" />\n\n#### Training Performance\n\n基于优化轨迹近似位于低维子空间的假设，所提出的DLDR可以将优化空间的维数从n降到d。为了验证该假设，我们在这样的低维子空间中优化DNN，检查其性能是否与原始空间中所有参数的训练相似。\n首先，我们为CIFAR-10进行了训练ResNet8[31]的实验，这也是开创性工作[10]所考虑的。如上所述，验证低维轨迹假设的标准是比较子空间和全参数空间中的训练性能，为此，我们使用SGD训练ResNet8的78330个参数。详细设置为：学习率为0.1，批量大小为128。请注意，在本文的实验中，SGD始终包含动量项，此处动量参数为0.9。经过3次试验，SGD平均获得83.84%的测试精度。\n然后，我们应用DLDR从参数中提取低维子空间，方法是在SGD训练的30个时期内对轨迹进行采样。详细的采样策略是在每个历元训练之后对模型参数进行采样。在图2a中，绘制了前5个预测成分的方差比，显示总方差的90%以上属于这五个成分。这一观察结果与我们关于存在这样一个低维子空间的假设一致，该子空间可以近似覆盖优化轨迹。\n\n接下来，**在DLDR提取的子空间中，我们从头开始训练神经网络**。我们在这里设置的维度是15，我们在**投影子空间中使用SGD优化器，称为P-SGD**。为了避免公平性的其他影响，我们使用与常规SGD相同的超参数设置，并从相同的初始化开始。从图2b中可以看出，当DLDR采样停止时，P-SGD很快就超过了性能（因此P-SGD的良好性能不是来自DLDR采样阶段给出的可通过的解决方案），并达到了与常规SGD相似甚至更好的精度。这里，与SGD相比的优势可能来自低维子空间的去噪（方差降低）效果，并且可以进一步研究。然而，至少，它表明我们可以在具有**显著较低维度（即15）**的子空间中有效地训练CIFAR-10的ResNet8，这有力地支持了我们的假设，即优化轨迹可以近似地位于低维子空间中。\n在表1中，我们报告了用于优化的维度和测试精度。在[7]中，7982个维度被用于实现平均58.35%的准确度，在[10]中提高到70.26。现在，DLDR可以找到一个维数更少的子空间，并获得更好的精度。在第5节中，我们将考虑更复杂的DNN架构和更复杂的任务，以进一步验证低维轨迹假设。\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118135412861.png\" alt=\"image-20221118135412861\" style=\"zoom:33%;\" />\n\n### DLDR-BASED QUASI-NEWTON ALGORITHM\n\n由于DNN通常有大量的参数需要优化，**一阶方法，即基于梯度下降的方法**是主要的方法。然而，一阶方法存在一些基本的局限性，例如围绕**最优点的收敛速度慢，以及对学习速率的高度敏感性**。二阶方法可以解决对于这些问题，但由于计算量大，应用它们来训练包含大量参数的DNN存在很大困难。取而代之的是，只使用部分二阶信息，如动量和累积信息，从而产生了许多流行的训练算法，如Adam[16]、RMSprop[17]和AdaGrad[32]，[33]。现在，使用所提出的DLDR，可以只找到几个（几十个）独立变量进行优化，这使得在训练复杂DNN时使用二阶方法变得适用。遵循这一思想，我们基于BFGS的框架开发了一种拟牛顿方法[18]，[19]。类似地，主要步骤包括基于历史梯度的黑森矩阵近似、拟牛顿更新和回溯线搜索，其详细信息在以下小节中给出。\n\n#### Hessian Matrix Approximation\n\n（可见slam十四讲p127）\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118141342146.png\" alt=\"image-20221118141342146\" style=\"zoom:33%;\" />\n\n1. 将参数的梯度投影到自变量空间；\n2. 在自变量空间中计算牛顿方向；\n3. 在训练过程中保持投影矩阵不变，将牛顿方向反投影到原始参数空间。\n\n#### Quasi-Newton Update\n\n虽然我们能找到只有几个独立变量来优化，但它们的梯度是通过在当前DNN框架中投影原始参数的梯度来计算的。因此，直接计算二阶梯度仍然不切实际。或者，我们采用拟牛顿方法来逼近Hessian矩阵及其逆矩阵。以这种方式，标准BFGS算法[18]，[19]与秩2校正更新一起使用，如下所示\n\n（没看懂）\n\n### NUMERICAL EXPERIMENTS\n\n在介绍了实验设置之后，我们将在本节中对以下方面进行数值评估。（1） 我们应用P-SGD来训练子空间中的DNN，其中**维数固定为40**，由DLDR提取，以验证不同神经网络架构上的低维轨迹假设。（2） 我们评估了所提出的P-BFGS算法的性能，并显示了其在加速训练方面的潜力。（3） 我们用标签噪声进行了实验，以证明从子空间训练中获得的**固有鲁棒性**。（4） 我们在**训练有素的网络上应用DLDR**，以进一步提高其性能\n\n#### Experiments Setup\n\n我们实验中使用的数据集包括CIFAR-10、CIFAR-100[8]和ImageNet[41]。对于CIFAR，所有图像均通过通道平均值和方差进行归一化。数据增强[31]也被执行：概率为0.5的水平图像翻转、4像素填充和裁剪。\n我们测试了ResNet20和ResNet32[31]以及其他11种DNN架构。这些网络中全参数的数量从0.27M到28.5M，但本文中我们始终只选择40个自变量。我们使用SGD优化器来训练DNN，其中权重衰减设置为1e-4，动量参数设置为0.9，批量大小设置为128。默认的初始学习率设置为0.1。对于CIFAR-10，我们训练DNN 150个时期，并在100个时期将学习率除以10，而对于CIFAR-100，我们训练200个时期，在150个时期进行除法。对于ImageNet，我们的代码是从官方PyTorch示例2修改而来的。实验是在Nvidia Geforce GTX 2080 TI上进行的。\n我们为CIFAR使用一个GPU，为ImageNet使用四个GPU。5个独立试验的平均值和标准差。DLDR和实验设置的更详细分析见附录。\nDLDR需要对优化轨迹进行采样。**对于CIFAR，我们采用最简单的采样策略：在每个训练周期之后对模型参数进行采样。对于ImageNet，参数在每个训练时期被均匀采样3次。更精细的采样策略可以提高性能。**\n对于P-SGD，我们采用与SGD相同的批量大小和动量因子。我们将初始学习率设置为1，训练时期设置为40，并在30个时期将学习率除以10。对于P-BFGS，我们将CIFAR的批大小设置为1024，ImageNet的批大小为256。请注意，这种二阶方法不需要学习速率表。\n\n#### Verification on Various Architectures\n\n在第3.3小节中，在CIFAR10上进行了实验，在此我们验证了CIFAR-100上的低维轨迹假设。我们将分别在所有参数和缩减子空间（后者实际上是建议的P-SGD）中通过SGD训练DNN。对于不同的神经网络架构，我们总是选择40个独立变量。如果SGD和P-SGD给出了可比的性能，则支持我们的假设，同时验证了所提出的DLDR的有效性。该实验包含11个流行的DNN，包括VGG11[3]、DenseNet121[42]、Inception[43]、NasNet[44]等，参数数量从780K到28.5M不等。\n在表2中，我们使用具有50/100/200个时期的SGD报告了测试精度，200个时期后的测试精度作为基线。然后，我们在40D子空间中应用P-SGD，DLDR从50或100个时期采样中提取这些子空间，并从初始化中进行40个时期的训练。\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118144500074.png\" alt=\"image-20221118144500074\" style=\"zoom:33%;\" />\n\n见表2中最后一列的结果。它清楚地表明，具有40个自变量的P-SGD可以在整个参数上达到SGD的竞争性能。\n这种竞争性能适用于所有这些架构，并有力地支持我们的低维轨迹假设。\n一个次要的发现是，一般来说，如果更好地提取子空间，性能会更好\n\n**Performance of P-BFGS Algorithm**\n\n在实证证明了低维轨迹假设之后，我们现在尝试一种二阶算法，即P-BFGS。我们首先考虑CIFAR-10上的ResNet20[31]。在图3a中，绘制了SGD的训练和测试精度曲线。灰色区域表示我们从何处获取DLDR样本，然后提取自变量。在获得40个自变量后，我们使用P-BFGS从相同的初始化开始，并绘制图3b中的训练曲线。仅**在2个时期后**，P-BFGS就比SGD（50个时期，即DLDR的采样）获得了更好的性能，并且在10个时期内，P-BFG达到了SGD（150个时期）的性能，这初步证明了应用二阶方法在效率上的优势\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118144822572.png\" alt=\"image-20221118144822572\" style=\"zoom:50%;\" />\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118145401179.png\" alt=\"image-20221118145401179\" style=\"zoom:33%;\" />\n\n\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118145518248.png\" alt=\"image-20221118145518248\" style=\"zoom:33%;\" />\n\n#### Robustness to Label Noise\n\n（这里没有说子空间是从哪里得到的，从带噪数据训练的参数中得到还是**干净数据**？）\n\n由于DNN的插值本质，它们对标签噪声敏感，即当标签不正确时，DNN将跟随这些不正确且无意义的标签。更糟糕的是，区分DNN学习的标签是正确的还是错误的，没有明显的区别[20]。目前，可以使用提前停止[50]（**即防止过拟合**），但如何选择最佳停止仍然具有挑战性，因为即使验证数据也可能损坏。在验证了DNN可以在低维子空间中训练之后，我们期望**低维属性可以自然地为DNN带来对标签噪声的鲁棒性。**\n为了检查**标签噪声**下的性能，我们考虑CIFAR-10，并将随机标签分配给训练数据的一部分c（针对不同的方法随机选择并固定）。然后，我们用损坏的数据训练ResNet20模型。有了标签噪声，SGD的完整训练性能显著下降，如图5中的蓝色曲线所示。提前停止（红色曲线）确实有帮助，但对干净数据的测试精度较差（c=0）。\n在低维子空间（绿色曲线）中的训练可以以较大的优势持续优于早期停止，同时在干净数据上保持与常规训练相同的性能。注意，这里我们在没有任何增强技术的情况下获得了鲁棒性，例如，对损失函数的修改[51]，[52]，因此结果有望通过这些技术进一步改进。\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118145913877.png\" alt=\"image-20221118145913877\" style=\"zoom:40%;\" />\n\nP-SGD对标签噪声的鲁棒性来自于训练子空间的低维特性，或者换句话说，**训练的自由度非常小**。为了进一步研究自变量数量的影响，我们将d从10变化到40，并在表4中报告了测试精度。在不同的标签噪声水平下，完全训练的PSGD（即，我们不选择提前停止）总是比完全训练的SGD具有更好的精度。\n我们还提供SGD获得的最佳精度，即，我们在训练期间监控测试精度，并选择出现的最佳测试精度。当然，在实践中无法达到SGD（最佳），但它可以作为一个参考，表明在低维子空间中训练DNN对标记噪声是鲁棒的。\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118150612211.png\" alt=\"image-20221118150612211\" style=\"zoom:40%;\" />\n\n\n\n#### Improving Well-trained Models\n\n在本小节中，我们研究了从井训练阶段提取的子空间。我们从在ImageNet上训练有素的ResNet18\\/50开始[41]（来自torchvision.models）。\n在表5中，他们的准确度显示为“训练有素”。这里，“训练有素”意味着继续进行最初的SGD培训无法提高绩效。现在我们提取具有30维的子空间（以0.005的学习率从SGD的5/10/15个时期采样），并应用P-BFGS。在这一阶段，解决方案接近最佳状态，前景更加平坦。因此，采样时间和维度可能比以前的实验少。由于在训练阶段获得了几个解决方案，**随机加权平均**（SWA，[53]）是一种有效且普遍采用的泛化改进方法，是适用的，我们将其性能作为比较。可以观察到，经过训练的模型可以通过子空间优化进一步改进，性能优于SWA\n\n<img src=\"Low Dimensional Trajectory Hypothesis is True/image-20221118150810037.png\" alt=\"image-20221118150810037\" style=\"zoom:33%;\" />\n\n#### CONCLUSIONS AND FURTHER WORKS\n\n本文的主要主张是低维轨迹假设。基于训练动力学，我们设计了一种高效的降维方法DLDR。在综合实验中，优化DLDR提取的几个（例如，几十个）独立变量可以获得与全参数常规训练类似的性能。\n优化性能和降维都从先前的工作中得到了显著改善[7]，[10]，有力地支持了这一假设，并表明DNN可以在很小的子空间中得到很好的训练。\n从DNN训练的新视角出发，我们尝试了三种后续应用，以进一步支持我们的假设，并获得了巨大的好处：1）随着维数的大幅减少，二阶方法变得适用，从中我们设计了P-BFGS算法，并为训练带来了极大的效率；2） 在低维子空间中的训练自然会给标签噪声带来鲁棒性；3） 我们还表明，子空间训练可以显著提高训练有素的模型的性能。这些应用进一步支持低维轨迹假设。\n尽管它们非常简单，例如，使用原始的BFGS框架，没有任何增强技术，但它们的性能意味着找到这样的低维子空间可以有助于理论和实践学习。进一步工作的可能方向包括**将子空间训练应用于微调任务**[27]，[28]，结合其他低秩训练方法[25]，[26]，以了解过度拟合、过度参数化[54]，并研究少数镜头学习[55]、元学习[56]等。\n\n\n\n\n\n\n\n## 代码\n\nhttps://blog.csdn.net/anshuai_aw1/article/details/82498374?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-82498374-blog-119275175.pc_relevant_3mothn_strategy_recovery&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-82498374-blog-119275175.pc_relevant_3mothn_strategy_recovery&utm_relevant_index=1\n\n![image-20221125171645874](Low Dimensional Trajectory Hypothesis is True/image-20221125171645874.png)\n"},{"url":"/2022/11/24/编程/图片处理相关/","content":"## opencv（numpy）、tensor、plt图像显示\n\n#### 灰度图像显示\n\n```python\n#opencv读取灰度图\nimg = cv2.imread('./test.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# or\nimg = cv2.imread('./test.jpg',0)\n\n#numpy转tensor\n# img = transforms.ToTensor()(img)\nimg = torch.tensor(img,dtype=torch.float32)\n\n#注意tensor转opencv时要将格式转为np.uint8才能正常显示\nimg_1 = img.squeeze().cpu().numpy()\nimg_1 = img.astype(np.uint8)\ncv2.imshow('img_1',img_1)\ncv2.waitKey(0)\n#plt显示灰度图,要加cmap='Greys_r',不然会以伪彩色图显示\nplt.imshow(img_1,cmap='Greys_r')\nplt.show()\n```"},{"url":"/2022/11/24/计算机视觉/UAV&Satellite/开题相关/","content":"- 针对UAV和卫星匹配的特定任务，在图像检索的基础上进行特征匹配（可细分为图像检索和特征匹配两个任务）\n\n\t- ### 目的：\n\n\t\t- 通过特征匹配结果协助进行图像检索\n\t\t\t- CMU&ICRA22 UAV俯瞰视觉定位竞赛（-2022.9.1）\n\t\t\t\t- <img src=\"开题相关/image-20221114102144712.png\" alt=\"image-20221114102144712\" style=\"zoom:50%;\" />\n\t\t- 通过特征匹配结果求解更精确的无人机位姿\n\t\t\t- UAV Pose Estimation using Cross-view Geolocalization with Satellite Imagery (ICRA 2019)\n\t\t\t\t- ![image-20221114102216892](开题相关/image-20221114102216892.png)\n\t\t- 通过特征匹配的feature map促进图像检索网络的训练\n\t\t\t- Multiple-environment Self-adaptive Network for Aerial-view Geo-localization  (arXiv 2022.4.18)\n\t\t\t\t- ![image-20221114102429372](开题相关/image-20221114102429372.png)\n\n\t- \n\n\t- ### 数据集：UAV和卫星图像的对应+相对位姿\n\t\n\t\t- 注：大多数数据集为图像检索数据集，即仅存在图像级的对应，不一定有每张图像的位姿，且没有点对点的监督信息\n\t\t\t- ==> 若要训练特征点匹配网络，只能采用自监督(随机单应变换等)/弱监督（相机位姿监督）的方法\n\t\t\t\t- 特征匹配自监督训练方式可参考：\n\t\t\t\t\t- SuperPoint: Self-Supervised Interest Point Detection and Description（CVPR 2018）\n\t\t\t\t\t\t- <img src=\"开题相关/image-20221114104109994.png\" alt=\"image-20221114104109994\" style=\"zoom:33%;\" />\n\t\t\t\t\t- UnSuperPoint:End-to-End Unsupervised Interest Point Detector And Descriptor (arXiv 2019)\n\t\t\t\t\t\t- <img src=\"开题相关/image-20221114110939059.png\" alt=\"image-20221114110939059\" style=\"zoom:33%;\" />\n\t\t\t\t\t- Digging Into Self-Supervised Learning of Feature Descriptors (3DV 2021)\n\t\t\t\t\t\t- <img src=\"开题相关/image-20221114103344274.png\" alt=\"image-20221114103344274\" style=\"zoom:33%;\" />\n\t\t\t\t\t- Looking Beyond Corners: Contrastive Learning of Visual Representations for Keypoint Detection and Description Extraction (arXiv 2021.12)\n\t\t\t\t\t\t- 对比学习的方式训练\n\t\t\t\t\t\t- ![image-20221114103411106](开题相关/image-20221114103411106.png)\n\t\t- ### 数据特点：\n\t\t\t\n\t\t\t- 旋转角度大==>可采用旋转不变CNN来改进网络\n\t\t\t\t- A case for using rotation invariant features in state of the art feature matchers （CVPRW 2022）\n\t\t\t\t- <img src=\"开题相关/image-20221114104424545.png\" alt=\"image-20221114104424545\" style=\"zoom:33%;\" />\n\t\t\t- UAV的斜视视角和卫星的俯视视角==>不同视角的转化、不同色彩风格的转化\n\t\t- UAV-Satellite View Synthesis for Cross-view Geo-Localization （IEEE Transactions on Circuits and Systems for Video Technology 2021）\n\t\t\t\t\t- <img src=\"开题相关/image-20221114104920560.png\" alt=\"image-20221114104920560\" style=\"zoom:33%;\" />\n\t\n\t\t\n\t\t\n\t\t"},{"url":"/2022/11/24/计算机视觉/事件相机/调研/","content":"## 事件相机\n\n特点：高动态范围，高帧率\n\n\n\n[「智科特测评」事件相机，可能你没有见过的新品种相机_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1DC4y1s7BB/?spm_id_from=333.337.search-card.all.click&vd_source=0bb4d13f4bd26eaf7f64c37f29dc9f27)![image-20221110092532490](调研/image-20221110092532490.png)\n\n![image-20221110092548998](调研/image-20221110092548998.png)\n\n[(4 封私信) 什么是事件相机，计算机视觉中事件相机这一方向需要学哪些相关知识？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/333451919/answer/2521206041)\n\n[一种事件相机描述子——DART (qq.com)](https://mp.weixin.qq.com/s/KSImGRLx-ZxggFXPZ8PQJQ)\n\n[事件相机角点检测，从原理到demo (qq.com)](https://mp.weixin.qq.com/s/GVyP4iTy9UgUtfwc1KpRmw)\n\n下面这幅图展示了实际事件相机拍摄的数据，红色和蓝色分别表示当前像素亮度的增加或减小。这幅图是将一段时间内的事件流累积获得的一组事件。\n\n![在这里插入图片描述](调研/2437c02329c2cc733d0febcb719b17e5.png)\n\n\n\n\n\n【事件相机整理】角点检测与跟踪总结https://blog.csdn.net/tfb760/article/details/106022399\n\nEKLT: Asynchronous, Photometric Feature Tracking using Events and Frames (IJCV'19)https://www.youtube.com/watch?v=ZyD1YPW1h4U\n\n\n\nTracking and Mapping of Stereo Event Camerahttps://www.youtube.com/watch?v=pnhlb_kc85Q\n\n[(54条消息) ICRA2021中的事件相机研究_larry_dongy的博客-CSDN博客](https://blog.csdn.net/tfb760/article/details/120400316)\n\n[(54条消息) 学习笔记之——Event Camera（事件相机）调研_gwpscut的博客-CSDN博客_事件相机](https://blog.csdn.net/gwplovekimi/article/details/115908307?spm=1001.2101.3001.6650.4&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-4-115908307-blog-120400316.pc_relevant_multi_platform_whitelistv3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-4-115908307-blog-120400316.pc_relevant_multi_platform_whitelistv3&utm_relevant_index=5)"},{"url":"/2022/11/24/计算机视觉/Feature Matching/LoFTR/","content":"## 代码运行\n\n```python\ntrain.py \"configs/data/megadepth_trainval_640.py\" \"configs/loftr/outdoor/loftr_ds_dense.py\" --exp_name=\"outdoor-ds-640\" --gpus=2 --num_nodes=1 --accelerator=\"ddp\" --batch_size=1 --num_workers=0 --pin_memory=true --check_val_every_n_epoch=1 --log_every_n_steps=1 --flush_logs_every_n_steps=1 --limit_val_batches=1. --num_sanity_val_steps=10 --benchmark=True --max_epochs=30\n\ndemo_loftr.py --weight /home/young/disk/code/feature_matching/LoFTR/logs/tb_logs/outdoor-ds-640-bs=6/version_2/checkpoints/epoch=6-auc@5=0.433-auc@10=0.606-auc@20=0.742.ckpt --input /home/young/disk/code/feature_matching/LoFTR/data/videos/video1.mp4\n```\n\n\n\n## 训练结果\n\n![image-20220918163227217](LoFTR/image-20220918163227217.png)\n\n\n\n修改train_list.txt来减少数据集的场景数目\n\ndataset的数量不会影响显存占用，输入网络的图像尺寸才会影响显存占用\n\n\n\n原数据量（640*640）的测试结果\n\n![image-20220930105732853](LoFTR/image-20220930105732853.png)\n\n\n\nquadattention（640*640）\n\n5 epoch  45.90  63.19  76.30\n\n30 epoch  49.75  66.76  79.87\n\nbest 27 epoch  50.96  68.05  80.32\n\n\n\n### test_1500 results\n\nloftr_od_ds\n\n![image-20221012112651765](LoFTR/image-20221012112651765.png)\n\n\n\nquad_od_ds (my)\n\n\n\n\n\nquad_od_ds(author)\n\n![image-20221012123020189](LoFTR/image-20221012123020189.png)\n\n\n\n### 用十分之一的数据测试结果\n\n![image-20220930110011605](LoFTR/image-20220930110011605.png)\n\n20epoch的效果相当于原2epoch\n\n\n\n\n\n## 原理部分\n\n线性attention\n\nhttps://zhuanlan.zhihu.com/p/157490738?from_voters_page=true\n\n![image-20221013150606391](LoFTR/image-20221013150606391.png)\n\n\n\n## 改进思路\n\n- 全部替换为旋转不变的transformer（包括cnn）\n- 添加几何信息指导\n- 大尺寸图像（图像尺寸在训练和测试时都是一样的吗？必须固定？）\n- 每一层attention都输出匹配对信息，最后对这些信息进行投票\n- 加入光流法or直接法\n- 粗配准的结果已经可以求一个大致的位姿，能否根据这个位姿进行优化\n- 可以设置一个目标：以最少数量的点，得到最准确的位姿，这样取出来的点就是最准确的点\n- 点的误差改为3d点的误差，而不是2d点\n- 粗匹配结果计算单应矩阵后，对图像（或者feature map）进行单应变换得到粗对齐，[(53条消息) RANSAC-Flow: generic two-stage image alignment（ECCV 2020）图像对齐论文代码详细分析_秋山丶雪绪的博客-CSDN博客](https://blog.csdn.net/weixin_43605641/article/details/120970142?spm=1001.2014.3001.5501)\n- 同时解决旋转和尺度的问题\n- 局部性失效时才采用更全局的信息\n- 图像对垂直化\n\n"},{"url":"/2022/11/24/计算机视觉/UAV&Satellite/调研/","content":"## [UAV Geolocalization Using Satellite Imagery](https://abhinavtripathi95.github.io/lets-talk/technical/2020/08/03/uav-geolocalization.html)\n\n### Motivation\n\n无人机通常靠GPS来获得全局姿态估计，然而为了使用GPS进行精确的地理定位，无人机必须能够从四个或更多的GPS卫星接收直接的视线。这可能是一个问题，如果存在高楼大厦，山脉或干扰器，可以阻碍来自卫星的信号。\n那么我们能在没有GPS的情况下推断出无人机的全局姿态吗?\n答案是肯定的，我们可以使用无人机下面附加的相机传感器来比较场景和卫星图像，并推断出无人机的位置。\n\n### Geolocalization as an Image Matching Problem\n\n考虑这样一个场景，您有一个卫星图像数据库，其中标注了它们的位置。\n通过将无人机相机的图像与数据库进行精确匹配，可以很好地获得无人机的经纬度近似。\n为了从卫星数据库中检索相似的图像，我们必须能够精确地匹配卫星图像与无人机相机馈送。\n因此，在本研究中，我们将自己局限于**航空图像匹配问题**，训练一个深度学习模型，**可以精确匹配来自卫星和无人机相机的图像**。\n\n\n\n## UAV Pose Estimation using Cross-view Geolocalization with Satellite Imagery (ICRA 2019)\n\n<img src=\"调研/image-20221031184827896.png\" alt=\"image-20221031184827896\" style=\"zoom:30%;\" />\n\n数据集是从谷歌地图上收集的“Sample matching pairs of UAV images from Google Earth (top row) and satellite images from Google Maps (bottom row).”\n\nscene localization network 用于对每张图片生成一个描述符，计算欧式距离d，用于计算权重\n\nCamera Localization network 是一个双分支网络，第一个分支输出一个热力图，最大值位置代表x,y水平位置，第二个分支直接回归垂直位置，朝向(heading)和倾斜角度(tilt)\n\nCross-view Geolocalization 模块，将多张图像经过两个网络后的结果根据d计算加权，生成最终相机位姿\n\n## Visual Localization with Google Earth Images for Robust Global Pose Estimation of UAVs（ICRA 2020）\n\n数据集：[CLOUD: Canadian Longterm Outdoor UAV Dataset | Dynamic Systems Lab | Prof. Angela Schoellig (dynsyslab.org)](https://www.dynsyslab.org/cloud-dataset/)\n\n包含季节变化和光照变化的无人机图像，以及UAV的GPS位置信息和姿态信息\n\n还包括对应的卫星图像（由谷歌地球生成）\n\n\n\n\n\n在这项工作中，所有真实的和渲染的图像都是用指向最低点的相机拍摄的\n\n\n\n## UAV Localization Using Autoencoded Satellite Images（IEEE ROBOTICS AND AUTOMATION LETTERS 2021）\n\n和上一篇是一个团队\n\n<img src=\"调研/image-20221031194705659.png\" alt=\"image-20221031194705659\" style=\"zoom:33%;\" />\n\n### 动机：\n\n由于谷歌地图中用于重建的卫星图像是多年前拍摄的，在照明、小对象移动(例如车辆、拖车)、大的结构变化(例如建筑物的增建/拆除)以及不寻常的对象重建方面与现场图像有所不同，特别是对于树木等非矩形对象。这使得**基于特征的方法在很多情况下很难获得准确和稳健的结果。**\n\n### 方法：\n\n图像在GE中围绕所需的飞行路线进行渲染。对这些特定于路径的图像进行自动编码器训练，以将它们压缩成小得多的矢量表示。同样的自动编码器也用于压缩实时图像。通过内积核将压缩的实时图像向量与附近的所有压缩GE图像向量进行比较。这会产生与每个相应GE图像姿势相关联的权重。\n根据这些权重，计算出带有伴随协方差的经度、纬度和航向的位置。\n\n### 数据集：\n\nuav图像+GE图像\n\n一天中六个不同时刻的1.1公里路径上的真实无人机图像数据集上进行了演示，覆盖了几种照明条件\n\n### 优点：\n\n查找速度快\n\n### 缺陷：\n\n不是端到端训练\n\n### 改进：\n\n用最新的基于特征的方法进行尝试\n\n\n\n## UAV-Satellite View Synthesis for Cross-view Geo-Localization\n\n（IEEE Transactions on Circuits and Systems for Video Technology TCSVT 2021)\n\n### 数据集：\n\n本研究使用的数据集是大学1652[30]，其中包含来自全球72所大学的1652栋建筑（1652个地点）。据我们所知，这是迄今为止**唯一一个同时包含无人机视图图像和卫星视图图像的数据集**。每个建筑物都与三个不同视角的图像相关联（见图7），包括一个卫星视图图像、54个不同高度和角度的无人机视图图像以及一个或多个地面视图图像。在本文中，我们使用了卫星图像（垂直视图）和无人机图像（斜视图）。大多数无人机视图是倾斜的，这为使用透视投影变换实现更好的匹配性能提供了巨大的潜力。\n\n### 方法：\n\n<img src=\"调研/image-20221102151027518.png\" alt=\"image-20221102151027518\" style=\"zoom:33%;\" />\n\n通过数据集的先验信息进行的透视投影变换（？？？），然后采用GAN网络生成和卫星图像风格相似的图像（同时视角也更相似），如下图所示\n\n<img src=\"调研/image-20221101105655053.png\" alt=\"image-20221101105655053\" style=\"zoom: 80%;\" />\n\n随后对特征图采用一个方环分割策略来获取距离图像中心不同距离的上下文特征信息\n\n```\n如图6所示，我们将高级特征分为正方形环形分区中的几个部分。由于地理目标通常位于图像的中心，周围有上下文信息，分割方法不仅可以获得地理目标信息（区域A和B），还可以获得与地理目标具有不同距离的若干上下文信息部分（区域C和D）。因此，我们可以显式地利用上下文信息来优化PCL。我们观察到，我们的分割策略在本质上对图像旋转是鲁棒的。例如，当将图6中中间行的图像旋转到顶行的图像时，四个区域（A、B、C和D）仍然包含与中间图像的对应区域相同的信息。因此，根据方环分割策略设计的网络对图像旋转具有良好的鲁棒性\n```\n\n(缺陷：只对90°旋转鲁棒，且目标不一定位于图像中心)\n\n<img src=\"调研/image-20221101110400421.png\" alt=\"image-20221101110400421\" style=\"zoom:33%;\" />\n\n最后对图片生成一个描述符，并比较和其他图像的余弦相似性\n\n\n\n### 疑问：\n\nUAV图像和卫星图像的视野范围基本一样？\n\n是否能生成已知对应关系的GAN网络\n\n\n\n## University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization\n\n（ACM Multimedia 2020）\n\n针对跨视角地理定位问题，提出一个baseline和dataset\n\n数据集包含三种视角（地面、无人机、卫星）\n\nGoogle Earth中的3D引擎用于模拟相机的不同视点。\n\n\n\n## Joint Representation Learning and Keypoint Detection for Cross-View Geo-Localization\n\n(IEEE TRANSACTIONS ON IMAGE PROCESSING 2022)\n\n我们的模型自动从卫星视图图像和无人机视图图像的相应区域提取关键点，而无需额外监督\n\nUSAM提取的关键点仅用于增强图像的特征识别，而不是检测实际映射。\n\n（1）我们的方法不需要额外的注释，例如相机姿势、深度等。\n\n（2） 我们的方法不进行方位估计。\n\n（3） 所提出的方法考虑了点对点关系\n\n![image-20221108091237749](调研/image-20221108091237749.png)\n\nUSC相当于算子\n\n-1 -1 -1\n\n-1  8 -1\n\n-1 -1 -1\n\n\n\n[AggMan96/RK-Net: Code for RK-Net (github.com)](https://github.com/AggMan96/RK-Net)\n\n## Real-time Geo-localization Using Satellite Imagery and Topography for Unmanned Aerial Vehicles\n\n使用谷歌地球提供的3D模型+blender渲染卫星图像和深度图像\n\n<img src=\"调研/image-20221108110109358.png\" alt=\"image-20221108110109358\" style=\"zoom: 50%;\" />\n\n## Multiple-environment Self-adaptive Network for Aerial-view Geo-localization \n\n(arXiv 2022.4.18)\n\n现有的方法在真实的天气（如雨天和雾天）下会遇到较大的性能下降，因为它们没有考虑训练数据和多个测试环境之间的域转换。为了缩小这一领域差距，我们提出了一种多环境自适应网络（MuSe-Net），以动态调整环境变化引起的领域偏移。\n特别是，MuSe-Net采用了包含一个多环境类型提取网络和一个自适应特征提取网络的两分支神经网络。顾名思义，多环境风格提取网络用于提取与环境相关的风格信息，而自适应特征提取网络利用自适应调制模块来动态地最小化与环境有关的风格差距。\n\n![image-20221108105026501](调研/image-20221108105026501.png)\n\n## CMU&ICRA22 UAV俯瞰视觉定位竞赛\n\n[ICRA2022] General Place Recognition: Visual Terrain Relative Navigation\n\n[CMU & ICRA22 UAV俯瞰视觉定位竞赛冠军技术方案分享|军事科学院_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1NG411G78L/?spm_id_from=333.337.search-card.all.click&vd_source=0bb4d13f4bd26eaf7f64c37f29dc9f27)\n\n[CMU&ICRA22 UAV俯瞰视觉定位竞赛亚军技术方案分享_VINS-Mono_VINS-Fusion_cvlife (sohu.com)](https://www.sohu.com/a/587059840_121124366)\n\n[竞赛官网GPR-Competition (google.com)](https://sites.google.com/andrew.cmu.edu/gpr-competition/)\n\n[数据集：MetaSLAM/ALTO: ALTO (Aerial-view Large-scale Terrain-Oriented) dataset (github.com)](https://github.com/MetaSLAM/ALTO)\n\n### 数据集简介：\n\n[自述文件.txt (dropbox.com)](https://www.dropbox.com/sh/q1w5dmghbkut553/AAASkcJEpfsV9PbdLjszzdYAa/UAV?dl=0&preview=readme.txt&subfolder_nav_tracking=1)\n\n- 俄亥俄州到匹兹堡的150公里长的飞行轨迹，使用直升飞机拍摄；\n\n- 轨迹上包括几种不同难度的环境，包括城市/郊区、森林、农村和其他自然地形。\n- 测试集中有几年前的图像（2017年同样是飞机拍的），同时也包括2012年的卫星图像\n- 500*500像素\n- 每张直升机图像有成对的卫星图像\n- 包括GPS（全局的位置，亚米级精度, UTM坐标系）和IMU（四元数，相机相对于ECEF参考帧的方向）信息\n\n```\nThis dataset contains nadir-facing RGB camera imagery captured via commercial helicopter (query), as well\nas by high-altitude plane (reference, captured by USGS). We include some query telemetry information that\nmay be useful for preprocessing or incorporated into your algorithm as contextual information.\nWe provide three data splits: Train, Val, and Test. These are non-overlapping and include (24701), (3979), \nand (4209) images, respectively. These are all part of a 150km helicopter flight over a variety of\ndifferent terrains, including Urban, Suburban, Rural, Dense Forest, Rivers, and Lakes.\nFor ease of use with deep learning pipelines, we include the images in png format. All images are RGB\nand in 500x500 pixel resolution.\n\nIn addition to the imagery in the Train and Val sets, we also provide a few csv files.  Their contents are as follows:\n    - gt_matches.csv: This file provides the ground truth best match between the query images and the reference images.\n        As states above, multiple query images may map to a single reference image. We determine \"best match\" by using the L2 \n        distance of the UTM coordinates associated with the respective images.\n        - query_ind: The index of the query image.\n        - query_name: The name of the query image.\n        - ref_ind: The index of the reference image that best matches the query image.\n        - ref_name: The name of the reference image that best matches the query image.\n        - distance: The distance (meters) between the query and best matching reference image.\n    \n    - query.csv: Telemetry information about each query image frame.\n        - easting: The ground truth Easting coordinate (meters) where the image was taken.\n        - northing: The ground truth Northing coordinate (meters) where the image was taken.\n        - altitude: The ground truth Altitude (meters) above the WGS84 ellipsoid surface.\n        - orient_x, orient_y, orient_z, orient_w: The orientation (scalar last quaternion) of the camera with respect the the ECEF reference frame.\n        - name: The name of the query image.\n\n    - reference.csv: Information about each reference image frame. Contains information about all offset reference images as well.\n        - easting: The ground truth Easting coordinate (meters) where the image was taken.\n        - northing: The ground truth Northing coordinate (meters) where the image was taken.\n        - name: The directory and name of the reference image.\n```\n\n\n\n## Deep learning-based robust positioning for all-weather autonomous driving\n\n定位能力负责精确预测AV在地图上的位置。AV的大多数核心组件（如预测和规划）都依赖于精确定位，例如在几厘米以内。尽管AV严重依赖于GPS等天基全球导航卫星系统的信号进行定位，但由于障碍物或反射，无线电信号可能在许多环境中丢失或退化。尤其是，在被高层建筑包围的城市地区，AV运营仍然极具挑战性。此外，GPS仅提供米级定位精度，而没有方位信息，这对AV乘客或周围环境中的乘客来说可能是致命的。\n\n\n自我运动估计方法应利用摄像机（丰富、密集的视觉信息）、激光雷达（可见范围内的精细粒度）和雷达（对恶劣天气的免疫力）的优势，同时解决其相对缺点。\n\n\n\n\n\n通过重建的质量来创建监督信号\n\n自监督的核心思想[(54条消息) 【SLAM系列】Unsupervised Learning of Depth and Ego-Motion from Video_^_^ 晅菲的博客-CSDN博客](https://blog.csdn.net/weixin_43882112/article/details/108610832)\n\n![image-20221106211924306](调研/image-20221106211924306.png)\n\n\n\n\n"},{"url":"/2022/11/24/计算机视觉/Feature Matching/loftr代码修改/","content":"### 待修改:\n\n- 输入改为单应变换对：\t\n\n  先测试一张正常一张(输入padding),另一张单应变换+背景填充\t\t\t\n  是否需要padding？\n\n- 修改dataset的read_megadepth_gray_homography  import my_aug\n"},{"url":"/2022/11/03/计算机视觉/HY项目/硬管加油/","content":"100ms\n\n<img src=\"硬管加油/image-20221021154218858.png\" alt=\"image-20221021154218858\" style=\"zoom: 33%;\" />\n\n200ms\n\n<img src=\"硬管加油/image-20221024102105878.png\" alt=\"image-20221024102105878\" style=\"zoom:43%;\" />\n\n500ms\n\n<img src=\"硬管加油/image-20221024102134774.png\" alt=\"image-20221024102134774\" style=\"zoom:40%;\" />\n\n1000ms\n\n<img src=\"硬管加油/image-20221021154234078.png\" alt=\"image-20221021154234078\" style=\"zoom:33%;\" />"},{"url":"/2022/11/03/计算机视觉/face_reg/对比学习综述/","content":"人脸反欺骗（FAS）面临的一个主要挑战是，由于各种真实世界攻击样本收集的成本高昂，因此无法预测的攻击。然而，在实际应用中，真实人脸样本相对容易收集。在本文中，我们旨在设计一种基于小规模真实世界攻击样本的人脸反欺骗训练策略。为此，我们设计了一个两步人脸反欺骗框架，在该框架中，基于自动编码器的网络首先通过负数据增强（NDA）对真实人脸和合成目标样本进行预训练。随后，仅使用小规模真实世界攻击样本对模型进行微调。重构学习使自动编码器能够学习与攻击样本不同的真实人脸样本的表示。基于NDA的合成攻击样本在表示空间中提供了与真实人脸样本不同的分布。广泛的实验表明，我们的人脸反欺骗方法仅使用小的真实世界跟踪样本进行训练，可以获得与监督SOTA方法相当的性能。此外，我们的方法在领域泛化测试场景中的性能也优于现有的SOTA性能，这意味着我们的模型在训练过程中可以很好地泛化未见过的场景\n\n\n\nInstDisc 并通过使用NCE loss 来解决大量实例类别引起的计算困难\n\nInvaSpread\n\nsimclrv1\n\nmocov1\n\nvit：mocov3  DINO \n\n\n\nSelf-supervised visual representation learning. \n\nSelf-supervised learning can independently discover potential representations from massive data without manual annotation. In computer vision and even the whole machine learning, contrastive learning has attracted a lot of research for self-supervised learning，e.g.,[x,x,x]. The core idea of contrastive learning lies in the difference of samples, which aims to make the samples with small differences closer in the feature space, and the samples with large differences further away from the feature space, so that different samples can be well distinguished.\n\nInstDisc first proposes a pretext task of instance discrimination, which regards each picture as a separate category, the aim of instance discrimination is to train the instance classifier instead of the category classifier, and store negative samples through the memory bank. Based on InstDisc, MoCov1 changes the memory bank to queue and updates features by momentum, resulting in a large and consistent dictionary that outperforms supervised pretraining on a large number of visual downstream tasks. Inspired by InvaSpread, SimCLRv1 does not use an external data structure to store negative samples. Through end-to-end learning, more data enhancement, and the use of projection head, SimCLRv1 also achieves significant results on a large number of downstream tasks. With the popularity of Vision Transformer (ViT), MoCov3 and DINO replaced the previous CNN backbone network with ViT, and both proposed corresponding improvement measures to solve the unstable problem during the training process of ViT.\n\nExisting self-supervised methods construct contrastive samples based on semantic categories. However, for the face anti-spoofing task, the difference between real face samples and attack samples lies in the sensory difference. Therefore, it is difficult for existing methods to achieve satisfactory performance on face anti-spoofing tasks. In this paper, we propose an NDA-based attack sample synthesis strategy to construct sensory contrast learning samples for self-supervised face anti-spoofing.\n\n自监督学习无需人工数据标注，能够从海量的数据中自主发现潜在的特征信息。最近几年对比学习在计算机视觉领域乃至整个机器学习领域都吸引了大量的研究。对比学习最核心的思想在于数据的区别，其目的是让区别小的数据在特征空间更加接近，让区别大的数据在特征空间更加远离，这样就能够很好地区分不同的数据。\n\nInstDisc首先提出了Instance Discrimination的代理任务（pretext task），将每一张图片都视为单独的类别，训练基于实例的分类器代替基于类别的分类器，并通过memory bank来存储负样本。在InstDisc的基础上，MoCo v1将memory bank改为队列，用动量的方式去更新特征，从而得到一个又大又一致的字典，并且在大量视觉下游任务上超过有监督预训练的方法。受Inva Spread的启发，simclr v1没有采用外部数据结构来存储负样本，通过端到端的学习，更多的数据增强，以及projection head的使用，也在大量下游任务上取得了显著成果。随着Vision Transformer的流行，MoCov3和DINO将之前的CNN骨干网络替换为了Vit，并都针对训练不稳定的问题提出了相应的改进措施。\n\n然而，上述对比学习的方法都是针对于粗粒度的特征学习，对于人脸反欺诈任务来说，真实人脸样本和攻击样本的差别很小，对比学习的方法难以将它们在特征空间中区分开来。为此我们提出了一种基于小规模真实世界攻击样本的人脸反欺诈训练策略，能够区分在特征空间中攻击样本与真实人脸样本的细粒度差异。\n\n\n\nsensory \n\n现有自监督方法基于语义类别构造对比样本。然而对于人脸反欺骗任务来说，真实人脸样本和攻击样本的差别在于感官上的不同。因此，现有方法在人脸反欺骗任务上难以取得令人满意的表现。在本文中，我们提出了一种基于NDA的攻击样本合成策略，构造感官上的对比学习样本，为了自监督人脸反欺骗。\n\nExisting self-supervised methods construct contrastive samples based on semantic categories. However, for the face anti-spoofing task, the difference between real face samples and attack samples lies in the sensory difference. Therefore, it is difficult for existing methods to achieve satisfactory performance on face anti-spoofing tasks. In this paper, we propose an NDA-based attack sample synthesis strategy to construct sensory contrast learning samples for self-supervise face anti-spoofing.\n\n\n\nlatex 代码\n\n{\\bf Self-supervised visual representation learning.}\nSelf-supervised learning can independently discover potential representations from massive data without manual annotation. In computer vision and even the whole machine learning, contrastive learning has attracted a lot of research\\cite{InstDisc,InvaSpread,CPC,CMC}for self-supervised learning. The core idea of contrastive learning lies in the difference of samples, which aims to make the samples with small differences closer in the feature space, and the samples with large differences further away from the feature space, so that different samples can be well distinguished.\n\nInstDisc\\cite{InstDisc} first proposes a pretext task of instance discrimination, which regards each picture as a separate category, the aim of instance discrimination is to train the instance classifier instead of the category classifier, and store negative samples through the memory bank. Based on InstDisc, MoCov1\\cite{mocov1} changes the memory bank to queue and updates features by momentum, resulting in a large and consistent dictionary that outperforms supervised pretraining on a large number of visual downstream tasks. Inspired by InvaSpread\\cite{InvaSpread}, SimCLRv1\\cite{simclrv1} does not use an external data structure to store negative samples. Through end-to-end learning, more data enhancement, and the use of projection head, SimCLRv1 also achieves significant results on a large number of downstream tasks. With the popularity of Vision Transformer (ViT)\\cite{ViT}, MoCov3\\cite{mocov3} and DINO\\cite{DINO} replaced the previous backbone network with ViT, and both proposed corresponding improvement measures to solve the unstable problem during the training process of ViT.\n\nExisting self-supervised methods construct contrastive samples based on semantic categories. However, for the face anti-spoofing task, the difference between real face samples and attack samples lies in the sensory difference. Therefore, it is difficult for existing methods to achieve satisfactory performance on face anti-spoofing tasks. In this paper, we propose an NDA-based attack sample synthesis strategy to construct sensory contrast learning samples for self-supervised face anti-spoofing."},{"url":"/2022/10/17/计算机视觉/face_reg/moco实验/","content":"\n\npython main_moco.py -a resnet18 --lr 0.012 --batch-size 128 --multiprocessing-distributed --world-size 1 --rank 0\n\npython main_lincls.py -a resnet18 --lr 15.0 --batch-size 128 --pretrained checkpoint_0199.pth.tar --dist-url 'tcp://localhost:10001' --multiprocessing-distributed --world-size 1 --rank 0\n\n\n\n| 数据集 | acer  | apcer | bpcer |\n| ------ | ----- | ----- | ----- |\n| 1_1    |       |       |       |\n| 2_1    |       |       |       |\n| 3_1    |       |       |       |\n| 3_2    |       |       |       |\n| 3_3    |       |       |       |\n| 3_4    |       |       |       |\n| 3_5    |       |       |       |\n| 3_6    |       |       |       |\n| 4_1    | 16.67 | 0     | 33.33 |\n| 4_2    |       |       |       |\n| 4_3    |       |       |       |\n| 4_4    |       |       |       |\n| 4_5    |       |       |       |\n| 4_6    |       |       |       |\n\n"},{"url":"/2022/10/07/计算机视觉/Feature Matching/quadattention/","content":"图片尺寸被32整除？\n\n\n\n![image-20220929165016098](quadattention/image-20220929165016098.png)"},{"url":"/2022/08/29/SNN/SNN基础/","content":"[(46条消息) Spiking-YOLO:脉冲神经网络高效的目标检测_是张先生呀的博客-CSDN博客](https://blog.csdn.net/jhonz/article/details/106663636)\n\n[Nature 长文综述：类脑智能与脉冲神经网络前沿 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/94556277)\n\n[(46条消息) SNN综述(1)：深度脉冲神经网络_Tianlong Lee的博客-CSDN博客_深度脉冲神经网络](https://blog.csdn.net/ly18846826264/article/details/106948921?spm=1001.2014.3001.5501)\n\n"},{"title":"特征匹配","url":"/2022/04/10/计算机视觉/Feature Matching/特征匹配/","content":"\n# 相关链接\n\n[(34条消息) 特征点检测与匹配相关论文梳理（持续更新）_秋山丶雪绪的博客-CSDN博客_特征匹配论文](https://blog.csdn.net/weixin_43605641/article/details/122572446)\n\n[(34条消息) 秋山丶雪绪的博客_CSDN博客-特征点检测与匹配,计算机系统结构,Python代码记录领域博主](https://blog.csdn.net/weixin_43605641)\n\n[2020最强匹配综述—《Image Matching from Handcrafted to Deep Features: A Survey》阅读总结_喵呜喵喵喵的博客-CSDN博客](https://blog.csdn.net/qq_42708183/article/details/109133806)\n\n[CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！ (qq.com)](https://mp.weixin.qq.com/s?__biz=MzI3NDIyMjcyNg==&mid=2652161460&idx=1&sn=4d2ee4e6973593e670ce460cc8eb590b&chksm=f0f73e5dc780b74b1a4b68d6909708c261a539383720e245c2133f48753a8e871f98fd221912&scene=21#wechat_redirect)\n\n[CVPR 2021 | 图像匹配挑战赛总结 (SuperPoint + SuperGlue 缝缝补补还能再战一年) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/387962855)\n\n[CVPR 2021 Image Matching 挑战赛双冠算法：揭秘AR导航背后的技术 - 极术社区 - 连接开发者与智能计算生态 (aijishu.com)](https://aijishu.com/a/1060000000224837)\n\n[ducha-aiki/pydegensac: Advanced RANSAC (DEGENSAC) with bells and whistles for H and F estimation (github.com)](https://github.com/ducha-aiki/pydegensac)（ransac的改进方案，效果更好）\n\n[hpatches/hpatches-dataset： HPatches： Homography-patchs dataset. (github.com)](https://github.com/hpatches/hpatches-dataset)\n\n[非常详细的sift算法原理解析_可时间倒数了的博客-CSDN博客_sift](https://blog.csdn.net/u010440456/article/details/81483145)\n\n[SIFT算法详解_zddhub的博客-CSDN博客_sift](https://blog.csdn.net/zddblog/article/details/7521424?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2-7521424-blog-81483145.pc_relevant_multi_platform_whitelistv1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2-7521424-blog-81483145.pc_relevant_multi_platform_whitelistv1&utm_relevant_index=3)\n\n[关于softmax、argmax、softargmax - 百度文库 (baidu.com)](https://wenku.baidu.com/view/1e08afaea3116c175f0e7cd184254b35eefd1aa4.html)\n\n[CVPR 2022 图像匹配挑战赛回顾 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/539740663)\n\n[LearninCorrespondencefromtheCycleConsistencyofTime - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/261310941)中的一段话很有意思，如下：\n\n![image-20221018194006676](特征匹配/image-20221018194006676.png)\n\n[(54条消息) 巧解图像处理经典难题之图像配准_小白学视觉的博客-CSDN博客](https://blog.csdn.net/qq_42722197/article/details/126775116)\n\n# 线特征匹配\n\nLBD，LSD 算法\n\n\n\nAn efficient and robust line segment matching approach based on LBD descriptor and pairwise geometric consistency\n\n\n\n## \n\n[lh9171338/Line-Segment-Detection-Papers: Line segment detection papers (github.com)](https://github.com/lh9171338/Line-Segment-Detection-Papers)\n\n\n\n# 双目深度图（立体匹配）\n\n参考代码 **[stereoDepth](https://github.com/aliyasineser/stereoDepth)**  在该代码的基础上加一个mask，输出mask内像素的平均距离\n\n\n\n这个是只算了圆心点的距离，不过可以借鉴里面的mask[ComputerVision/main.py at master · niconielsen32/ComputerVision (github.com)](https://github.com/niconielsen32/ComputerVision/blob/master/StereoVision/Python/main.py)\n\n### 立体标定与立体校正\n\n[(30条消息) 【立体视觉】双目立体标定与立体校正_ZealCV的博客-CSDN博客_立体校正](https://blog.csdn.net/u011574296/article/details/73826420)\n\n[来聊聊双目视觉的基础知识（视察深度、标定、立体匹配） - 极术社区 - 连接开发者与智能计算生态 (aijishu.com)](https://aijishu.com/a/1060000000139727)\n\n\n\n# Learning Feature Descriptors using Camera Pose Supervision\n\n![image-20220530163756268](特征匹配/image-20220530163756268.png)\n\ncaps预训练模型在mepdepth测试集测试结果\n\n![image-20220531171555853](特征匹配/image-20220531171555853.png)\n\n\n\n训练测试\n\n```python\n/home/young/anaconda3/envs/sgp/bin/python /home/young/code/SGP/code/perception2d/sgp.py --config /home/young/code/SGP/code/perception2d/config_sgp_sample.yml\nlabel root caps_pseudo_label/bs will be overwritten to enter teaching mode\nDataset size: 3337\n0it [00:00, ?it/s][ WARN:0@1.508] global /io/opencv_contrib/modules/xfeatures2d/misc/python/shadow_sift.hpp (13) SIFT_create DEPRECATED: cv.xfeatures2d.SIFT_create() is deprecated due SIFT tranfer to the main repository. https://github.com/opencv/opencv/issues/16736\n3337it [10:10,  5.47it/s]\nlabel root caps_pseudo_label/bs exists, entering learning mode.\ntensorboard log files are stored in caps_logs/bs/caps_sgp\nNo ckpts found, training from scratch...\n/home/young/anaconda3/envs/sgp/lib/python3.8/site-packages/torch/nn/functional.py:3981: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n  warnings.warn(\n/home/young/anaconda3/envs/sgp/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /tmp/pip-req-build-ex__3qls/aten/src/ATen/native/BinaryOps.cpp:467.)\n  return torch.floor_divide(self, other)\ncaps_sgp | Step: 1, Loss: 0.16486\ncaps_sgp | Step: 2, Loss: 0.08825\ncaps_sgp | Step: 3, Loss: 0.19555\ncaps_sgp | Step: 4, Loss: 0.10034\ncaps_sgp | Step: 5, Loss: 0.08762\nsaving ckpts caps_outputs/bs/caps_sgp/000005.pth...\nlabel root caps_pseudo_label/00 does not exist, entering teaching mode.\n0it [00:00, ?it/s]Reloading from caps_outputs/bs/caps_sgp/000005.pth, starting at step=5\n3337it [16:32,  3.36it/s]\nlabel root caps_pseudo_label/00 exists, entering learning mode.\ntensorboard log files are stored in caps_logs/00/caps_sgp\nNo ckpts found, training from scratch...\ncaps_sgp | Step: 1, Loss: 0.16329\ncaps_sgp | Step: 2, Loss: 0.13774\ncaps_sgp | Step: 3, Loss: 0.12036\ncaps_sgp | Step: 4, Loss: 0.11186\ncaps_sgp | Step: 5, Loss: 0.11013\nsaving ckpts caps_outputs/00/caps_sgp/000005.pth...\nlabel root caps_pseudo_label/01 does not exist, entering teaching mode.\nReloading from caps_outputs/00/caps_sgp/000005.pth, starting at step=5\n3337it [16:32,  3.36it/s]\nlabel root caps_pseudo_label/01 exists, entering learning mode.\ntensorboard log files are stored in caps_logs/01/caps_sgp\nNo ckpts found, training from scratch...\ncaps_sgp | Step: 1, Loss: 0.21352\ncaps_sgp | Step: 2, Loss: 0.08665\ncaps_sgp | Step: 3, Loss: 0.08282\ncaps_sgp | Step: 4, Loss: 0.12540\ncaps_sgp | Step: 5, Loss: 0.09589\nsaving ckpts caps_outputs/01/caps_sgp/000005.pth...\n\nProcess finished with exit code 0\n\n```\n\n\n\n\n\n\n\n\n\n# 论文中提到的问题\n\n- 特征检测器比描述符更难学习，因为对于描述符的定义更为明确，而人们通常不清楚哪些点是有趣的，难以进行人为标记\n- 特征点的性质：可区分和可重复\n\t- 1.能够发现图像中局部的具备区分度的信息；同一性约束（identity constraint）\n\t\t2.当场景经过多种变换后，仍能够重复检测出一致的特征；协变约束（covariant constraint ）\n- 虽然机器学习显然有助于照明不变性，但对于视点不变性，传统方法仍然具有惊人的竞争力。\n- 监督学习不能很好地作用于关键点的检测，因为在这种情况下，监督学习的本质可以归结为复制现有的检测器，而不是发现更好、更容易的关键点。更有效的方向是自监督/无监督的学习\n- 将网络的某部分用传统方法代替，能极大减少网络的参数量\n- 数据集的质量>数量？\n\n\n\n## 损失函数\n\n[度量学习 - 损失函数汇总[译\\] - AI备忘录 (aiuai.cn)](https://www.aiuai.cn/aifarm1697.html)\n\n[深度学习loss清单-未完待续 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/80761087?from_voters_page=true)\n\n\n\n\n\n# CVPR 2022 Image matching challenge\n\n有用的点：\n\n- 对匹配聚类除去外点后框选bbox，再进行匹配\n\n\n\n\n\n无用的点：\n\n- 使用分割过滤移动对象\n- \n\n\n\n#  The MegaDepth Dataset\n\n- colmap的sfm和mvs重建，得到原始深度图\n\n- 采用更谨慎的mvs方法，更倾向于少而准确的真实数据，未计算出真实值的点输出为0\n- 采用语义分割来增强和过滤深度图，过滤了天空部分的深度，以及移动物体的深度。\n\n\n\n**Multi-View Optimization of Local Feature Geometry 中提到：**\n\n由于稀疏和密集重建之间的不一致，我们丢弃了 16 个场景。虽然 MegaDepth 数据集提供了跨多种视点和照明条件的训练数据，但由于密集重建中的错误，地面实况流有时不是完全准确的亚像素。因此，我们使用参考补丁的随机扭曲来合成具有完美几何流注释的相同条件补丁对，以生成合成对应物。\n\n# Loftr\n\n```\n\ndata_cfg_path=\"configs/data/megadepth_trainval_640.py\"\nmain_cfg_path=\"configs/loftr/outdoor/loftr_ds_dense.py\"\n\nn_nodes=1\nn_gpus_per_node=4\ntorch_num_workers=0\nbatch_size=1\npin_memory=true\nexp_name=\"outdoor-ds-640\"\n\npython -u ./train.py \"configs/data/megadepth_trainval_640.py\" \"configs/loftr/outdoor/loftr_ds_dense.py\" --exp_name=\"outdoor-ds-640\" --gpus=4 --num_nodes=1 --accelerator=\"ddp\" --batch_size=1 --num_workers=4 --pin_memory=true --check_val_every_n_epoch=1 --log_every_n_steps=1 --flush_logs_every_n_steps=1 --limit_val_batches=1. --num_sanity_val_steps=10 --benchmark=True --max_epochs=30\n```\n\n\n\n## 不变性与等变性\n\nhttps://zhuanlan.zhihu.com/p/427205686\n\n![image-20221013105241665](特征匹配/image-20221013105241665.png)\n\n描述符应该是不变性的，特征点位置应该是等变性的\n\n\n\n## UnSuperPoint:End-to-End Unsupervised Interest Point Detector And Descriptor\n\n![在这里插入图片描述](特征匹配/30262663c9724bd29d37ff71a4bb3557.png)\n\n![在这里插入图片描述](特征匹配/87d6fe3e3daf4388bf782cdadee9af4f.png)\n\n\n\n\n\n## 综述\n\n\n\nimage matching, also known as image registration or correspondence\n\naims to identify then correspond the same or similar structure/content from two ormore images\n\n","categories":["位姿估计"]},{"title":"飞机6D位姿估计","url":"/2021/12/10/计算机视觉/位姿估计/飞机姿态估计(杨业鹏-20220310203222)/","content":"\n数据集不是问题，只需要模型，每张图片的姿态，mask（这个可以由前两者知道）\n\n可以用来制作数据集[GitHub - 3D-FRONT-FUTURE/3D-FUTURE-ToolBox: FUTURE3D Toolbox: Rendering, Projection, and Re-Projection](https://github.com/3D-FRONT-FUTURE/3D-FUTURE-ToolBox#model-aligned-rendering)\n\nopencv姿态估计\n\n![image-20220218151001060](飞机姿态估计/image-20220218151001060.png)\n\n![image-20220218151033771](飞机姿态估计/image-20220218151033771.png)\n\n\n\n## 相机参数 （opencv 是竖着来的）\n\n**牛博相机**\n\n内参\n\n792.216153833603\t0\t0\n0\t791.177146289919\t0\n308.699841948659\t229.681830886849\t1\n\n![image-20220219165155403](飞机姿态估计/image-20220219165155403.png)\n\n畸变系数\n\n​      cameraParams.RadialDistortion                          0.159289987523910\t-0.944011768029619\n\n​      cameraParams.TangentialDistortion                  0\t0\n\n​      cameraParams.EstimateTangentialDistortion   0\n\n\n\n![image-20220222191218258](飞机姿态估计/image-20220222191218258.png)\n\n\n\n**涛哥相机**\n\n内参\n\n1312.06174094713\t0\t0\n0\t1309.31954409816\t0\n916.121844560839\t518.441493882532\t1\n\n畸变系数\n\n​    cameraParams.RadialDistortion                          -0.369910731594361\t0.123663102946957\n\n​      cameraParams.TangentialDistortion                  0\t0\n\n​      cameraParams.EstimateTangentialDistortion  0\n\n\n\n**工业相机**\n\n![image-20220306194950707](飞机姿态估计/image-20220306194950707.png)\n\nhttps://www.cnblogs.com/gooutlook/p/15240265.html\n\n然后就可以用cv2.VideoCapture()运行了\n\n12mm镜头：\n\n内参\n\n3603.41629066370\t                           0\t                                          0\n0\t                                                      3598.94564763421\t               0\n717.982210742481\t                      516.831199201901\t               1\n\n畸变系数\n\ncameraParams.RadialDistortion          0.125212949613634\t-0.596274067781849\n\ncameraParams.TangentialDistortion                  0\t                              0\n\ncameraParams.EstimateTangentialDistortion  0\n\n\n\n6mm镜头：\n\n内参：\n\n1839.64945443219\t0\t0\n0\t1839.25847451371\t0\n675.840419167291\t550.401546804488\t1\n\n![image-20220716191300260](飞机姿态估计(杨业鹏-20220310203222)/image-20220716191300260.png)\n\n畸变系数：\n\ncameraParams.RadialDistortion         0.0154582469945801\t-0.0450341498973294\n\ncameraParams.TangentialDistortion                  0\t                              0\n\ncameraParams.EstimateTangentialDistortion  0\n\n![image-20220716191529975](飞机姿态估计(杨业鹏-20220310203222)/image-20220716191529975.png)\n\n![image-20220716191619052](飞机姿态估计(杨业鹏-20220310203222)/image-20220716191619052.png)\n\n**手机相机**\n\n![image-20220429103711210](飞机姿态估计(杨业鹏-20220310203222)/image-20220429103711210.png)\n\n2304.000000, 2304.000000, 960.000000, 540.000000, 0.000000, 0.000000, 0.000000, 0.000000\n\nopencv格式：\n\n2304.000000        0        960.000000    0     2304.000000    540.000000   0     0       1 \n\n \n\npython调用手机ip摄像头[(42条消息) python调用手机摄像头 - CSDN](https://www.csdn.net/tags/MtjakgxsMDQ3Ni1ibG9n.html)\n\n\n\n## 特征点提取测试\n\n### sift\n\n![image-20220219171551879](飞机姿态估计/image-20220219171551879.png)\n\n匹配成功的特征点数目为：142，筛选后的特征点数目为：60,ransac后的特征点数目为：28\n时间：0.240354s\n\n![image-20220222094720459](飞机姿态估计/image-20220222094720459.png)\n\n\n\n### orb\n\n![image-20220219171614674](飞机姿态估计/image-20220219171614674.png)\n\n匹配成功的特征点数目为：468，筛选后的特征点数目为：180,ransac后的特征点数目为：75\n时间：0.237791s\n\n![image-20220222094802357](飞机姿态估计/image-20220222094802357.png)\n\n\n\n3dmodel的细节太少，完全配不准\n\n![image-20220222093435654](飞机姿态估计/image-20220222093435654.png)\n\n\n\n### 方法\n\n基于对应关系的方法主要针对纹理丰富的目标物体，首先将需要计算位姿的目标物体的3D模型投影到N个角度，得到N张2D模板图像，记录这些模板图上2D像素和真实3D点的对应关系。当单视角相机采集到RGB图像后，通过特征提取（SIFT，FAST，ORB等），寻找特征点与模板图片之间的对应关系。通过这种方式，可以得到当前相机采集图像的2D像素点与3D点的对应关系。最后使用PnP算法即可恢复当前视角下图像的位姿。\n\n\n\n如果目标物体有丰富纹理，可以寻找当前目标2D特征点与模版图像中特征点的对应，而模板图像的特征点具有3D坐标，这样就可以使用PNP算法得到物体6D位姿了；具体可以参考ORB-SLAM2，先构建场景的稀疏3D点，同时保存一些关键帧，关键帧上的2D ORB特征点对应3D点，这样可以对当前目标图像，寻找最相似关键帧，然后根据ORB特征点的匹配使用PNP得到粗位姿(重定位)，也可以再用BA优化；如果想要去除背景干扰，可以检测出目标物体再匹配；\n\n\n\n2.如果目标是弱纹理或者没有纹理，那么可以利用轮廓匹配，此时模板图像可以是轮廓图，可以看看LineMode或者一些基于草图的3D模型检索算法，将当前目标提取的轮廓图和已有的模版轮廓图匹配，寻找最相似轮廓图对应的6D位姿；Vuforia有物体3D检测跟踪的Demo，变换视角使当前目标的轮廓和预先设定的轮廓重合，表示得到对应的位姿，再进行跟踪；3.基于RGB图像获得物体的深度图，转为点云和已有3D模型的点云进行粗配准和细配准，单目的有很多基于深度学习的算法，双目的可以自己算；4.如果能够具有高逼真3D模型，可以基于仿真环境生成大量虚拟6D位姿训练集，再在少量真实采集的6D位姿数据集上FineTune，应该是也可以的；你看情况试一试，方法的鲁棒性得看目标物体本身的特性以及具体应用场景的复杂程度吧；\n\n\n\n\n\n采集数据（包括RGB和深度图）\n\n![image-20220222165734657](飞机姿态估计/image-20220222165734657.png)\n\n![image-20220222210155972](飞机姿态估计/image-20220222210155972.png)\n\n杂乱背景下\n\n![image-20220222211656964](飞机姿态估计/image-20220222211656964.png)\n\n\n\n1. RGB-D 图像中的rgb图片提供了像素坐标系下的x，y坐标，而深度图直接提供了相机坐标系下的𝑍坐标，也就是相机与点的距离。\n2. 根据 RGB-D 图像的信息和相机的内参，可以计算出任何一个像素点在**相机坐标系**下的坐标。\n3. 根据 RGB-D 图像的信息和相机的内参与外参，可以计算出任何一个像素点在世界坐标系下的坐标。\n4. 相机视野范围内，相机坐标系下的障碍物点的坐标，就是[点云](https://so.csdn.net/so/search?q=点云&spm=1001.2101.3001.7020)传感器数据，也就是相机坐标系下的点云数据。点云传感器数据可以根据 RGB-D 图像提供的坐标与相机内参算出来。\n5. 所有世界坐标系下的障碍物点的坐标，就是点云地图数据，也就是世界坐标系下的点云数据。点云地图数据可以根据RGB-D 图像提供的坐标与相机内参和外参算出来。\n\n\n\n![image-20220224172757631](飞机姿态估计/image-20220224172757631.png)\n\n\n\nsvm分类 2d-3d配准\n\n\n\n\n\n\n\n\n\n\n\n### 将图像沿x轴翻转后验证匹配效果(翻转不同于旋转)\n\norb+单应\n\n![image-20220301103819128](飞机姿态估计/image-20220301103819128.png)\n\nsift+单应 （效最好）\n\n![image-20220301104133380](飞机姿态估计/image-20220301104133380.png)\n\nsift+基础矩阵（误配太多）\n\n![image-20220301104729842](飞机姿态估计/image-20220301104729842.png)\n\n\n\n经测试，sift+单应ransac效果最好\n\n\n\n\n\nflann匹配效果还不如暴力匹配，且时间也不会快很多，如下为flann匹配效果\n\n![image-20220301140828199](飞机姿态估计/image-20220301140828199.png)\n\n\n\n\n\n\n\nransac 次数太少导致算的不准确\n\n\n\n\n\n[Celebrandil/CudaSift: A CUDA implementation of SIFT for NVidia GPUs (1.2 ms on a GTX 1060) (github.com)](https://github.com/Celebrandil/CudaSift)\n\n\n\nransac算法当匹配不到的时候会出现这种情况\n\n\n\n![image-20220330153303463](飞机姿态估计(杨业鹏-20220310203222)/image-20220330153303463.png)\n\n或许自己编写ransac，可以排除多点汇聚一点的情况\n\n[HomographyEstimation/Homography.py at master · hughesj919/HomographyEstimation (github.com)](https://github.com/hughesj919/HomographyEstimation/blob/master/Homography.py)\n\n\n\n### LoFTR 效果\n\n[LoFTR_demo_single_pair.ipynb - Colaboratory (google.com)](https://colab.research.google.com/drive/1BgNIOjFHauFoNB95LGesHBIjioX74USW?usp=sharing#scrollTo=BSBHOV3GFUc3)\n\noutdoor_ds\n\n![image-20220810150819782](/mnt/data/blog/计算机视觉/位姿估计/飞机姿态估计(杨业鹏-20220310203222)/image-20220810150819782.png)\n\n\n\nindoor_ds_new\n\n![image-20220810150737221](/mnt/data/blog/计算机视觉/位姿估计/飞机姿态估计(杨业鹏-20220310203222)/image-20220810150737221.png)\n\n```python\n# Draw \n# M, mask = cv2.findHomography(mkpts0, mkpts1, method=cv2.RANSAC, ransacReprojThreshold=1.0,confidence=0.9999,maxIters=10000)\nM, mask = cv2.findFundamentalMat(mkpts0, mkpts1, method=cv2.RANSAC, ransacReprojThreshold=1.0,confidence=0.9999,maxIters=10000)\nprint(mkpts0.shape)\nkps0 = []\nkps1 = []\nmconf_1 = []\nfor i in range(len(mask)):\n    if mask[i] == 1:\n        kps0.append(mkpts0[i])\n        kps1.append(mkpts1[i])\n        mconf_1.append(mconf[i])\nmkpts0 = np.float32(kps0).reshape(-1,2)\nmkpts1 = np.float32(kps1).reshape(-1,2)\nmconf = mconf_1\nprint(mkpts0.shape)\nprint(mconf)\ncolor = cm.jet(mconf, alpha=0.7)\ntext = [\n    'LoFTR',\n    'Matches: {}'.format(len(mkpts0)),\n]\nfig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text)\n\n# A high-res PDF will also be downloaded automatically.\nmake_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text, path=\"LoFTR-colab-demo.pdf\")\nfiles.download(\"LoFTR-colab-demo.pdf\")\n```\n\n不加ransac\n\n![image-20220722110121475](飞机姿态估计(杨业鹏-20220310203222)/image-20220722110121475.png)\n\n\n\n![image-20220722105527038](飞机姿态估计(杨业鹏-20220310203222)/image-20220722105527038.png)\n\n加ransac\n\n![image-20220722105557805](飞机姿态估计(杨业鹏-20220310203222)/image-20220722105557805.png)\n\n![Image](飞机姿态估计(杨业鹏-20220310203222)/E-7cRlXXEAQm4rP)\n\n![Image](飞机姿态估计(杨业鹏-20220310203222)/E-7fUp5WEAAda8S)\n\n对于旋转图像不行\n\n![image-20220805141445728](飞机姿态估计(杨业鹏-20220310203222)/image-20220805141445728.png)\n\nransac后\n\n![image-20220805141537159](飞机姿态估计(杨业鹏-20220310203222)/image-20220805141537159.png)\n\n![image-20220805141954237](飞机姿态估计(杨业鹏-20220310203222)/image-20220805141954237.png)\n\n![image-20220805141937782](飞机姿态估计(杨业鹏-20220310203222)/image-20220805141937782.png)\n\n![image-20220805142217265](飞机姿态估计(杨业鹏-20220310203222)/image-20220805142217265.png)\n\n![image-20220805142224530](飞机姿态估计(杨业鹏-20220310203222)/image-20220805142224530.png)\n\n\n\n对于upright图像效果就很好\n\n![image-20220805142459812](飞机姿态估计(杨业鹏-20220310203222)/image-20220805142459812.png)\n\n\n\n<img src=\"飞机姿态估计(杨业鹏-20220310203222)/选区_015.png\" alt=\"选区_015\" style=\"zoom: 20%;\" /><img src=\"飞机姿态估计(杨业鹏-20220310203222)/选区_016-1659870333888.png\" alt=\"选区_016\" style=\"zoom: 20%;\" />\n\n![image-20220807190447071](飞机姿态估计(杨业鹏-20220310203222)/image-20220807190447071.png)\n\n![image-20220807190323929](飞机姿态估计(杨业鹏-20220310203222)/image-20220807190323929.png)\n\n\n\n\n\n旋转测试 10°\n\n![image-20220807191915518](飞机姿态估计(杨业鹏-20220310203222)/image-20220807191915518.png)\n\n20°\n\n![image-20220807192011326](飞机姿态估计(杨业鹏-20220310203222)/image-20220807192011326.png)\n\n30°\n\n![image-20220807193601106](飞机姿态估计(杨业鹏-20220310203222)/image-20220807193601106.png)\n\n\n\n40°\n\n![image-20220807193249541](飞机姿态估计(杨业鹏-20220310203222)/image-20220807193249541.png)\n\n单应矩阵\n\n![image-20220807193446637](飞机姿态估计(杨业鹏-20220310203222)/image-20220807193446637.png)\n\n基础矩阵\n\n![image-20220807200756731](飞机姿态估计(杨业鹏-20220310203222)/image-20220807200756731.png)\n\n50°\n\n![image-20220807193851167](飞机姿态估计(杨业鹏-20220310203222)/image-20220807193851167.png)\n\n![image-20220807193858105](飞机姿态估计(杨业鹏-20220310203222)/image-20220807193858105.png)\n\n\n\n60°\n\n![image-20220807193951562](飞机姿态估计(杨业鹏-20220310203222)/image-20220807193951562.png)\n\n![image-20220807194001962](飞机姿态估计(杨业鹏-20220310203222)/image-20220807194001962.png)\n\norb具有旋转不变性\n\n![image-20220807201347923](飞机姿态估计(杨业鹏-20220310203222)/image-20220807201347923.png)\n\n其他视角测试\n\n感觉下面这张图，一些语义上的明显特征并没有检测出来，比如转角处\n\n![image-20220807195418356](飞机姿态估计(杨业鹏-20220310203222)/image-20220807195418356.png)\n\n用findFundamentalMat后还行\n\n![image-20220807200350733](飞机姿态估计(杨业鹏-20220310203222)/image-20220807200350733.png)\n\norb效果不行\n\n![image-20220807202758873](飞机姿态估计(杨业鹏-20220310203222)/image-20220807202758873.png)\n\n### orb的测试\n\n![image-20220722105926685](飞机姿态估计(杨业鹏-20220310203222)/image-20220722105926685.png)\n\n![image-20220722110035632](飞机姿态估计(杨业鹏-20220310203222)/image-20220722110035632.png)\n\n![image-20220722110443971](飞机姿态估计(杨业鹏-20220310203222)/image-20220722110443971.png)\n\n![image-20220722110533388](飞机姿态估计(杨业鹏-20220310203222)/image-20220722110533388.png)\n\n\n\n![image-20220722110213392](飞机姿态估计(杨业鹏-20220310203222)/image-20220722110213392.png)\n\n![image-20220722110326300](飞机姿态估计(杨业鹏-20220310203222)/image-20220722110326300.png)\n\n### 可以尝试下面的方法\n\n[修改OpenCV一行代码，提升14%图像匹配效果 - 水木清扬 - 博客园 (cnblogs.com)](https://www.cnblogs.com/shuimuqingyang/p/14428270.html)\n\n[beblid-opencv-demo/demo.py at main · iago-suarez/beblid-opencv-demo (github.com)](https://github.com/iago-suarez/beblid-opencv-demo/blob/main/demo.py)\n\n经过测试，该描述子（BEBLID）效果比ORB更好，匹配对数更多，且计算更快\n\n![image-20220325134532890](飞机姿态估计(杨业鹏-20220310203222)/image-20220325134532890.png)\n\n\n\n![image-20220325134541777](飞机姿态估计(杨业鹏-20220310203222)/image-20220325134541777.png)\n\n\n\n\n\n\n\n## 制作6D姿态数据集\n\nhttps://github.com/DLR-RM/BlenderProc\n\n![image-20220303204720768](/mnt/data/blog/AI/飞机姿态估计/image-20220303204720768.png)\n\nWhile distance and depth images sound similar, **they are not the same**: In [distance images](https://en.wikipedia.org/wiki/Range_imaging), each pixel contains the actual distance from the camera position to the corresponding point in the scene. In [depth images](https://en.wikipedia.org/wiki/Depth_map), each pixel contains the distance between the camera and the plane parallel to the camera which the corresponding point lies on.\n\n在距离图像中，每个像素包含从相机位置到场景中相应点的实际距离。\n\n在深度图像中，每个像素包含相机和平行于相机的平面之间的距离，对应点位于该平面上。\n\n要算PNP应该用 **depth image**\n\n\n\n在bop_object_pose_sampling中ply格式带颜色可以直接显示颜色\n\n```\nblenderproc run examples/datasets/bop_object_on_surface_sampling/main.py \n              <path_to_bop_data> \n              <bop_dataset_name> \n              resources/cctextures \n              examples/datasets/bop_object_on_surface_sampling/output\n              \nblenderproc run examples/datasets/bop_object_on_surface_sampling/main.py /media/young/young/dataset/linemod lm cc1 examples/datasets/bop_object_on_surface_sampling/output              \n```\n\nblenderproc run blenderproc/scripts/download_cc_textures_1.py cc1\n\n```\nblenderproc run examples/datasets/bop_challenge/config_lm_upright.yaml /media/young/young/dataset/linemod lm /home/young/code/bop_toolkit cc1 examples/datasets/bop_challenge/output\n```\n\nblenderproc run blenderproc/scripts/download_cc_textures_1.py cc1\n\n\n​              \n\n```\nblenderproc run examples/datasets/bop_challenge/config_lm_upright.yaml /media/young/young/dataset/linemod lm /home/young/code/bop_toolkit cc1 examples/datasets/bop_challenge/output\nblenderproc run examples/datasets/bop_challenge/main_lm_upright.py /media/young/young/dataset/linemod cc1  examples/datasets/bop_challenge/output --num_scenes=1\n\nblenderproc run examples/datasets/bop_challenge/main_tless_random_test.py /media/young/young/dataset/linemod cc1  examples/datasets/bop_challenge/output2 --num_scenes=1\n```\n\n\n\n**进入debug模式**\n\n```\nimport pydevd_pycharm\npydevd_pycharm.settrace('localhost', port=12345, stdoutToServer=True, stderrToServer=True)\n```\n\n\n\n#### 生成自定义数据集步骤\n\n1.在blenderproc主目录下运行（base环境）**生成深度图和RGB**\n\nblenderproc run examples/datasets/bop_challenge/main_tless_random_test.py /media/young/young/dataset/linemod cc1  examples/datasets/bop_challenge/output2 --num_scenes=1\n\n然后在生成的数据集目录下加入模型文件\n\n<img src=\"飞机姿态估计/image-20220307221151331.png\" alt=\"image-20220307221151331\" style=\"zoom:50%;\" />\n\n2.运行calc_gt_masks.py（test环境）在/home/young/code/bop_toolkit/下  **生成mask**\n\npython calc_gt_masks.py\n\n\n\n\n\n\n\n#### 后续安排\n\n1. 选择合适的算法并测试效果，最好是能直接接入视频流的\n2. 自己用数据集训练\n3. 训练测试效果\n4. 思考改进\n\n\n\n\n\n▪\n\n▪ 测试传统方案模板切换效果\n\n▪测试传统方案模板切换效果，拍摄了从飞机头向机尾方向拍摄的十几张模板图像，根据匹配的数量来切换模板：当匹配数低于一定数量时，切换模板，遍历所有模板并选择匹配数最高的那一个作为当前模板。\n\n▪ 研究svm识别特征点的方案\n\n▪1. 数据集制作：先用sift提取特征点，每张图手动选取空间位置相同的N个关键点，在这些关键点周围选择一个patch，即每张图选择N个以关键点为中心的patch\n\n▪2. 训练N个SVM对来识别输入的patch是不是属于这一类\n\n▪3. 最终测试时也是先提取特征点周围的patch作为SVM的输入，判断是否属于某一类关键点\n\n▪4. 根据识别出的关键点进行pnp解算位姿\n\n\n\n\n\n## 有价值的参考\n\n[Object pose estimation and tracking using OpenCV APIs. - YouTube](https://www.youtube.com/watch?v=iIAoc99doI4)\n\n核心思想：\n\n1.Capture the scene from two views.2. Estimate the sift features in both image. 3. Pair the matching features and using triangulation, compute the corresponding 3D coordinates wrt local object frame. 4. Now you have object image, its Sift features and descriptors, and corresponding to each features you have its 3D coordinates.  5. Compute the sift features on new image and compare with the saved features and you will have your object pose in new image.\n\n<img src=\"飞机姿态估计(杨业鹏-20220310203222)/image-20220326100404001.png\" alt=\"image-20220326100404001\" style=\"zoom:30%;\" />\n\n\n\n[[IROS'12\\] 3D Textureless Object Detection and Tracking: An Edge-based Approach - YouTube](https://www.youtube.com/watch?v=UJtpBxdDVDU)\n\n传统的基于边缘的无纹理姿态估计（有空可以参考一下）\n\n\n\n[Measure the size of an object | with Opencv, Aruco marker and Python - YouTube](https://www.youtube.com/watch?v=lbgl2u6KrDU)\n\n通过使用Aruco marker来进行物体的尺寸估计(矩形物体)\n\n其实和姿态估计关联性不大，相机只能正对，通过检测标志，进行等比例的尺寸估计\n\n<img src=\"飞机姿态估计(杨业鹏-20220310203222)/image-20220326100539423.png\" alt=\"image-20220326100539423\" style=\"zoom:25%;\" />\n\n\n\n\n\n[OpenCV tutorial: Real Time pose estimation of a textured object (BRISK) - YouTube](https://www.youtube.com/watch?v=Eq2WNv1blfc)\n\nopencv官方实例，c++实现\n\n\n\n[Pose Estimation of Objects in OpenCV Python - YouTube](https://www.youtube.com/watch?v=US9p9CL9Ywg)\n\n估计棋盘格的位姿，不过默认深度为零，值得参考\n\n\n\n[Optical Flow Tracking Grid and its use for Real-Time Object Detection - YouTube](https://www.youtube.com/watch?v=LjjJQ81RbX0)\n\n光流法进行物体实时跟踪\n\n\n\n**这个代码的思想可以借鉴，以及写代码的格式可以学习**\n\n[image_pose_estimation/image_processing.py at 63926444e53b227d3e65ca9a27f4b2512ee72386 · GigaFlopsis/image_pose_estimation (github.com)](https://github.com/GigaFlopsis/image_pose_estimation/blob/63926444e53b227d3e65ca9a27f4b2512ee72386/src/image_processing.py#L150)\n\n[Ros Image Pose Estimation with blur detector - YouTube](https://www.youtube.com/watch?v=V6yGv8Z46hM)\n\n\n\n\n\n**这个人的视频挺好的**    不过主要是了解一些库的使用\n\n[Pose Estimation of Objects in OpenCV Python - YouTube](https://www.youtube.com/watch?v=US9p9CL9Ywg)\n\n[niconielsen32/ComputerVision (github.com)](https://github.com/niconielsen32/ComputerVision)\n\n\n\n飞机图像分割\n\n[airplane-segmentation/image-segmentation.ipynb at main · rguitar96/airplane-segmentation (github.com)](https://github.com/rguitar96/airplane-segmentation/blob/main/image-segmentation.ipynb)\n\n\n\n**orb-slam值得借鉴**\n\n[ORB_SLAM/Initializer.cc at master · raulmur/ORB_SLAM (github.com)](https://github.com/raulmur/ORB_SLAM/blob/master/src/Initializer.cc)\n\n[orb-slam-py/vo.py at master · borgwang/orb-slam-py (github.com)](https://github.com/borgwang/orb-slam-py/blob/master/vo.py)\n\n\n\n## 计算单应分数\n\nsymmetric transfer errors[(23条消息) 重投影误差与对称转移误差_MeJnCode的博客-CSDN博客_重投影误差](https://blog.csdn.net/sinat_17496535/article/details/51673285)\n\n\n\nmatchsocre\n\n​\t91530.96435108903\n\nknnmatchscore\n\n​\t126337.44244072789      34.97s      兼具速度和分数    \n\nbfsocre\n\n​\t131447.43567962793  速度最慢但是分数高\n\n\n\n灰度+knnmatch        用灰度图效果貌似和彩色差不多\n\ntotal_score: 128866.53040396319\n总时间：34.372146s\n\n![image-20220330105008400](飞机姿态估计(杨业鹏-20220310203222)/image-20220330105008400.png)\n\n![image-20220330170325486](飞机姿态估计(杨业鹏-20220310203222)/image-20220330170325486.png)\n\n以上的情况要么是初步筛选不够，要么是点选的太多了，\n\n\n\n飞机轮廓\n\nsfm\n\n算rotation\n\n\n\n\n\n\n\n```\norb.detect_and_compute_beblid_knn(max_temp_pic,input_image,0.93,ransacReprojThreshold=2,confidence=0.95,maxIters=10000)\ntotal_score: 108451.39269577163\n\n orb.detect_and_compute_beblid_bf(max_temp_pic,input_image,3,ransacReprojThreshold=2,confidence=0.95,maxIters=10000)\ntotal_score: 94354.40717807441\n\norb.detect_and_compute_orb_bf(max_temp_pic,input_image,3,ransacReprojThreshold=2,confidence=0.95,maxIters=10000)\ntotal_score: 49999.934895973216\n\n\nself.orb = cv.ORB_create(extract_number, 1.2, 8, 10, 0, 2, cv.ORB_HARRIS_SCORE, 10, 20)\nself.descriptor = cv.xfeatures2d.BEBLID_create(2)\ntotal_score: 115350.02440894888\n```\n\n## 三维重建\n\n完整流程参考[Creating 3D Game Models from Video using Photogrammetry - YouTube](https://www.youtube.com/watch?v=bDHJM6nAKtc)\n\n### colmap\n\n[【踩坑】colmap中的相机位姿定义及其可视化结果的隐含转换-爱代码爱编程 (icode.best)](https://icode.best/i/80715147507186)\n\n[SIFT特征详解 - 简书 (jianshu.com)](https://www.jianshu.com/p/94196a92cc18)\n\n使用17张图进行3维重建 ![image-20220224200905982](飞机姿态估计/image-20220224200905982.png)\n\n![2022-02-28 16-49-13 的屏幕截图](/mnt/data/blog/AI/飞机姿态估计.assets/2022-02-28 16-49-13 的屏幕截图.png)\n\n![2022-02-28 16-50-05 的屏幕截图](/mnt/data/blog/AI/飞机姿态估计.assets/2022-02-28 16-50-05 的屏幕截图.png)\n\n[Output Format — COLMAP 3.8 documentation](https://colmap.github.io/format.html)\n\n[(25条消息) 尺度不变特征转换SIFT_GarfieldEr007的博客-CSDN博客](https://blog.csdn.net/GarfieldEr007/article/details/50333157)\n\n读取colmap生成的文件信息：\n\n[Blender-Addon-Photogrammetry-Importer/read_write_model.py at f79c09ea9eda34574d20ed05ceda9226ce7f778c · SBCV/Blender-Addon-Photogrammetry-Importer (github.com)](https://github.com/SBCV/Blender-Addon-Photogrammetry-Importer/blob/f79c09ea9eda34574d20ed05ceda9226ce7f778c/photogrammetry_importer/ext/read_write_model.py)\n\n\n\n<img src=\"飞机姿态估计(杨业鹏-20220310203222)/image-20220422114317283.png\" alt=\"image-20220422114317283\" style=\"zoom:50%;\" />\n\n\n\n<img src=\"飞机姿态估计(杨业鹏-20220310203222)/image-20220429111227223.png\" alt=\"image-20220429111227223\" style=\"zoom:50%;\" />\n\n\n\nblack plane 关键点3d坐标：\n\n-0.750285, 1.89654, 2.12803\n\n2.09217, 2.07706, 3.77801\n\n3.06018, 1.19712, 5.24177\n\n3.62335, 1.34729, 5.57277\n\n3.14614, 2.24611, 4.19743\n\n4.50369, 1.61071, 3.76958（可能不准）\n\n4.71773, 2.53198, 2.84707\n\n3.36851, 3.18192, 3.22556\n\n4.18717, 4.45633, 2.43622\n\n3.66017, 4.333, 2.20296\n\n2.20928, 2.97569, 2.84228\n\n\n\n\n\n\n\nblue plane 关键点3d坐标：\n\n-0.280649, 1.68422, 2.36646\n\n2.54254, 1.73229, 3.39873\n\n3.73118, 0.98228, 4.81425\n\n4.29214, 1.09763, 4.95887\n\n3.77839, 1.93858, 3.73114\n\n4.86345, 1.44302, 3.34977\n\n4.89913, 2.31779, 2.45851\n\n3.80463, 2.78573, 2.84414\n\n4.49756, 3.98496, 2.07535\n\n3.95248, 3.8895, 1.9103\n\n2.59718, 2.55167, 2.60149\n\n\n\n### opencv sfm\n\n[Ashok93/Structure-From-Motion-SFM-: Structure from Motion (Sfm) in Python using OpenCV (github.com)](https://github.com/Ashok93/Structure-From-Motion-SFM-)\n\n\n\n[CesarAsturias/SFM-and-numerical-optimization: Structure from Motion with Python and OpenCV, using numerical optimization techniques (github.com)](https://github.com/CesarAsturias/SFM-and-numerical-optimization)这个代码注释不错\n\n\n\n[mapillary/OpenSfM: Open source Structure-from-Motion pipeline (github.com)](https://github.com/mapillary/OpenSfM)\n\n<img src=\"飞机姿态估计(杨业鹏-20220310203222)/image-20220405000136370.png\" alt=\"image-20220405000136370\" style=\"zoom:50%;\" />\n\n\n\n这个纯python而且有教程，可以尝试\n\n[muneebaadil/how-to-sfm: A self-reliant tutorial on Structure-from-Motion (github.com)](https://github.com/muneebaadil/how-to-sfm)\n\n\n\n## 精度测试\n\n![image-20220517164105523](飞机姿态估计(杨业鹏-20220310203222)/image-20220517164105523.png)\n\n\n\n\n\n## 改进\n\nhttps://opencv.org/evaluating-opencvs-new-ransacs/  ransac的改进\n\n\n\n\n\n# 关键点检测\n\n- [如何使用 PyTorch 训练自定义关键点检测模型_求则得之，舍则失之的博客-CSDN博客_关键点检测模型](https://blog.csdn.net/weixin_43229348/article/details/123500917)\n\n- [关键点检测数据准备和基于U-net网络的模型设计——工业组件4个关键点的检测模型_君名余曰正则的博客-CSDN博客_工业关键点检测](https://blog.csdn.net/weixin_41782172/article/details/119249916?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-3-119249916-blog-123500917.pc_relevant_aa2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-3-119249916-blog-123500917.pc_relevant_aa2&utm_relevant_index=4)\n\n\t[ExileSaber/KeyPoint-Detection: 关键点检测的模型，包括直接使用关键点坐标作为返回值、使用heatmap作为返回值的两种模型 (github.com)](https://github.com/ExileSaber/KeyPoint-Detection)\n\n- [PaddleDetection/configs/keypoint at release/2.4 · PaddlePaddle/PaddleDetection · GitHub](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/keypoint)\n\n- [OpenCV 中图像坐标系统与Python中NumPy Arrays之间的关系_大熊背的博客-CSDN博客_numpy坐标原点](https://blog.csdn.net/lz0499/article/details/80978433)\n\n- 数据增强[torchlm/transforms.md at main · DefTruth/torchlm (github.com)](https://github.com/DefTruth/torchlm/blob/main/docs/api/transforms.md)\n\n![image-20220712144612104](飞机姿态估计(杨业鹏-20220310203222)/image-20220712144612104.png)\n\n\n\ntorch.backends.cudnn.benchmark 可以加速网络\n\n\n\n![image-20220717212727457](飞机姿态估计(杨业鹏-20220310203222)/image-20220717212727457.png)\n\n![image-20220717212735537](飞机姿态估计(杨业鹏-20220310203222)/image-20220717212735537.png)\n\nTraceback (most recent call last):\n  File \"airplane_kps_pose\\main\\test_main_yolo.py\", line 11, in <module>\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n  File \"PyInstaller\\loader\\pyimod03_importers.py\", line 495, in exec_module\n  File \"torch\\__init__.py\", line 135, in <module>\n    raise err\nOSError: [WinError 126] 找不到指定的模块。 Error loading \"C:\\Users\\admin\\AppData\\Local\\Temp\\_MEI467162\\torch\\lib\\torch_python.dll\" or one of its dependencies.\n\n\n\n把torch_utils.py转成torch_utils.pyc后即可完美运行。(但遇到在别的电脑上不能运行的问题，最后发现与cuda的库没有加进去有关系)\n\n\n\n## 打包\n\n[【Pyinstaller打包Pytorch框架】_Sleeep_的博客-CSDN博客_pytorch 打包](https://blog.csdn.net/qq_42811827/article/details/124035548?ops_request_misc=%7B%22request%5Fid%22%3A%22165811149016781435485512%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=165811149016781435485512&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~hot_rank-2-124035548-null-null.142^v32^pc_rank_34,185^v2^control&utm_term=pyinstaller打包pytorch&spm=1018.2226.3001.4187)\n\n[python项目（非单一.py文件）用Pyinstaller打包发布成exe，在windos上运行程序。_隔壁敲代码的王先生的博客-CSDN博客](https://blog.csdn.net/weixin_41870706/article/details/101780731)\n\n```\npyinstaller test_main_yolo.py -D -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\weights -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\runs -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\\models -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\\sub_main -p  D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\\utils -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\n```\n\npyinstaller test_main_yolo.py -D -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\weights -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\runs -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\\models -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\\sub_main -p  D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\\utils -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\n\n\n\n#### 问题及解决方案\n\nspec文件如下：\n\n```python\n# -*- mode: python ; coding: utf-8 -*-\n\n\nblock_cipher = None\n\n\na = Analysis(\n    ['test_main_yolo.py'],  #打包的主程序\n    pathex=['D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\weights',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\runs',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\models',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\sub_main',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\utils',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main',\n    'C:\\\\Windows\\\\System32', #这个貌似不用加\n    'C:\\\\Windows\\\\System32\\\\downlevel', #这个貌似不用加\n    'D:\\\\programs\\\\Anaconda\\\\envs\\\\pytorch\\\\Lib\\\\site-packages\\\\torch\\\\lib'], #pathex添加py文件的搜索路径，以及加一些库文件(dll)\n    binaries=[],  \n    datas=[('D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\weights\\\\best.pt','weights'),\n    ('D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\weights\\\\min_loss_7_15.pth','weights'),\n    ('D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\utils\\\\torch_utils.pyc','utils')],   #datas放非python文件或非dll文件,每个括号后面的路径的根是生成的dist目录\n    hiddenimports=[],\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    [],\n    exclude_binaries=True,\n    name='test_main_yolo',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    console=True,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\ncoll = COLLECT(\n    exe,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    name='test_main_yolo',\n)\n\n```\n\n首先安装pyinstaller，推荐使用`pip install https://github.com/pyinstaller/pyinstaller/archive/develop.zip` (据说这样能装最新的版本，错误更少)\n\n\n\n切到主程序所在的目录下，使用 `pyinstaller -D XXX.py`  会打包成文件夹的形式，这样方便调试。（推荐！）  而使用-F会生成单个exe，运行时exe会解压到c盘的临时目录运行，可能会有路径问题或者运行更慢？\n\n\n\n如果提示缺少什么文件，就去修改spec文件，修改后使用pyinstaller XXX.spec 再次打包。\n\n\n\n！！！！打包好后可能在自己的电脑上可以正常运行，但在别的电脑上会遇到torch调用的问题，（涉及CUDA），类似下面的情况：\n\nOSError: [WinError 126] 找不到指定的模块。 Error loading \"G:\\airplane_kps_pose_exe\\test_main_yolo\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n\n![image-20220719185432781](飞机姿态估计(杨业鹏-20220310203222)/image-20220719185432781.png)\n\n解决方式，通过[安装](https://github.com/lucasg/Dependencies)工具Dependens，通过这个工具，我们可以得到dll文件所需要的依赖项。如下：\n\n![image-20220719185746611](飞机姿态估计(杨业鹏-20220310203222)/image-20220719185746611.png)\n\n发现在其他电脑上没有上述依赖，以及很多cuda相关的依赖，于是直接将本机上CUDA\\v11.1\\bin目录下的所有dll文件复制到pyinstaller打包后生成的文件中torch的lib文件夹中。\n\n再次运行可能还会报错说找不到torch的lib下的其他dll或依赖，这时直接删除掉报错的文件，可能再运行还报错，那么再删除，我删了2-3个之后可以正常运行了。","categories":["位姿估计"]},{"title":"SLAM十四讲代码bug及解决","url":"/2021/11/24/计算机视觉/SLAM十四讲/","content":"\n### 第三章编译报错：\n\nerror: ‘decay_t’ is not a member of ‘std’; did you mean ‘decay’?\n\nhttps://blog.csdn.net/qq_15987811/article/details/122937722 修改编译的C++版本为c++14\n\n```\nset(CMAKE_CXX_FLAGS \"-std=c++14\")\n```\n\n\n\n### 第四章安装Sophus报错\n\nerror: implicitly-declared ‘Eigen::Map<Sophus::SO2<double> >::Map(const Eig\n\nhttps://blog.csdn.net/weixin_41698305/article/details/116547361\n\n\n\n### 第五章\n\n安装opencv\n\n直接通过apt-get一键安装https://blog.csdn.net/weixin_43909881/article/details/94013882\n\n```\nsudo apt-get update\nsudo apt-get install libcv-dev\nsudo apt-get install libopencv-dev\n```\n\n\n\n### 视觉SLAM十四讲（第二版）环境安装心得体会https://blog.csdn.net/weixin_43863574/article/details/107080443\n\n\n\n### 编译基本过程\n\n```\nmkdir build \n\ncd build\n\ncmake -D CMAKE_BUILD_TYPE=Release .. #此处的参数是为了带调试信息\n\nmake -j20  #10核cpu最高设置20\n\nsudo make install  #将程序安装至系统中。如果原始码编译无误，且执行结果正确，便可以把程序安装至系统预设的可执行文件存放路径。默认/usr/local/bin\n\n\n```\n\n### 第6章\n\n#### 安装 Ceres\n\n安装前需要安装以下依赖:\n\n```bash\nsudo apt install -y \\\n    liblapack-dev \\\n    libsuitesparse-dev \\\n    libcxsparse3 \\\n    libgflags-dev \\\n    libgoogle-glog-dev \\\n    libgtest-dev\n\n```\n\n之后即可正常安装：\n\n```bash\ngit clone https://github.com/ceres-solver/ceres-solver.git\ncd ceres-solver\nmkdir build && cd build\ncmake -DCMAKE_BUILD_TYPE=Release ..\nmake -j2\nsudo make install\n\n```\n\n#### 安装 G2O\n\n安装前需要安装以下依赖:\n\n```bash\nsudo apt install -y \\\n    qt5-qmake \\\n    qt5-default \\\n    libqglviewer-dev-qt5\\\n    libsuitesparse-dev \\\n    libcxsparse3 \\\n    libcholmod3\n\n```\n\n在安装G2O前，ubuntu20.04如果安装了anaconda可能会有qt5的版本冲突，我在g2o的github issue上找到了解决办法https://github.com/Shubodh/MR-project1-pgo/issues/5\n\n- Make sure to do `conda deactivate` before starting anything. Remove any environment, even `base`.\n\n- Type `export NO_CONDA_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin`\n\n- Now go to your g2o repository, make a folder `build` and do `cd build`\n\n- Instead of the `cmake ../` mentioned by [@Shubodh](https://github.com/Shubodh) sir, use `PATH=$NO_CONDA_PATH cmake ../`\n\n- Instead of the command `make` mentioned by [@Shubodh](https://github.com/Shubodh) sir, use `make -j8` to use threading for building. It is faster\n\n    \n\n安装 G2O：\n\n```bash\ngit clone https://github.com/RainerKuemmerle/g2o.git\ncd g2o\nmkdir build && cd build\ncmake -DCMAKE_BUILD_TYPE=Release ..\nmake -j2\nsudo make install\n```\n\n\n\n### 第7章\n\nSLAM十四讲，第七章程序ch7报错， error: ‘CV_LOAD_IMAGE_COLOR’ was not declared in this scope\n\nhttps://blog.csdn.net/CxC2333/article/details/107848500\n\n\n\nerror while loading shared libraries: libg2o_core.so: cannot open shared object file: No such file or directory\n\nhttps://blog.csdn.net/LittleEmperor/article/details/80840198\n\n![2022-02-23 18-38-54 的屏幕截图](/mnt/data/blog/AI/SLAM十四讲.assets/2022-02-23 18-38-54 的屏幕截图.png)\n\n\n\n### 射影变换\n\n[MVG读书笔记——三维空间中的射影几何(一）_炽霜的博客-CSDN博客](https://blog.csdn.net/frozenspring/article/details/77132296?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-77132296-blog-100829246.pc_relevant_paycolumn_v3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-77132296-blog-100829246.pc_relevant_paycolumn_v3&utm_relevant_index=4)****\n\n","tags":["算法"],"categories":["SLAM"]},{"title":"卫星姿态估计","url":"/2021/10/20/计算机视觉/位姿估计/卫星姿态估计/","content":"\n## Satellite Pose Estimation Challenge: Dataset,Competition Design and Results\n\n背景：为了解决地球轨道拥堵问题和延长地球静止轨道卫星的寿命，清除碎片和在轨服务等任务概念越来越受到学术界和工业界的关注。执行这些任务的一个关键是目标航天器相对于服务航天器的位置和姿态(即姿态)的可用性。相对于其他方法，使用基于单目视觉的方法对质量和功率的要求很小，且系统结构更为简单。\n\n难点：\n\n1. 用于航天器位姿估计的数据集是缺乏的。主要原因是很难获得数以千计的具有精确注释的姿态标签的目标航天器的星载图像。\n\t此外，由于缺乏通用数据集，无法系统地评估和比较不同位姿估计算法的性能。\n\n2. 目标距离和背景是主要的挑战\n\n\t\n\n与直接位姿估计方法相比，基于透视n点(PNP)解算器的位姿估计方法具有更高的精确度。\n\n\n\n相关工作：\n\n数据集：\n\n1. SPEED：第一个公开可用的用于航天器姿态估计的机器学习数据集，最初于2019年2月发布。包括合成和真实数据集两部分（真实数据集是用相机拍的1：1模型）\n\n<img src=\"卫星姿态估计/image-20211101091807417.png\" alt=\"image-20211101091807417\" style=\"zoom:50%;\" />\n\n\n\n2. URSO：采用虚幻4引擎进行仿真的合成卫星图像数据集\n\n3. SPEED+:\n\n\t![image-20211101111534454](卫星姿态估计/image-20211101111534454.png)\n\t\n4. BOP数据集？\n\n\n\n又出现了一个吊打其他所有的SPARK数据集\n\n![image-20220214152033019](卫星姿态估计/image-20220214152033019.png)\n\n\n\n\n\n比赛：Kelvins Pose Estimation Challenge(KPEC)   欧洲航天局和斯坦福大学合作组织的比赛\n\n[Kelvins - Pose Estimation Challenge - Home (esa.int)](https://kelvins.esa.int/satellite-pose-estimation-challenge/)\n\n[Kelvins - Pose Estimation 2021 - Home (esa.int)](https://kelvins.esa.int/pose-estimation-2021/home/)\n\n\n\n2021误差估计方法：\n\n![image-20211101113240159](卫星姿态估计/image-20211101113240159.png)\n\n凯尔文姿势估计比赛的入门工具包：\n\n[EuropeanSpaceAgency / speed-utils · GitLab](https://gitlab.com/EuropeanSpaceAgency/speed-utils)\n\n\n\n\n\n[janblumenkamp/esa-kelvin-pose-estimation (github.com)](https://github.com/janblumenkamp/esa-kelvin-pose-estimation)\n\n<img src=\"卫星姿态估计/image-20211101164942148.png\" alt=\"image-20211101164942148\" style=\"zoom:70%;\" />\n\n## Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering （2020 ICRA）\n\n此论文的最佳解决方案在ESA位姿挑战[5]的合成数据集上获得了**第三名**，在真实数据集上获得了第二名。（不依赖于pnp的最佳方案）\n\n该方案：基于resnet的架构直接回归3D位置，该团队使用基于高斯混合模型的软分类来估计角度\n\n前两名的方案：2D关键点回归；图像裁剪+缩放和鲁棒的PnP\n\n![image-20211022093553834](卫星姿态估计/image-20211022093553834.png)\n\n该网络采用带有预先训练的权重的ResNet架构作为网络主干（backbone），为了保持空间特征分辨率，去掉了原网络的最后一个全连接层和全局平均池化层，只在第二层留下了一个池化层。全局池化层被替换为了stride为2，的3*3卷积层（bottleneck layer），用来压缩CNN的特征。该网络的缺点是网络本身**不能处理多个对象**。\n\n**3D位置(Location)估计**是一个简单的含有两个全连接层的分支。\n\n不是最小化绝对欧几里得距离，而是最小化相对误差，即下式的第1项\n\n<img src=\"卫星姿态估计/image-20211104204920303.png\" alt=\"image-20211104204920303\" style=\"zoom:50%;\" />\n\n\n\n**旋转角度估计**包括两种不同的方法，硬分类和软分类，硬分类输出一个四元数；软分类分两部分：第一个部分输出一组四元数，第二部分输出对应每个四元数的概率\n\n**sim-to-real augmentation pipeline** 将5张带标签的真实数据集进行扩充，得到了很好的效果\n\n\n\n此论文方法的精度依赖于大量的参数（500M），且与前两名的得分还有较大差距\n\n此论文的实验揭示了几个网络超参数选择和不同估计旋转方向方法的影响。结果表明，基于软分类的方向估计方法比直接回归方法具有更好的估计效果。\n\n存在的问题：\n\n<img src=\"卫星姿态估计/image-20211105130442853.png\" alt=\"image-20211105130442853\" style=\"zoom:40%;\" />\n\n![image-20211111094705105](卫星姿态估计/image-20211111094705105.png)\n\n\n\n展望：一个很有前途的方向是使用递归神经网络和使用URSO生成的视频序列来解决跟踪问题。作为未来的工作，作者还计划将URSO扩展到SLAM，以定位几何形状未知的目标。\n\n其他：\n\n在benchmark for 6d object pose estimation这篇论文中提出**目前基于点对特征的方法表现最好，优于模板匹配法、基于学习的方法和基于3D局部特征的方法**   (20年基于学习的方法表现更好)\n\n不过通过避免直接估计姿势，而是使用CNN来回归预定义3D关键点的2D投影，并最终使用稳健的PNP解决方案(例如嵌入在RANSAC中)来估计姿势也可取得很好的效果\n\n \n\n\n\n\n\n## A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators\n\n\n\n\n\n\n\n## Vision-based attitude estimation for spacecraft docking operation through deep learning algorithm\n\n这篇感觉就在urso那篇论文上改了下网络和损失函数，没太多创新点，但是对urso论文中一些概念解释还不错\n\n而且实验的结果也不好，对于旋转的角度误差大的离谱\n\n<img src=\"卫星姿态估计/image-20211104204858051.png\" alt=\"image-20211104204858051\" style=\"zoom:60%;\" />\n\n\n\n\n\n## POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS\n\n这篇论文提出SPN网络以及发布了**SPEED**数据集\n\n**SPN**网络总体结构如下：\n\n![image-20211105162747501](卫星姿态估计/image-20211105162747501.png)\n\n下图解释了参考系、相对位置和相对姿态的定义：\n\n<img src=\"卫星姿态估计/image-20211105162830993.png\" alt=\"image-20211105162830993\" style=\"zoom:40%;\" />\n\n下图说明了SPN方法中所使用的卷积神经网络：\n\n![image-20211105163010801](卫星姿态估计/image-20211105163010801.png)\n\n该网络的branch1用到了《Faster R-CNN: Towards Real-Time Object Detection with **Region Proposal Networks** 》，输出目标的矩形框\n\n<img src=\"卫星姿态估计/image-20211105185913215.png\" alt=\"image-20211105185913215\" style=\"zoom:40%;\" />\n\nk是指每个特征点对应的先验框anchors的数量\n\n在每个滑动窗口位置，我们同时预测 k 个候选矩形框，那么 reg layer 有 4k 个输出用于表示 k 个矩形框的坐标位置及尺寸大小信息。 cls layer 输出 2k 个概率用于表示每个矩形框包含/不包含物体的概率信息\n\n[重温目标检测--Faster R-CNN - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1436729)\n\n[Region Proposal Networks 详解_qq314000558的专栏-CSDN博客](https://blog.csdn.net/qq314000558/article/details/82082911)\n\nbranch2和３使用了一种混合分类与回归的方法，来确定相对旋转角度。(也可称为基于软分类的**方向估计**)\n\n方向估计使用两个头分支：一个进行硬分类，给出一组预定义的四元数，以找到距离真值最近N个的四元数，然后第二个分支估计这N个四元数的权重，最终的方向由加权平均四元数给出。\n\nbranch2 执行分类任务。设置m（1000）个预定义旋转角度，输出每个角度是前n（3）个最接近真值的角度的概率。branch2的输出为向量v，其维度为m*1。\n\nbranch3使用branch2的输出作为输入，执行回归任务，输出向量w，其维度也为m*1。输出brach2每个结果的权重，但后面只取了前n个概率最大结果的权重（即最大的n个vj所对应的结果）使用，其余m-n个权重输出并未使用。然后将这n个权重通过softmax函数\n\n\n\n由brach1输出的二维边界框、brach2和3所得到的相对旋转角度、几何约束相结合，使用高斯-牛顿算法估计相对位置。（基于包围框检测的**位置估计**）\n\n\n\n\n\n## Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement（第一名）\n\n姿态估计分为三步：**目标检测、关键点回归、PnP求解**\n\n![image-20211108213040358](卫星姿态估计/image-20211108213040358.png)\n\n\n\n这篇文章的采用了HRNet\n\n![image-20211109112921791](卫星姿态估计/image-20211109112921791.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Segmentation-driven Satellite Pose Estimation（第二名）\n\n[ppt——EPFL_CVLAB](https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf)\n\n[GitHub - cvlab-epfl/segmentation-driven-pose: Segmentation-driven 6D Object Pose Estimation. CVPR 2019.](https://github.com/cvlab-epfl/segmentation-driven-pose)根据他们实验室的这篇论文做的\n\n\n\n这个实验室的另外一篇6D姿态估计论文[GitHub - cvlab-epfl/single-stage-pose: Single-Stage 6D Object Pose Estimation, CVPR 2020](https://github.com/cvlab-epfl/single-stage-pose)：\n\n之前很多工作，都是先检测出2D图片上的一些关键点，然后建立2D-3D的correspondings，最后通过基于 ***RANSAC*** 的 ***Pnp*** 算法，求出最后的6D姿态。这篇文章主要的创新点是把基于 ***RANSAC*** 的 ***Pnp*** 算法集成到了网络之中，形成了一个End-to-end的网络。\n\n\n\n\n\n\n\n\n\n## REAL-TIME, FLIGHT-READY, NON-COOPERATIVE SPACECRAFT POSE ESTIMATION USING MONOCULAR IMAGERY\n\n在保证精度的同时又十分轻量级\n\n![image-20211108211901970](卫星姿态估计/image-20211108211901970.png)\n\n\n\n姿态估计分为三步：**目标检测、关键点回归、PnP求解**\n\n目标检测将感兴趣区域裁剪后输出给关键点回归网络。关键点回归网络对航天器模型上预定的三维表面关键点的二维位置进行回归，然后利用PnP求得姿态。\n\n\n\n\n\n\n\n## Segmentation-driven 6D Object Pose Estimation（CVPR 2019 by cvlab）\n\n[cvlab-epfl/segmentation-driven-pose: Segmentation-driven 6D Object Pose Estimation. CVPR 2019. (github.com)](https://github.com/cvlab-epfl/segmentation-driven-pose)\n\n![image-20220120102831564](卫星姿态估计/image-20220120102831564.png)\n\n\n\n\n\n## Single-Stage 6D Object Pose Estimation（CVPR 2020 by cvlab）\n\n传统方法采用网络回归关键点，再加上ransac+pnp(但是这部分不属于神经网络的一部分)。\n\n本文将ransac+pnp融入到深度神经网络之中。\n\n\n\n\n\n## Wide-Depth-Range 6D Object Pose Estimation in Space(CVPR 2021 by cvlab)\n\n[cvlab-epfl/wide-depth-range-pose: Wide-Depth-Range 6D Object Pose Estimation in Space, CVPR 2021 (github.com)](https://github.com/cvlab-epfl/wide-depth-range-pose)\n\n[洛桑理工CVPR-21：太空中大深度范围6D物体位姿估计 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/440557881)\n\n[洛桑理工CVPR-21：太空中大深度范围6D物体位姿估计 (qq.com)](https://mp.weixin.qq.com/s/APbqBaxLoZfcrjOi69jQLA)\n\n### 0. 摘要\n\n空间6D姿态估计带来了独特的挑战，这在地面环境中并不常见。最显著的区别之一是**缺乏大气散射**，这使得物体可以从很远的距离看到，同时使**照明条件复杂化**。目前可用的基准数据集没有充分强调这一方面，并且大多描述了非常接近的目标。\n\n在处理**大深度范围变化**下的姿态估计之前的工作依赖于**两个阶段**的方法，首先估计尺度，然后在调整大小的图像块上进行姿态估计。相反，我们提出了一种**单级**分层端到端可训练网络，该网络对规模变化更具鲁棒性。我们证明，它不仅在合成图像以类似于在空间拍摄的图像上，而且在标准基准上都优于现有方法。\n\n### 1. 介绍\n\n<img src=\"卫星姿态估计/image-20211206214920301.png\" alt=\"image-20211206214920301\" style=\"zoom:40%;\" />\n\n目前面临的挑战：目标尺度和方向变化大（需要不同的图像特征）、空间材料镜面反射、图像某些部分的过度/不足曝光以及其他部分的细节缺失\n\n19年的卫星姿态估计比赛中最好的方法采用的是两阶段的方法：探测器发现一个围绕目标的轴对齐的盒子，该盒子被重新采样到一个统一的大小，最后由一个6D姿态估计器进行处理。但这个方法在几个方面仍有缺点，如下：\n\n1. 目标检测和姿态估计被视为两个单独的过程，这排除了联合的训练\n2. 其次，它只向正在使用的编解码器结构的最后一层提供监督信号，而不是向解码金字塔的所有级别提供监督信号，这将增加鲁棒性。\n3. 两个过程都执行了许多相似的特征提取计算，这导致了不必要的重复工作\n4. 这些方法依赖于基于深度学习的6D对象姿势估计的主要方法，包括训练网络以最小化预定义3D关键点的2D重投影误差，该误差无法处理较大的深度范围变化：如图2所示，重投影误差受单个关键点到相机的距离的强烈影响，不明确考虑这一点会降低性能\n\n<img src=\"卫星姿态估计/image-20211206221242701.png\" alt=\"image-20211206221242701\" style=\"zoom:40%;\" />\n\n为了解决这些缺点，我们引入了一个单一阶段的分层端到端可训练网络，如图3所示，该网络可产生鲁棒且不区分比例的6D姿势。\n\n这大多数仅从最后一层估计姿势的网络不同。为了跨尺度使用信息，该网络逐步缩小学习到的特征，为结果金字塔的每个层级导出3D到2D对应，最后使用基于RANSAC的PnP策略从这些对应集合推断出单个可靠姿势。\n\n<img src=\"卫星姿态估计/image-20211206221605058.png\" alt=\"image-20211206221605058\" style=\"zoom:50%;\" />\n\n\n\n为了跨尺度使用信息，它逐步缩小学习到的特征，为结果金字塔的每个层级导出3D到2D对应，最后使用基于RANSAC的PnP策略从这些对应集合推断出单个可靠姿势。这与大多数仅从最后一层估计姿势的网络不同。为了解决图2中的问题，我们**基于3D位置而不是2D投影**来最小化训练损失，从而使该方法对目标距离保持不变。我们使用特征金字塔网络（FPN）[24]作为主干，但与大多数依赖此类网络的方法不同，我们将每个训练实例分配到多个金字塔级别，以促进多尺度信息的联合使用。\n\n简言之，我们的贡献是一种新的6D姿势估计架构，它可以可靠地处理具有**挑战性条件下的大规模变化**。我们将证明，在已建立的**SPEED**数据集上，它**优于所有最先进的方法，同时速度也快得多**。此外，我们还介绍了一个**更大规模的卫星姿态估计数据集**，该数据集具有比SPEED更真实、更复杂的图像，并且我们表明，我们的方法在这个更具挑战性的场景中提供了相同的好处。最后，我们证明了我们的方法即使在**深度变化较小的图像**上（例如具有挑战性的遮挡LINEMOD数据集）也优于最新技术。\n\n### 2. 相关工作\n\n一般来说标准6D姿态估计流程为：首先建立三维到二维的对应关系，然后使用PnP解算器计算姿势\n\n之前的其他方法的设计都是为了在标准的计算机视觉基准上有效，其特点是尺度变化较小，当描绘对象的深度范围在不同的图像中发生显著变化时，它们的性能较差。部分尝试处理缩放问题的工作多采用两阶段的方法，使得目标检测和姿态估计分离开来，使得网络结构大大复杂化，引入大量冗余操作，进而导致实时性不高。\n\n我们的**主要贡献**是利用单个网络固有的层次结构，在各个层次上共享权重，以处理尺度变化问题。我们证明了这一点既健壮又高效。\n\n分层处理，如图像金字塔，是多尺度图像理解的经典理念。最近，这一想法已经通过特征金字塔网络（FPN）转化为深度学习领域，现在它是许多目标检测框架的标准组件，我们将此想法用于6D姿态估计。然而，与大多数将每个金字塔级别显式关联到单个预定义比例的对象检测方法不同，我们引入了一种**动态采样策略**，其中每个训练实例利用所有金字塔级别，尽管权重不同。这使我们能够在推理时融合来自不同层面的预测，从而得到更稳健的6D姿势估计。\n\n我们将实验重点放在星载物体的6D姿态估计上，因为在这种情况下，对缩放的鲁棒性非常重要，特别是在接近需要运动同步的非合作目标（如空间垃圾）时。空间工程界有自己关于6D姿态估计的文献。虽然它的发展方式类似于计算机视觉的进步，但它主要关注手工制作的方法，只有少数作品提出了基于深度学习的方法。其主要原因是缺乏大量的空间物体注释数据。\n\n**第二个贡献**是提出了一个基于物理渲染创建的**SwissCube数据集**。该数据集中的图像是使用基于物理的光谱光传输模拟创建的，该模拟涉及一个立方体卫星的精确参考3D模型，该模型考虑了太阳、地球、恒星等的影响。\n\n之前由ESA提出的SPEED数据集有以下的缺点：\n\n1. 没提供卫星的三维模型，重建模型有误差\n2. 通过基于非物理的渲染技术合成的，不能反映空间照明的复杂性\n3. 深度分布不均匀，只有很少的图像描述了距离相机很远的卫星。\n\n<img src=\"卫星姿态估计/image-20211207095921055.png\" alt=\"image-20211207095921055\" style=\"zoom:50%;\" />\n\n### 3. 方法\n\n使用特征金字塔在多尺度上回归预定义3D点的2D投影\n\n#### 3.1 网络金字塔结构（Pyramid Network Architecture）\n\n大多数6D姿势估计深度网络依赖于编码器-解码器体系结构。因此，为了处理6D对象姿态估计的大范围变化，我们**使用编码器网络固有的层次结构，而不是依赖于额外的对象检测网络**，它提取不同尺度的特征。具体而言，我们在框架中使用Darknet-53[34]作为主干，并采用与FPN[24]中设计的用于目标检测的网络架构相同的网络架构，该网络架构由k=5级特征图组成，{F1、F2、F3、F4、F5}，**每个特征图具有越来越大的感受野**。\n\n我们从金字塔的**每一层回归对象3D关键点的2D位置，而不是仅从特征图F5计算单个姿势估计**。为此，我们依赖于[11]中的分段驱动方法，在每个特征地图的每个空间位置生成特征向量，以输出3D关键点的2D投影（表示为相对于相应单元中心的偏移量）和每个对象类的对象性分数。因此，每个单元的特征向量是一个C×（2×8+1）维向量，由8个2D关键点的偏移位置（见下图）和1个objectness(物体存在于该cell的概率)。要对分割掩码进行编码，所有特征单元都需要参与对象性预测，包括不包含目标对象的单元。相反，如下所述，只有选定的单元参与姿势回归器的训练。\n\n<img src=\"卫星姿态估计/image-20211208101923121.png\" alt=\"image-20211208101923121\" style=\"zoom:50%;\" />\n\n[(29条消息) 理解物体检测中的Objectness_AI公园-CSDN博客](https://blog.csdn.net/u011984148/article/details/112504800)\n\n#### 3.2 集合感知采样（Ensemble-Aware Sampling）\n\n[特征向量（Feature Vectors） - 简书 (jianshu.com)](https://www.jianshu.com/p/86f39172b68a)\n\n<img src=\"卫星姿态估计/image-20211209163724502.png\" alt=\"image-20211209163724502\" style=\"zoom:40%;\" />\n\n处理尺度变化过大问题的通用方法为：分治策略——将训练集分为不重复的组，之后在训练过程中将不同的pyramid层分配到不同尺度。此策略在目标检测中是够用的，因为可以在不同的层输出最优结果。但是在6D估计中却不能够联合多层信息提升鲁棒性，单个层的输出会引入较高的噪声。\n\n具体采样过程如下，需要根据某一个训练实例的尺寸大小动态确定在每一层的采样数量。λ控制着每层的激活cell数量，λ=0时在所有层取相同的cell数量；λ>20过大时退化为FPN结构中的“hard assignment”\n\n![img](卫星姿态估计/v2-1fad96cf5150c0d3a42a8580cdead3c9_720w.jpg)\n\n#### 3.3 三维空间中的损失函数 （Loss Function in 3D Space）\n\n在采样过程中选择的特征向量用于回归3D包围盒的8个角的2D投影。在回归2D关键点位置时，大多数方法采用的损失函数为：\n\n<img src=\"卫星姿态估计/image-20211207194451711.png\" alt=\"image-20211207194451711\" style=\"zoom:50%;\" />\n\n<img src=\"卫星姿态估计/image-20211207194548714.png\" alt=\"image-20211207194548714\" style=\"zoom:50%;\" />\n\n但正如图2所示，这种损失函数是次优的，特别是在存在大深度变化的情况下，因为(a)它更强调某些关键点而不是其他关键点(不同关键点误差对损失函数的影响不同)，(b)并且还取决于物体的相对位置（物体不同位置的误差影响也不同）。\n\n为了解决以上问题，本文在3D空间引入损失函数（其实就是把预测的2D关键点根据位姿还原到3D，然后再和真实值比较）\n\n[Focal loss论文详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/49981234) \n\n\n\n#### 3.4 多尺度融合推理 （Inference via Multi-Scale Fusion）\n\nEnsemble-awaresampling（智能采样策略）使得每个层都能够生成有效pose信息，因此可以在所有层中使用阈值=0.3过滤objectness score。所有结果可以统一使用RANSAC+PnP方法融合，也可以使用基于学习的方法融合（Hu-Single stage 6D pose）。\n\n本文为简单使用RANSAC+PnP方法。首先估计物体尺寸S，选取所有层中objectness score最高的特征向量，通过8个box角计算S。明确尺寸后，从每个层中选取Nk个高分特征cell，以此来构造如下2D-3D对应关系。最后通过RANSAC+PnP方法完成姿势估计。\n\n**推理总结**：依然使用智能采样策略，通过最高分确定尺寸S，之后正常使用采样策略从各个层中选取特征数量，全部用于姿势估计。\n\n\n\n### 4.实验\n\n首先在SPEED数据集上测试，采用的评估指标为比赛的指标。\n\n然后介绍SwissCube数据集，采用ADI-0.1d指标，以评估不同深度范围的性能。并在真实图像进行了测试(请注意，这些真实的图像不是在太空中捕获的，而是在实验室环境中使用目标的模型和OptiTrack运动捕获系统来获取一些图像的地面真实姿势信息。)。\n\n最后为了证明该方法的通用性，在描述小深度变化的Occluded-LINEMOD标准数据集上对其进行了评估。\n\n\n\n#### 4.1 Evaluation on the SPEED Dataset\n\n<img src=\"卫星姿态估计/image-20211208103411496.png\" alt=\"image-20211208103411496\" style=\"zoom:50%;\" />\n\n#### 4.2 Evaluation on the SwissCube Dataset\n\n<img src=\"卫星姿态估计/image-20211208104306785.png\" alt=\"image-20211208104306785\" style=\"zoom:50%;\" />\n\nSwissCube Dataset 删除了进入地球阴影的部分，因为这部分的图像基本纯黑。一共有500个场景，每个场景100帧图像。\n\n相机距离在1d-10d之间，d为SwissCube 卫星的直径（不含天线），分为近、中、远三类，分别对应距离[1d-4d], [4d-7d],  [7d-10d]\n\n![image-20211208105825232](卫星姿态估计/image-20211208105825232.png)\n\n<img src=\"卫星姿态估计/image-20211208105838151.png\" alt=\"image-20211208105838151\" style=\"zoom:50%;\" />\n\n<img src=\"卫星姿态估计/image-20211208110509594.png\" alt=\"image-20211208110509594\" style=\"zoom:50%;\" />\n\n<img src=\"卫星姿态估计/image-20211208110521554.png\" alt=\"image-20211208110521554\" style=\"zoom:50%;\" />\n\n<img src=\"卫星姿态估计/image-20211208140720710.png\" alt=\"image-20211208140720710\" style=\"zoom:50%;\" />\n\n<img src=\"卫星姿态估计/image-20211208141002214.png\" alt=\"image-20211208141002214\" style=\"zoom:50%;\" />\n\n\n\n#### 4.3. Results on Real Images\n\n\n\n#### 4.4. Evaluation on Occluded-LINEMOD\n\n\n\n\n\n\n\n### 代码实测结果\n\npython test.py --config_file ./configs/swisscube.yaml --num_workers 0 --weight_file './swisscube_pretrained.pth' --running_device 'cuda'\n\npython train.py --config_file ./configs/swisscube.yaml --num_workers 0 --weight_file './swisscube_pretrained.pth' --running_device 'cuda'\n\n![image-20211210103613085](卫星姿态估计/image-20211210103613085.png)\n\n![image-20211210103622008](卫星姿态估计/image-20211210103622008.png)\n\n作者提供的模型精度远高于论文中的精度\n\nbackbone的输出：\n\n<img src=\"卫星姿态估计/image-20211214103709819.png\" alt=\"image-20211214103709819\" style=\"zoom:50%;\" />\n\nfpn的输出：\n\n<img src=\"卫星姿态估计/image-20211214103620809.png\" alt=\"image-20211214103620809\" style=\"zoom:60%;\" />\n\n\n\nPoseHead的输出：\n\n\n\n\n\n测试时总model的输出pred：由score, cls_id, R, T组成 \n\n![image-20211215103709168](卫星姿态估计/image-20211215103709168.png)\n\n\n\nspeed数据集模型重建：\n\n![image-20220105160244996](卫星姿态估计/image-20220105160244996.png)\n\n![image-20220105160316940](卫星姿态估计/image-20220105160316940.png)\n\n![image-20220105165415536](卫星姿态估计/image-20220105165415536.png)\n\n![image-20220105165427442](卫星姿态估计/image-20220105165427442.png)\n\n\n\n## 申请书要求\n\n![image-20211109163033078](卫星姿态估计/image-20211109163033078.png)\n\n![image-20211109163041316](卫星姿态估计/image-20211109163041316.png)\n\n\n\n## 相关调研\n\n### **马鸿英**学长：\n\n天线姿态估计\n\n<img src=\"卫星姿态估计/image-20211116143216549.png\" alt=\"image-20211116143216549\" style=\"zoom:33%;\" />\n\n目标：只用测量出偏角θ\n\n方法：最小化重投影误差\n\n<img src=\"卫星姿态估计/image-20211116143457174.png\" alt=\"image-20211116143457174\" style=\"zoom: 33%;\" />\n\n这个方法在《POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS》这篇卫星姿态估计论文中使用。\n\n<img src=\"卫星姿态估计/image-20211116143633683.png\" alt=\"image-20211116143633683\" style=\"zoom: 33%;\" />\n\n### 张澜涛学长：\n\n他们目前还在做锥套的检测，飞机的姿态估计还没怎么开始做，目前打算用某个关键点检测的方法，两相机各看到一些固定的关键点，三角测量这些关键点得到关键点坐标，算点集空间变换。\n\n经讨论觉得卫星姿态估计和飞机的姿态估计几乎差不多，后面我们可以一起做，只不过他们的项目对于检测速度的要求特别高，而我目前看到的一篇轻量化的卫星姿态估计的网络最快也只能达到6.6Hz（不过这个得看在什么设备上跑）。\n\n\n\n\n\n## 2021官方入门工具包\n\n[EuropeanSpaceAgency / speedplus-utils · GitLab](https://gitlab.com/EuropeanSpaceAgency/speedplus-utils)\n\n![image-20211117102805335](卫星姿态估计/image-20211117102805335.png)\n\n","tags":["Satellite","Pose Estimation"],"categories":["位姿估计"]},{"title":"位姿估计基础及论文","url":"/2021/10/20/计算机视觉/位姿估计/姿态估计基础(杨业鹏-20220310203222)/","content":"\n\n\n## 3D旋转的表示\n\n1. 旋转矩阵\n\n2. 欧拉角\n\n\t使用动态欧拉角会出现万向锁现象；静态欧拉角不存在万向锁的问题。通时还要注意不同的旋转顺序所得的结果是不同的。\n\n\t[3d旋转欧拉角与万向锁！Cocos Creator 3D!_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Bt4y1v7R1?from=search&seid=2218520216726804689&spm_id_from=333.337.0.0)\n\n3. 四元数\n\n\t[四元数的可视化_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1SW411y7W1/?spm_id_from=autoNext)\n\n\t[Visualizing quaternions | 3blue1brown + Ben Eater](https://eater.net/quaternions/video/intro)交互式体验四元数\n\n\t![image-20211104112131946](姿态估计基础/image-20211104112131946.png)![image-20211104112421258](姿态估计基础/image-20211104112421258.png)![image-20211104112437943](姿态估计基础/image-20211104112437943.png)\n\n\t四元数就相当于后三个数定义了一个单位向量作为旋转轴，第一个数代表旋转的角度\n\n\t设旋转角为θ(从轴的正向箭头处看下去，θ为正则向逆时针旋转，为负则向顺时针旋转)，**q = cos(θ/2) + sin(θ/2)(xi + yj + zk) = q0 + q1i + q2j +q3k**\n\n[旋转矩阵、欧拉角、四元数理论及其转换关系_aibotlab的博客-CSDN博客_欧拉角转四元数](https://blog.csdn.net/lql0716/article/details/72597719)\n\n\n\n## PnP\n\n待\n\n\n\n## 软分类和硬分类\n\n软分类：使用的是概率模型，输出不同类对应的概率，最后的分类结果取概率最大的类，如多SVM组合分类；\n\n硬分类：使用的是非概率模型，分类结果就是决策函数的决策结果；\n\n\n\n\n\n## BOP Challenge 2020 on 6D Object Localization\n\nIn the BOP Challenge 2019, methods using the **depth image channel**, whichwere mostly based on the **point pair features (PPF's)** [10], clearly outperformed methods relying **only on the RGB channels**, all of which were based on deep neural networks (DNN's).  （19年及以前PPF方法优于深度学习方法）\n\n20年的比赛中有五个DNN方法超过了19年的冠军(采用PPF)，第三名的方法只使用了RGB通道，完全没用深度通道。\n\n![image-20211123085515345](姿态估计基础/image-20211123085515345.png)\n\nBOP数据集说明：\n\n[BOP数据集格式说明_屠龙之术-CSDN博客_bop文件](https://blog.csdn.net/u014712806/article/details/112339410)\n\nPBR全称(Physicallly-Based Rendering)\n\n## CosyPose: Consistent multi-view multi-object 6D pose estimation\n\n这个代码没跑出来[ylabbe/cosypose: Code for \"CosyPose: Consistent multi-view multi-object 6D pose estimation\", ECCV 2020. (github.com)](https://github.com/ylabbe/cosypose)\n\n\n\n## HybridPose: 6D Object Pose Estimation under Hybrid Representations（**CVPR 2020**）\n\n[6D位姿估计 HybridPose: 6D Object Pose Estimation under Hybrid Representations_不忘初心~-CSDN博客](https://blog.csdn.net/john_bh/article/details/103998704)\n\n[chensong1995/HybridPose: HybridPose: 6D Object Pose Estimation under Hybrid Representation (CVPR 2020) (github.com)](https://github.com/chensong1995/HybridPose)这个star多，正在跑\n\n\n\n复现遇到的bug：\n\n[(25条消息) win10安装visual studio C++ build tools 提示安装包丢失或毁坏_与君共勉-CSDN博客_buildtools_msbuild.msi](https://blog.csdn.net/qq_15158911/article/details/107887490)\n\n![image-20211123112129454](姿态估计基础/image-20211123112129454.png)\n\n\n\n\n\n## DPOD: 6D Pose Object Detector and Refiner\n\n![Screenshot from 2021-12-05 20-02-59](姿态估计基础.assets/Screenshot from 2021-12-05 20-02-59.png)\n\n这个bug解决不了，放弃了\n\n## CDPN: Coordinates-based Disentangled Pose Network for Real-time RGB-based 6-DoF Object Pose Estimation\n\nhttps://github.com/LZGMatrix/CDPN_ICCV2019_ZhigangLi\n\n正在尝试\n\n\n\n\n\n\n\n## Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation（ICCV2019）\n\n由于遮挡和对称性等问题，仅使用**RGB图像**估计物体的6D姿态仍然具有挑战性。如果没有专家知识或专业的扫描设备，也**很难构建具有精确纹理的三维模型**。为了解决这些问题，我们提出了一种新的位姿估计方法Pix2Pose，它可以在没有纹理模型的情况下预测每个目标像素的三维坐标。设计了一种自动编码器结构来估计三维坐标和每个像素的期望误差。然后将这些像素级预测用于多个阶段，形成2D-3D对应关系，用RANSAC迭代的PnP算法直接计算姿态。我们的方法通过利用最近在生成性对抗训练中的成果来精确地恢复被遮挡的部分，从而对遮挡具有鲁棒性。此外，提出了一种新的损耗函数变压器损耗，通过将预测引导到最接近的对称姿态来处理对称目标，对包含对称和遮挡目标的三个不同基准数据集的计算表明，我们的方法优于仅使用RGB图像的最新方法。\n\n<img src=\"/mnt/data/blog/AI/姿态估计基础/image-20220308091537677.png\" alt=\"image-20220308091537677\" style=\"zoom:50%;\" />\n\ntensorflow  &  训练步骤复杂https://github.com/kirumang/Pix2Pose\n\n\n\n## Real-Time Seamless Single Shot 6D Object Pose Prediction（CVPR2018）\n\n论文链接：https://arxiv.org/abs/1711.08848\n代码链接：https://github.com/Microsoft/singleshotpose\n\n**主要思想**：我们提出了一种单阶段方法来同时检测RGB图像中的一个物体并预测其6D姿态，不需要多个阶段或检查多个假设。不像最近提出的一些单阶段技术，它只预测一个近似6D的姿势，然后必须细化，我们是足够精确的，不需要额外的后处理。它的速度非常快，在Titan X（帕斯卡）GPU上每秒50帧，因此更适合实时处理。我们的方法的关键部分是一个新的CNN架构，直接预测对象的3D边界框的投影顶点的2D图像位置，然后用PnP算法估计物体的6D姿态。我们的单目标和多目标姿态估计方法在LINEMOD和OCCLUSION数据集上明显优于其他最近基于CNN的方法。\n\n主要贡献： 论文的主要贡献是一个新的网络架构，即一个快速和准确的单阶段6D姿势预测网络，不需要任何后处理。它以无缝和自然的方式扩展了用于二维检测的单阶段CNN结构去执行6D检测任务。实现基于YOLO，但该方法适用于其他单阶段检测器，如SSD及其变体。\n\n\n\n代码环境太老了\n\n新版代码  https://github.com/a2824256/singleshotpose_imp   （据说自制模型位姿估计异常） \n\nSingle Shot 6D Object Pose Prediction代码复现--测试https://codeantenna.com/a/nuYk97dWkZ\n\n## SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again\n\n主要思想：提出了一种新的基于RGB数据的三维模型实例检测和6D姿态估计方法。为此，我们扩展了流行的SSD范式，以覆盖完整的6D姿势空间，并仅对合成模型数据进行训练。我们的方法可以与当前最先进的方法在多个具有挑战性的RGBD数据集上竞争或超越。此外，我们的方法在10Hz左右，要比相关的其它方法快很多倍。\n\n主要贡献：\n\n（1） 一个仅利用合成三维模型信息的训练阶段\n（2） 模型位姿空间的分解，便于对称性的训练和处理\n（3） SSD的一种扩展，产生2D检测并推断出正确的6D姿势\n\n论文链接：https://arxiv.org/abs/1711.10006v1\n代码链接：https://github.com/wadimkehl/ssd-6d    没有提供训练代码\n\n\n\n## BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models\n\nrgbd输入\n\n**特别之处**：\n\n- 不需要被跟踪物体的3D模型\n- 稳定，不受明显遮挡的影响\n- 10Hz\n- 在NOCS上效果非常好，在YCBInEOAT上效果与se(3)-TrackNet类似（比se差一点）\n\n \n\n## GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation（CVPR 2021）（基于几何信息指导的单目6D物体姿态直接回归算法）\n\n\n\n\n\n## EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach\n\n[【6D Pose/论文阅读】EfficientPose_遗梦少年的博客-CSDN博客](https://blog.csdn.net/qq_41707788/article/details/120832323)\n\nhttps://blog.csdn.net/rush9838465/article/details/112475562\n\n代码： https://github.com/ybkscht/EfficientPose\n\n论文： https://click.endnote.com/viewer?doi=10.48550%2Farxiv.2011.04307&token=WzM0NTc1MTMsIjEwLjQ4NTUwL2FyeGl2LjIwMTEuMDQzMDciXQ.TMWGnTMXlvPuSUwFgmZJHcon4Ek\n\n测试失败\n\n\n\n## DOPE  在ros上跑的\n\n[yehengchen/DOPE-ROS-D435: Object 6DoF Pose Estimation for Assembly Robots Trained on Synthetic Data - ROS Kinetic/Melodic Using Intel® RealSense D435 (github.com)](https://github.com/yehengchen/DOPE-ROS-D435)\n\n[NVlabs/Deep_Object_Pose: Deep Object Pose Estimation (DOPE) – ROS inference (CoRL 2018) (github.com)](https://github.com/NVlabs/Deep_Object_Pose)\n\n\n\n## PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation\n\nhttps://github.com/zju3dv/pvnet\n\nhttps://github.com/zju3dv/clean-pvnet\n\n作者还开源了他们用[blender](https://so.csdn.net/so/search?q=blender&spm=1001.2101.3001.7020)合成数据代码：https://github.com/zju3dv/pvnet-rendering\n\n其他博客：\n\nPVNET代码复现讲解https://www.freesion.com/article/85511243426/\n\npvnet——总结https://blog.csdn.net/Marilynviolet/article/details/100747094\n\n![image-20220308170517455](/mnt/data/blog/AI/姿态估计基础/image-20220308170517455.png)\n\n```\nROOT=/home/young/code/clean-pvnet\ncd $ROOT/lib/csrc\nexport CUDA_HOME=\"/usr/local/cuda-11.5\"\ncd ransac_voting\npython setup.py build_ext --inplace\ncd ../nn\npython setup.py build_ext --inplace\ncd ../fps\npython setup.py build_ext --inplace\n\n# If you want to run PVNet with a detector\ncd ../dcn_v2\npython setup.py build_ext --inplace\n\n# If you want to use the uncertainty-driven PnP\ncd ../uncertainty_pnp\nsudo apt-get install libgoogle-glog-dev\nsudo apt-get install libsuitesparse-dev\nsudo apt-get install libatlas-base-dev\npython setup.py build_ext --inplace\n\n\nROOT=/path/to/clean-pvnet\ncd $ROOT/data\nln -s /media/young/young/dataset/LINEMOD/LINEMOD linemod\nln -s /path/to/linemod_orig linemod_orig\nln -s /path/to/occlusion_linemod occlusion_linemod\n\n# the following is used for tless\nln -s /path/to/tless tless\nln -s /path/to/cache cache\nln -s /path/to/SUN2012pascalformat sun\n```\n\n安装中遇到cuda版本问题\n\n```\nconda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch\n```\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/young/code/pvnet/lib/utils/extend_utils/lib\n\n```\nln -s /media/young/young/dataset/LINEMOD/LINEMOD $ROOT/data/LINEMOD\n```\n\n\n\n\n\n本文解决了在严重遮挡或截断下从单个 RGB 图像进行 6DoF 姿态估计的挑战。 最近的许多工作表明，一种两阶段方法，首先检测关键点，然后解决用于姿势估计的 Perspective-n-Point (PnP) 问题，取得了显着的性能。 然而，这些方法中的大多数仅通过回归图像坐标或热图来定位一组稀疏关键点，这对遮挡和截断很敏感。 相反，我们引入了逐像素投票网络 (PVNet) 来回归指向关键点的逐像素单位向量，并使用这些向量使用 RANSAC 对关键点位置进行投票。 这为定位被遮挡或截断的关键点创建了灵活的表示。 这种表示的另一个重要特征是它提供了关键点位置的不确定性，PnP 求解器可以进一步利用这些不确定性。 实验表明，所提出的方法在 LINEMOD、Occlusion LINEMOD 和 YCBVideo 数据集上大大优于现有技术，同时对实时姿态估计也很有效。 我们进一步创建了一个截断 LINEMOD 数据集，以验证我们的方法对截断的鲁棒性。 该代码将在 https://zju-3dv.github.io/pvnet/ 上提供。\n\n\n\n传统方法依赖手工特征，对于图像变化和背景干扰不够鲁棒（why？）\n\n传统网络关键点回归+pnp，有遮挡问题\n\n我们提出了一种使用**像素级投票网络** (PVNet) 进行 6D 姿势估计的新框架。 基本思想如图 1 所示。PVNet 不是直接回归关键点的图像坐标，而是预测表示从对象的每个像素到关键点的方向的单位向量。 然后这些方向根据 RANSAC 对关键点位置进行投票。——关键点的**向量场**表示方法\n\n该思想的动机在于，刚体的特性——即我们看到其一部分就能推断出其他部分的相对方向。\n\n不确定性驱动的pnp\n\n\n\n\n\n## 学习路线\n\n这篇博客的评论区贼强\n\n[ 刚体6D位姿估计方法综述_dsoftware的博客-CSDN博客_6d位姿估计](https://blog.csdn.net/dsoftware/article/details/97955570?spm=1001.2101.3001.6650.9&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-9.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-9.no_search_link)\n\n\n\n<img src=\"姿态估计基础/image-20211125112415235.png\" alt=\"image-20211125112415235\" style=\"zoom:50%;\" />\n\n纹理特征和几何细节丰富的用对应点的方法、纹理特征和几何细节弱的用基于模板的方法、有遮挡的或者是类别级对象的用基于投票的方法\n\n当有遮挡时，基于模板的方法效果最差\n\n\n\n\n\n了解物体6Dpose的意义，多种表示李代数欧拉角四元数等，用PnP求解6Dpose的原理等；其次，阅读综述论文，看业界为了求解6D姿态用了哪些方法，传统的以及深度学习的，可以只看摘要和前沿；再次，挑选几篇代表性、前沿、开源的代码，实际复现测试，分析优缺点；最后，结合自身项目，确定输入数据是纯RGB，还是RGB-D，还是Lidar点云，有没有对应的3D模型，对于速度或者精度要求多高，是不是只针对实例级别物体等，选择最接近的某个算法，在其上面改进\n\n有很多基于RGB图像的6D位姿估计方法：\n\nPVNet: Pixel-wise Voting Network for 6DoF Pose Estimation；\n\nPix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation；\n\nImplicit 3D Orientation Learning for 6D Object Detection from RGB Images；\n\nEPOS: Estimating 6D Pose of Objects with Symmetries；\n\nDPOD: 6D Pose Object Detector and Refiner；\n\nCDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation；\n\nSegmentation-driven 6D Object Pose Estimation等等\n\n\n\n[物体6-Dof pose estimation主流方法汇总_3D视觉工坊-CSDN博客](https://blog.csdn.net/yong_qi2015/article/details/117004423?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.pc_relevant_default&spm=1001.2101.3001.4242.2&utm_relevant_index=2)\n\nUncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image\n\n## 经典特征点检测算法\n\n[sift、surf、orb 特征提取及最优特征点匹配 - 闽A2436 - 博客园 (cnblogs.com)](https://www.cnblogs.com/multhree/p/11296945.html)\n\n如果对计算实时性要求非常高，可选用ORB算法，但基本要保证正对拍摄；如果对实行性要求稍高，可以选择SURF；基本不用SIFT。\n\n[6.SIFT(尺度不变特征变换)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Qb411W7cK?p=4)\n\n[Surf算法特征点检测与匹配_牧野的博客-CSDN博客_surf算法](https://blog.csdn.net/dcrmg/article/details/52601010)\n\n[最大稳定极值区域（MSER）检测_zizi7的专栏-CSDN博客_mser算法](https://blog.csdn.net/zizi7/article/details/50379973/)\n\n[图像特征算法(三)——ORB算法简述及Python中ORB特征匹配实践 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/261966288)\n\n\n\n使用传统的方法进行单目相机位姿估计：[GitHub - nanfeng-dada/pose_estimation: 单目位姿估计，传统机器视觉，opencv pnp算法](https://github.com/nanfeng-dada/pose_estimation)\n\n\n\n[[SLAM入门之视觉里程计(5)：单应矩阵 - Brook_icv - 博客园 (cnblogs.com)](https://www.cnblogs.com/wangguchangqing/p/8287585.html)](https://zhuanlan.zhihu.com/p/74597564)\n\n![image-20220217110036174](姿态估计基础/image-20220217110036174.png)\n\n文博分享：\n\n[Awesome-Image-Registration-Organization · GitHub](https://github.com/Awesome-Image-Registration-Organization)\n\n医学上的应用https://github.com/fabio86d/HipHop_2D3Dregistration\n\n\n\n[高翔博士slam课程深度图像数据除以5000的含义_dbdxnuliba的博客-CSDN博客_高翔博士slam](https://blog.csdn.net/dbdxnuliba/article/details/108215073)\n\n![img](姿态估计基础/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RiZHhudWxpYmE=,size_16,color_FFFFFF,t_70.png)\n\n### [位姿的刚体变换与坐标系转换 – 数值之刃 MathSword](http://www.mathsword.com/rigidtrans_coordinatetrans/)\n\n\n\n### 经典的单目RGB视觉跟踪算法，没有深度学习各种网络\n\n输入一个包含目标物体的序列帧（offline的视频序列或相机输入视频流），然后还需要一个目标物体的三维模型。\n\n[啥是Region-based实时单目RGB三维(刚性)物体跟踪（一） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/102024694?utm_source=wechat_timeline)\n\n\n\n\n\n### Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach\n\n现有问题：\n\nlarge number of acquisitions from multiple views are often required in order to obtain a complete 3D model, which is not preferred in some real-time applications; the depth sensors in general cannot work outdoor and have a limited sensing range;\n\n## MediaPipe\n\nhttps://google.github.io/mediapipe/solutions/objectron#model_name\n\nhttps://www.youtube.com/watch?v=f-Ibri14KMY&t=231s\n\n![image-20220310162325247](姿态估计基础/image-20220310162325247.png)\n\n\n\n\n\n## 大组会——刚体6D位姿估计\n\n[基于深度学习的特征匹配与位姿估计+端-云协同的AR技术与平台_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1fa411c7hc?spm_id_from=333.337.search-card.all.click)\n\n[Xiaowei Zhou's Homepage (xzhou.me)](https://www.xzhou.me/)\n\n\n\n[基于深度学习的特征点提取，特征点检测的方法总结_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1C34y1S7ik/)\n\n\n\n[【官方字幕】“3D几何与视觉技术”全球在线研讨会_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1bA411E7Y9?p=6)\n\n\n\n### 定义\n\n[(21条消息) 物体6D位姿的含义_Guoguang Du的博客-CSDN博客_物体6d位姿](https://blog.csdn.net/dsoftware/article/details/106101681)\n\n\n\n\n\n## ORB-SLAM\n\nBA优化存在实时性问题\n\n要实时BA的话，需要提供以下条件：\n\n1. 关键帧的关键点匹配\n2. 关键帧的数量要尽可能少，避免冗余\n3. 关键帧要具有显著的视差和大量的回环匹配\n4. 关键帧位姿的初始化以及关键点位置的初始化\n5. A local map in exploration where optimization is focused to achieve scalability.\n6. The ability to perform fast global optimizations (e.g., pose graph) to close loops in real time\n\n","tags":["Pose Estimation"],"categories":["位姿估计"]},{"title":"ubuntu","url":"/2021/09/23/软件使用/ubuntu(杨业鹏-20220829094535)/","content":"\n深度学习环境搭建\n\nhttps://blog.csdn.net/m0_37412775/article/details/109355044?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-5.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-5.no_search_link\n\n\n\n自动挂载磁盘\n\nhttps://blog.csdn.net/qq_35451572/article/details/79541106\n\n上述链接中的有问题\n\nsudo mount /dev/sda2  /home/young/disk/\n\n\n\n修改默认启动项为windows（仅一次有效）\n\nsudo grub-reboot 2\n\nUbuntu中apt与apt-get命令的区别(基本用apt就行) https://blog.csdn.net/taotongning/article/details/82320472\n\n截图工具flameshot安装https://www.zhihu.com/question/55398701\n\n录屏https://zhuanlan.zhihu.com/p/459998747  vokoscreen\n\n**定时自动重启**:\n\n- https://www.cnblogs.com/WangYangkai/p/16353876.html\n\n- 这个更有效：https://blog.csdn.net/JulyLi2019/article/details/122368367\n\n<img src=\"ubuntu(杨业鹏-20220829094535)/image-20220830194210619.png\" alt=\"image-20220830194210619\" style=\"zoom:40%;\" />\n\n\n\n\n\nswitchyomega 让浏览器不开代理也能在手动代理设置下上网\n\nhttps://www.phpvar.com/archives/4567.html\n\n\n\n\n\n\n\n## 服务器\n\n**挂载程序**：`nohup **command** > out.txt 2>&1 &`\n\n*2>&1：将错误信息重定向到标准输出。*\n\n*使用 & 在程序结尾来让程序自动运行*\n\n*为了能够后台运行，那么我们就可以使用nohup这个命令。*\n\n**查看运行结果**：`tail -f out.txt `\n\n\n\n查看运行的python程序`ps -ef|grep python`\n\n![image-20220712205442574](/mnt/data/blog/软件使用/ubuntu(杨业鹏-20220829094535)/image-20220712205442574.png)\n\n关闭程序`kill -9 2732481`\n\n 关闭我的所有python进程    ps -ef|grep yyp|grep python|grep -v grep|awk '{print $2}'|xargs kill -9\n\n查看文件大小：ls -lh\n\n查看文件夹大小：du -h --max-depth=1\n\n\n\n## clash下载\n\nhttps://www.cnblogs.com/Jiang13537/p/15571504.html\n\n\n\n\n\n## wget\n\n-c 自动重试 \n\n-t 0 无限重试\n\n\n\n## ln -s 软链接\n\n后面跟**绝对路径**\n\n\n\n## 查看GPU占用\n\nwatch -n 0 nvidia-smi\n\nwatch -n 0 gpustat -cpu   (pip install gpustat)\n\n\n\n## 使用shell并行跑程序\n\nhttps://artisan.blog.csdn.net/article/details/87691890?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-87691890-blog-104047419.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-87691890-blog-104047419.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=5\n\n#### 使用&和wait改造（在命令后面加 & 即可，记得最后要加wait）\n\n- 在每个进程中使用&符号进行让脚本在后台运行，无需等待当前进程结束。\n- 为了确保每个进程都执行完成，最后务必使用wait关键字，用来确保每一个子进程都执行完成。","tags":["ubuntu"]},{"title":"【FPGA】图像处理","url":"/2021/07/23/嵌入式和硬件/FPGA/【FPGA】图像处理/","content":"\n## 图像的中值滤波\n\n统计排序滤波器是非线性滤波器的一种，这里我们学习较为常见的**中值滤波**，并以掩膜大小为3*3进行举例，来说明使用FPGA设计二维中值滤波器的流程。\n\n### 一、行缓存\n\n在FPGA数字图像处理中，行缓存的使用非常频繁，例如我们需要图像矩阵操作的时候就需要进行缓存，例如图像的均值滤波，中值滤波，高斯滤波以及sobel边缘查找等都需要行缓存设计。\n\n![image-20210723151332772](【FPGA】图像处理/image-20210723151332772.png)<img src=\"【FPGA】图像处理/image-20210723151214106.png\" alt=\"image-20210723151214106\" style=\"zoom: 80%;\" />\n\n图像数据以数据流的方式，先从左到右，然后从上到下将每一个像素数据输出。若要获取如上图中的3*3矩阵，则必须对行数据进行缓存，如下图所示，采用两个FIFO即可解决行缓存的问题，下图即可同时获取到第一个掩膜所覆盖的9个数据。基本思路是，第一行数据依次输入进来写入fifo1，当写到第一行最后一个数据时，开始从fifo1依次读出数据然后写入fifo2，依次类推。通过行缓存的方式使前三行的输出对齐。\n\n![line_buffer.png](【FPGA】图像处理/line_buffer.png)\n\n### 二、并行全比较排序\n\n排序是一种重要的数据运算，传统的排序方法主要靠软件串行方式实现，包括冒泡法、选择法、计数法等，这些算法大多采用循环比较，运算费时，实时性差。不能满足工程上越来越高的实时性要求。并行全比较实时排序算法是基于序列中任意两个数并行比较实现。由于全部数字同时进行比较处理，将会占用大量的处理空间，因此此算法也可称为“**以空间换时间**”排序算法。\n\n假设有一数组{20，80，40，40，60，70}，定为A0=20、A1=80、 A2=40、A3=40、A4=60、A5=70，要求对该数组按从大到小的顺序排列。排序按以下过程进行 ： \n\n**(1) 两两比较，并累积分数。**考虑到可能出现相同的数，规定**下标越大的数优先级越高**，即：\n\n```c\n若m>n,则选用  （Am>=An）? 1：0 ;\n若m<n，则选用  (Am>An)? 1:0 ;\n```\n\n![image-20210723155419045](【FPGA】图像处理/image-20210723155419045.png)\n\n**(2) 按积分排序。**由上图可知，积分越大，表明对应的数越大。\n\n\n\n#### 3*3数据的中值\n\n若直接对3*3掩膜所覆盖的9个数据进行并行全比较排序，资源消耗较大，这里可以分别对每一行的3个数据进行并行全比较排序，获得每一行的中值，再将这3个中值再进行一次排序，取出中值。（这样取出来的不一定是真正的中值，但比较近似，算法复杂度也不高）\n\n\n\n\n\n## TOF的工作原理和数据处理流程\n\n### 一、什么是TOF相机\n\n双目相机是一种被动接收自然光的传感器，通过接收自然光利用三角测距的方式描述三维环境。本文的TOF相机则是一种主动发射红外光的传感器，通过发射光脉冲并接收打到物体反射回来的脉冲信号，最后计算光脉冲的飞行时间得到目标物体的距离。\n\n与激光雷达不同，TOF相机不是以逐点扫描的方式感知环境信息，而是以面阵的方式得到深度图。\n\n### 二、TOF相机的技术原理\n\nTOF相机有两种成像方法，分别为脉冲法和连续波调制法，下面分别介绍其技术原理和优缺点。\n\n1. **脉冲法**：通过记录脉冲波从发出到返回的时间来计算相机到物体的距离，即**D=0.5×c×Δt**（其中c表示光在空气中传播的速度，Δt表示脉冲信号从相机到目标往返的时间）\n\n\t<img src=\"【FPGA】图像处理/image-20210727103401146.png\" alt=\"image-20210727103401146\" style=\"zoom:150%;\" />\n\n\t通过一个**高频率**的时钟驱动计数器对收发脉冲之间的时间进行计数，使得计数时钟的周期必须远小于发送脉冲和接收脉冲之间的时间才能够保证足够的精度。但是如果要达到毫米级别的测量，对控制时钟，发射单元等电子元器件的精度都是一项挑战。这就是为什么无人驾驶中应用的激光雷达传感器常基于脉冲法，因为该方法比较适合**中远距离**的测量。\n\n\t**脉冲法**测量方式简单，占空比窄检测距离远；但是它易受环境光和元器件精度影响，测量精度相对较低。\n\n2. **连续波调制法**：使用调制光照射场景, 并测量通过场景中的物体反射后返回光的相位延迟。得到相位延迟后, 再使用正交采样技术测量间接得到距离，即**D=0.5×c×φ×T/2π**（其中c表示光在空气中传播的速度，T表示调制周期，ϕ表示发射和接收波形的相位差）\n\n\t![image-20210727103341053](【FPGA】图像处理/image-20210727103341053.png)\n\n\t该方法比较适合**中短距离**的测量，精度往往可以达到毫米级，在机器人等应用中使用的TOF相机大多采用的是连续波调制的方法。**相位差**可以消除测量器件带来的固定偏差但是采样次数多，导致测量耗时帧率低。\n\n### 三、如何将相位偏移转换为距离？\n\n两种方法的距离求解公式都是D=c×t。那么t是如何得到的呢？脉冲法比较直接，就不多叙述。\n\n连续波调制法通过测量相位差来计算时间。采用**四次曝光**原理来测量相位差，即传感器上的每个像素以相等的间隔对场景反射的光进行四次采样。\n\n在连续波调制过程中通常将连续波近似为正弦波划分成4个窗口进行采样，并且采样时间间隔相同。设定四次曝光的采样相位分别为：0°，90°，180°，270°。如下图所示：\n\n<img src=\"【FPGA】图像处理/image-20210727141830496.png\" alt=\"image-20210727141830496\" style=\"zoom: 67%;\" />\n\n![image-20210727144035880](【FPGA】图像处理/image-20210727144035880.png)\n\n物体与传感器的距离和相位差可定义如下：\n$$\nDmax = c/2f  \\tag{1}\n$$\n\n$$\nφ=acrtan[(Q_3-Q_1)/(Q_0-Q_2)] \\tag{2}\n$$\n\n$$\nD=cφ/4πf \\tag{3}\n$$\n\n其中Dmax是最大测量距离，D是目标距离，f是ToF发射波的频率，c是光速，Q0-Q3分别为0°-270°的采样值。\n\n公式2的具体推导过程见参考文献[1]。\n\n\n\n### 参考文献\n\n[[1]王胤. 应用于三维成像飞行时间法建模及其误差分析[D].湘潭大学,2017.](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&filename=1017269863.nh&v=MjQ4NDIyNkdiRytGOW5LckpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3VmWU9Sb0Z5bmdWcnpNVkY=&uid=WEEvREcwSlJHSldSdmVqM1BLVW9SbUxZeG1WK0ZTTVUyWWFNVTVibm1CUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)\n\n[[2]一文详解深度相机之TOF成像,新机器视觉](https://mp.weixin.qq.com/s/5dqn_vrW86plQRPEGY12Dw)\n\n[[3]Y. Fang, X. Wang, Y. Su, K. Zhang and B. Su, \"The Accuracy Analysis of TOF Camera Based on ANOVA,\" 2018 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW), 2018, pp. 1-2, doi: 10.1109/ICCE-China.2018.8448475.](https://ieeexplore.ieee.org/document/8448475)\n\n","tags":["图像处理"],"categories":["FPGA"]},{"title":"verilog学习笔记","url":"/2021/07/16/嵌入式和硬件/FPGA/verilog学习笔记/","content":"\n\n\n## Modelsim使用技巧\n\n\n\n### 基本操作流程\n\n\n\n#### 图形界面操作\n\n1. 新建工程（建议给每个工程单独建文件夹）\n2. 调试的时候直接在library中的work目录下对testbench文件右键simulate\n\n\n\n\n\n#### 命令行操作\n\n①创建一个工程和工程库;\n②加载设计文件（包括你编写好的testbench）；\n③编译源文件；\n④运行仿真，并查看结果；\n⑤最后进行工程调试。\n上面为利用modelsim的仿真步骤，利用do文件可以完成以上1—4步。\n\n例程\n（1） sim_run.bat文件用于批处理\n@set modelsim=C:\\modeltech64_10.7\\win64\\vsim.exe\n@if exist vsim.wlf del vsim.wlf\n%modelsim% -do sim_top.do\n\n（“％环境变量名％”用法的含义是取指定环境变量的值）\n首先设置modelsim的执行路径，可以是vsim.exe也可以是modelsim.exe\n然后删除历史运行的vsim.wlf文件，以便后面进行更新\n最后用DOS命令调用modelsim工具，并在modelsim工具中执行sim._top.do文件\n(2)sim_top.do文件\nvlib work\n//建立work库\n\nvmap work work \n//将目前逻辑库work和实际工作库work相映射对应  \nvlog -f vlog-rtl.list \n#将vlog-rtl.list文件中所有文件编译\n\nvopt +acc=npr tb_top -o tb_top_opt\n#仿真时优化tb_top输出tb_top_opt\nvsim tb_top_opt\n#仿真tb_top_opt\n\n(也可以直接 vsim -novopt work.tb_top)\n\ndo wave.do\n#运行wave.do，添加wave的信息\nrun 20ms\n#运行20ms （run -all：运行全过程）\n（3）vlog-rtl.list\n../rtl/*.v\n#上级文件夹rtl的所有v文件\n../../calc_engine/*.v\n#上上级文件夹calc_engine文件夹的所有v文件\n../sim/tb/*.v \n上级sim/tb文件夹内的所有v文件\n（4）wave.do\n通过添加并设置波形参数，保存为wave.do文件\n\n更多详情可参考 http://blog.sina.com.cn/s/blog_7e2e98ad0101gxx7.html\n\n\n\n log -r /* 对所有的信号记录，运行这个命令后即使在仿真前没有把信号加入wave窗口，仿真完成后直接加入wave窗口就可以查看波形，比较方便，但是缺点是当工程较大和仿真时间很长时仿真速度较慢，占用内存也较大。\n vsim  -voptargs=\"+acc\"\n+acc意思是设计中所有的信号都提供入口，可以观察，而log命令的意思就是在运行仿真的时候信号不用加入wave窗口在开始仿真后直接可以看波形，意思就是他记录了所有的信号波形\n观察其它窗口的结果，用view *命令显示 。view *命令可以观察包括signals、wave、dataflow等窗口文件，也可以分别打开。例如用view signals来观察信号变量。\n\n\n\n\n\n## verilog语法\n\n- always块中被赋值的变量只能为reg\n\n- 不可在不同的always块中对同一个reg赋值\n\n- assign 语句对已经定义的wire赋值，wire也可在被定义的时候赋值，wire变量只能被赋值一次\n\n- 组合逻辑用阻塞赋值，非组合逻辑用非阻塞赋值。——**组合逻辑**电路在逻辑功能上的特点是任意时刻的输出仅仅取决于该时刻的输入，与电路原来的状态无关。**时序逻辑**电路在逻辑功能上的特点是任意时刻的输出不仅取决于当时的输入信号，而且还取决于电路原来的状态，或者说，还与以前的输入有关\n\n\t```verilog\n\t组合逻辑：always @(*)  //组合always块相当于assign语句，因此组合电路存在两种表达方法\n\t时序逻辑：always @(posedge clk)\n\t```\n\n- 在**时序逻辑**中，不完整的 if…else… 结构并**不会生成锁存器**，而组合逻辑中不完整的 if…else… 结构就会生成锁存器\n\n- `timescale 1 ns / 1 ps 该指令用于定义时延、仿真的单位和精度\n\n- `default_nettype wand 用于默认设置为线网类型 \n\n- `default_nettype none 会关闭隐式声明功能，有助于查bug\n\n- [verilog设计过程寄存器使用#1的问题_烟花一时的博客-CSDN博客](https://blog.csdn.net/qq397381823/article/details/87521885)\n\n- {1'b0,1'b0,1'b0,1'b0,1'b0,1'b0,1'b0,1'b0}可以简单写成{8{1'b0}}\n\n- input、inout 类型不能声明为 reg 数据类型，因为 reg 类型是用于保存数值的，而输入端口只能反映与其相连的外部信号的变化，不能保存这些信号的值。\n\n\toutput 可以声明为 wire 或 reg 数据类型。\n\n- ###### 端口连接规则:\n\n\t**输入端口**\n\n\t模块例化时，从模块外部来讲， input 端口可以连接 wire 或 reg 型变量。这与模块声明是不同的，从**模块内部**来讲，***input 端口必须是 wire 型变量***。\n\n\t**输出端口**\n\n\t模块例化时，**从模块外部来讲，output 端口必须连接 wire 型变量**。这与模块声明是不同的，从**模块内部**来讲，output 端口可以是 wire 或 reg 型变量。\n\n\t**输入输出端口**\n\n\t模块例化时，从模块外部来讲，inout 端口必须连接 wire 型变量。这与模块声明是相同的。\n\n\t**悬空端口**\n\n\t模块例化时，如果某些信号不需要与外部信号进行连接交互，我们可以将其悬空，即端口例化处保留空白即可，上述例子中有提及。\n\n\toutput 端口正常悬空时，我们甚至可以在例化时将其删除。\n\n\tinput 端口正常悬空时，悬空信号的逻辑功能表现为高阻状态（逻辑值为 z）。但是，例化时一般不能将悬空的 input 端口删除，否则编译会报错。\n\n- ```verilog\n\treg [7:0] mem [255:0];//定义在向量名之前的是向量的位宽，定义在向量名之后的维度可以理解为向量数组的长度\n\t```\n\t\n- ```verilog\n\talways @(*) begin\n\t    casez (in[3:0])\n\t        4'bzzz1: out = 0;   // in[3:1]输入什么都可以\n\t        4'bzz1z: out = 1;\n\t        4'bz1zz: out = 2;\n\t        4'b1zzz: out = 3;\n\t        default: out = 0;\n\t    endcase\n\tend\n\t//case项是按顺序检查的(实际上，它更像是生成一个巨大的真值表然后生成超大的门)。注意有输入(例如，4'b1111)匹配多个case项。选择第一个匹配(因此4'b1111匹配第一个case项，out = 0)。\n\t```\n\t\n- ```verilog\n\tfor(i=0;i<3;i=i+1) begin \n\t  \t……  //循环中的 i 最高可到 2\n\tend\n\t\n\tfor(初始化表达式1; 布尔表达式2; 步进表达式4){\n\t         循环体3\n\t}//执行顺序：1234-->234-->234-->不满足为止。\n\t```\n\t\n\t\n\t\n- 端口声明规范，例子如下\n\n\t```verilog\n\tmodule ax_pwm\n\t#(\n\t\tparameter N = 32 //pwm bit width \n\t)\n\t(\n\t    input         clk,\n\t    input         rst,\n\t    input[N - 1:0]period,\n\t    input[N - 1:0]duty,\n\t    output        pwm_out \n\t    );\n\t //在声明时统一使用wire，避免模块连接的时候出现问题\n\treg[N - 1:0] period_r;\n\treg[N - 1:0] duty_r;\n\treg[N - 1:0] period_cnt;\n\treg pwm_r;//若要对输出的值进行修改，则再定义一个reg变量，使用assign语句对wire赋值\n\tassign pwm_out = pwm_r;\n\t```\n\n\t\n\n仿真时的文件操作\n\n- ```verilog\n\t//下面进行文件读操作\n\tinteger file;\n\tinitial begin   \n\t    file = $fopen(\"../data/tof.txt\",\"r\");//只读方式打开文件\n\t    if (!file)\n\t        $display(\"Could not open file!\");\n\t    else begin\n\t        $display(\"Open file success!\");\n\t    end\n\tend\n\t-------------------------------------------\n\talways @(posedge clk or negedge rstn) begin\n\t    if(~rstn)begin\n\t        ……\n\t    end\n\t    else begin\n\t        $fscanf(file,\"%d\",data_i);//将file文件按行读取至data_i,每个时钟读取一次\n\t    end\n\tend\n\t-------------------------------------------\n\t//下面进行文件写操作\n\twire [DW-1:0] data_o;\n\tinteger file_out;\n\t\n\tinitial begin\n\t    file_out = $fopen(\"data_out.txt\", \"w\");\n\t    if (!file)\n\t        $display(\"Could not open file_out!\");\n\t    else begin\n\t        $display(\"Open file_out success!\");\n\t    end\n\tend\n\talways @(posedge clk) begin\n\t    if (……) begin\n\t        $fwrite(file_out, \"%d\\n\", data_o);//将data_o逐行写入\n\t    end    \n\t    else if (……) begin\n\t        $fclose(file_out);//关闭文件\n\t        $finish;//关闭仿真\n\t    end\n\tend\n\t```\n\n### 三段式状态机\n\n![这里写图片描述](verilog学习笔记/70.jpeg)\n\n```verilog\n采用三段式的写法：\n1.状态触发\n2.状态转移\n3.结果输出\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n//第一个always块，时序逻辑，描述现态转移到次态\nalways @ (posedge clk or negedge rst) begin\n    if(rst_n) \n        current_state<=S0;\n    else\n        current_state<=next_state;\nend\n\n//第二个always块，组合逻辑，描述状态转移的条件\nalways @ (current_state) begin  //或者always@(*)\n    next_state = x; //初始化，使得系统复位后能进入正确的状态（可省）\n    case(current_state)\n    S0：begin\n        if(condition1)        next_state = S1;//状态转移\n        else if (condition2)  next_state = S2;//状态转移\n        else next_state <= S0;\n        end\n    S1：begin\n        if(condition3)       next_state = S3;//状态转移\n        else if (condition4) next_state =S4; \n        …\n        end\n    ……\n    default: begin\n            if(condition0)   next_state = S0;//状态转移\n           end\n    endcase\nend\n//第三个always块，时序逻辑，描述输出。这部分也可采用assign输出\nalways @ (posedge clk or negedge rst) begin\nif(rst)\n    out0；\nelse\n    case(current_state)\n    S0: out0;\n    S1: out1;\n    ….\n    default:Out0;\n    endcase\nend\n//////////////////////////////////////////////////////////////////////////////\n```\n\n### generate语句\n\n```verilog\n//四位全加器的实现\nmodule top_module (\n    input [3:0] x,\n    input [3:0] y, \n    output [4:0] sum);\n\n    wire [3:0] cout;\n    assign sum[4]=cout[3];\n    \n    fa fa0(\n        .a(x[0]),\n        .b(y[0]),\n        .cin(1'b0),\n        .cout(cout[0]),\n        .sum(sum[0])\n    );\n    \n    genvar i;\n    generate \n        for(i=1;i<=3;i=i+1)\n        begin:adds\n       \t\tfa fa1(\n            .a(x[i]),\n            .b(y[i]),\n            .cin(cout[i-1]),\n            .cout(cout[i]),\n            .sum(sum[i])\n    \t\t);\n        end\n    endgenerate\nendmodule\n\nmodule fa (\n\tinput a,b,cin,\n    output cout,sum\n);\n    assign {cout,sum} = a+b+cin;    \nendmodule\n    \n```\n\n\n\n## 数电知识\n\n[逻辑代数最大项和最小项的概念表达式的转换及关系_阿锋不知道丶的博客-CSDN博客](https://blog.csdn.net/weixin_43342105/article/details/104515921)\n\n[卡诺图化简法_hahasusu的专栏-CSDN博客_卡诺图](https://blog.csdn.net/hahasusu/article/details/88244155)   \n\n[卡诺图圈零的时候 表达式怎么写圈1表达式是与或式 圈零是或与式？_百度知道 (baidu.com)](https://zhidao.baidu.com/question/427622855038635732.html)\n\n","tags":["verilog","modelsim"],"categories":["FPGA"]},{"title":"日常随笔","url":"/2021/07/04/随笔/日常随笔/","content":"\n#### 2021/7/4\n\n好久没有静下来写点什么了，这几天放假回家，一直都在和朋友们玩儿，散步，吃饭，聊天，看鬼片，拍照，打麻将，游泳，遛狗，弹琴，唱歌。好像挺充实的，但心中却又总觉得空落落的，为什么呢，可能是心中有好多话压抑着没有言说吧。此刻，当我独自一人，戴着耳机，听着舒缓的音乐，敲击着键盘，写下心中所想，才觉得轻松了许多。\n\n的确像许多人说的那样，我好像总是把自己的事情埋在心里。可能是语言表达能力不够吧，也可能是没有到那种情绪吧。和朋友们一起玩的时候，常常他们玩的很嗨的时候，我内心却毫无波澜。他们或笑，或跳，或讲故事，或聊综艺，而我只是静静地听着，好像他们越兴奋，我离他们越远。不知道这算不算所谓的社恐，不过对于有些特别好的朋友不会出现这样的情况。\n\n总有人问我，你不打游戏，也不看综艺，也不追剧，你每天都在干什么。是啊，我每天在干什么呢。我想说，我打游戏，而且打的也不少，只是越长大越觉得，缺少了小时候那种对游戏不知名的热爱，现在在我看来打游戏只是一种社交；我也会看剧，只是看的没有许多狂热看剧爱好者看的多，我喜欢好剧，喜欢那种能让人回想好久的剧，而不是紧跟最新更新的剧。若你认为除了这些，就没有其他的事情可做了，或者说就没有其他的娱乐可做了，那你挺惨的。对于我来说，我有着更多喜欢做的事情。我喜欢和朋友一起运动，我喜欢一个人在房间的时候唱歌弹琴，我喜欢在清晨背上包，到图书馆安安静静地看一本书，我喜欢在夕阳落下之前去健身房健身，我喜欢在黄昏之时出去遛狗，我喜欢在月亮高悬之时散步聊天。\n\n"},{"title":"21年一战上海交通大学电子系819初试及复试逆袭经验贴","url":"/2021/04/01/随笔/一战上海交通大学电子系819初试及复试逆袭经验贴/","content":"\n### 前言\n\n我的成绩排名并不太好，写此篇单纯是想记录一下自己的经历，仅供参考。\n\n参考书什么的，我会简单一提，主要讲下对于819专业课的理解。我更多的想写一写我在考研的过程中的一些感悟和小建议，在我看来这比单纯的列写推荐书目以及进度安排更为重要。\n\n首先，我希望学弟学妹们在考研之前认真的想一想以下几个问题：\n\n1. 我为什么要考研？\n2. 为什么不选择工作或出国呢？\n3. 我是一个能沉得住心的人吗？或者说，考研这枯燥的几个月甚至是一年，我能坚持下来吗？\n4. 你能接受考研失败吗？\n\n其次，我会说说我自己对这几个问题的理解，以及我的一些小感悟和小建议。\n\n最后，我会讲述我的**复试经历**。（复试面试时一共5位老师，其中有**4位**老师在面试完后都给我打电话邀请我加入团队）\n\n<!--more-->\n\n### 个人背景介绍（21届考研）\n\n本科：武汉某985，加权排名年级前27%，参加过两年电赛培训。\n\n初试成绩：总分370– （21年分数线355），出乎意料的数学爆炸（只有一百零几分），所以可以说初试考的很糟糕，且初试成绩排名处于录取人数（120人）的边缘。\n\n复试：总分160+（满分200分），复试成绩使我排名上涨40+。\n\n------\n\n### 参考书推荐\n\n**专业课：**专业课我考的还不错，我的推荐书目可能有点不一样。信号与系统（ss)  当然还是推荐上交本科的教材——胡光锐老师编写的《信号与系统》（作为课本）以及配套的习题《信号与系统习题精解与考研指导》（俗称白皮书）。不过数字信号处理（dsp）我其实不太推荐奥本海默的书，反而我强推上交本科的教材——刘兴钊老师的《数字信号处理》 （作为课本）。\n\n奥本海默的书实在是写的太细了，其实对于最近几年的命题趋势来说，没必要这么细。我一开始也是看的奥本的书，啃的很难受，而且上面很多不会考的内容。同时奥本书后的的习题，我刷的时候感觉就是一直抄答案（个人认为时间较紧的话刷一遍重点题就行了，不过记得做好笔记）。直到我有一天翻开了**刘兴钊**老师的书，里面的内容都是活生生的考点啊！而且内容简洁明了，我觉得更适合中国学生，看着本书的效率完爆看奥本的书。而且我做了真题后发现，真题中很多题目都是直接来自于这本书的例题中，同时课后的习题质量也很高，做一遍选填题后对概念的理解会更深入。而且最近几年dsp考试的题型都更偏向于概念，选填题增多，刘的书更符合这个趋势。\n\n总结，其实819并不是特别难，说难其实是因为奥本的书以及习题特别难啃，但是在我看来，可以以刘的书为主力，奥本的书为辅助。如果不追求130以上的分数，看刘的书绝对是更好的选择，特别是对于时间紧张的同学。\n\n**数学：**不建议大家一直纠结于买谁的书好，多刷题才是王道，而且不要以为以前很少考的今年就不考，复习一定要全面！（我就是吃了这个亏）。具体推荐书目大家看看别的帖子吧，别太纠结。\n\n**英语、政治：**相信我，如果时间紧的话，这两门没必要花太多时间，最后的分数差不了多少。在保证数学专业课能拿高分的情况下再多看看英语，政治最后突击一下分都差不多。（当然，这两门你想打80+的话还是得多下功夫的，但是实际情况是你可能比别人多看很久，最后却只比别人高几分或者甚至还比别人低。大家一般都在70分左右）\n\nps：我是有颈椎问题的，初试效率可能比其他人低很多。不过没经历过颈椎病的人估计也无法感同身受，就不多说了，总之，**注意身体**！！！！\n\n------\n\n### 问题解答以及一些建议\n\n先说说我对下面这几个问题的理解吧。\n\n1. **我为什么要考研？**\n\n\t是因为大家都考研吗？还是因为父母的期待？还是因为自己对于职场的恐惧，想给自己一点缓冲的时间？还是因为自己对学术很感兴趣，想进一步深造？还是觉得自己本科过的太混了或者觉得本科学校不够好？还是觉得考研后工资更高？我相信每个人对这个问题都有不同的答案，不管是什么样的回答，希望这个答案能足以坚持你走完这条并不平坦的路。\n\n2. **为什么不选择工作或出国呢？**\n\n\t其实很多人在考研途中又去选择了工作，或者边找工作边考研。如果能更早的明确以后的方向，岂不是更好？\n\n\t如果你的专业是互联网行业相关（别的我不了解），认真准备一年或者半年是绝对可以找到一份不错的工作的，而且研究生的工资并不会高特别多，如果你中途退出或者因为没有认真准备而没过初试，转而去找工作，为什么不一开始就选择找工作呢？\n\n\t如果家庭条件允许，出国其实也是一个很好的选择，同等努力下，出国可以进入更好的学校。（我个人对国外不向往，热爱祖国，当然家庭条件也不允许哈哈~）\n\n3. **我是一个能沉得住心的人吗？或者说，考研这枯燥的几个月甚至是一年，我能坚持下来吗？**\n\n\t当你备考时，会陆陆续续听到周围许多同学都找到了工作，有的甚至工资很高，你会继续坚持吗？\n\n\t当你备考时，找到工作的和保研了的人都在和小伙伴愉快的玩耍，你会继续坚持吗？\n\n\t当你备考时，找工作和出国的同学都拿到了offer，而你的未来还很迷茫，你会继续坚持吗？\n\n\t当你备考时，遇到了一位心爱的ta，你会继续坚持吗？\n\n\t……\n\n4. **你能接受考研失败吗？**\n\n\t我曾经以为我能接受，但我现在觉得我的确接受不了。若你运气不太好（从另一方面也可以说运气比较好了），初试差很多分，考完就知道自己绝对考不上，那么你马上开始准备找工作（1月份开始），春招的时候（3月份最集中）你还有很大的可能找到一份不错的工作。若你运气很差，初试过了复试又被刷了，这时可能已经到了4月份中旬左右，春招已经基本接近尾声，那么，你的机会还有多少？对于我这个不想二战的人来说，我的确很难接受。那么你怎么想？\n\t\n\t\n\n\n**我的一些小建议：**\n\n1. 尽早确定学校，最好不要中途换\n\n2. 找几个一起考研的小伙伴，互相监督，互相鼓励，更容易坚持下去哦~\n\n3. 学的好不如选的好，选择学校和专业也是一场博弈，非常重要！\n\n4. 知乎很好，但不要贪多哦！每个人的说法都各有不同，还是得跟着自己的节奏来\n\n5. 考研其实也没那么难，别被一些文章和公众号营造的焦虑困扰。\n\n6. 如果觉得自己没啥奖项和项目经历，尽量选择本校或者复试占比小的学校（上交初试500分，复试200分。复试中面试占80分，笔试占100分，英语占20分）\n\n7. 千万不要以为初试完了就完事大吉了，复试同样重要！特别是简历要精心打磨，上面的每一个点都要能回答。\n\n8. **身体第一！**千万要注意身体啊！！！少久坐！多运动！一定不要因为考研而不坚持运动了啊！不要熬夜！\n\n9. 初试更重要的是**坚持**，复试则与你本科前几年所学的知识和项目关系更大，不过不同学校不太一样，有的甚至复试的面试分数基本没啥差距，这也是一个很重要的因素。\n\n------\n\n### 复试经历\n\n3月初可能是我最迷茫最黑暗的时候，那时初试分数刚出来，群里做了个成绩调查，统计结果是400分以上的比以往任何一年都多，大家都预测今年分数线会上涨很多（关于这个，几乎每年都会有人在群里喊分数线必涨，很搞人心态）。我估摸着自己可能处于分数线边缘，非常难受。一边复习一边看调剂信息，同时还想着是不是得准备找工作了，还有毕业设计的老师也在催进度。四重折磨让我压力山大，爸妈和我的很多朋友来鼓励我，很感谢他们。最后我还是决定赌一把，相信光~\n\n我算是很幸运，比复试线高出了十几分。这让我信心大增，本科的项目经历也为我在复试中增色不少。最终复试逆袭，**排名上涨了40多名**。同时在复试面试结束后，有**4位**面试老师都给我打电话邀请我加入。想看具体的**复试经历**请见我的另一篇博客——《考研感悟》[考研感悟 | young's blog (youngyyp.github.io)](https://youngyyp.github.io/2021/03/02/随笔/考研感悟/)\n\n","tags":["考研"],"categories":["随笔"]},{"title":"【电赛】线路负载及故障检测装置","url":"/2021/03/13/嵌入式和硬件/【电赛】线路负载及故障检测装置/","content":"\n这是2019年的国赛题，赛题链接为 https://youngyyp.github.io/download/2019_c.pdf\n\n我们小组的报告下载链接为 https://youngyyp.github.io/download/2019report.pdf\n\n<!--more-->\n\n题目要求如下图所示：\n\n![image-20210314203332601](【电赛】线路负载及故障检测装置/image-20210314203332601.png)\n\n![image-20210314204048793](【电赛】线路负载及故障检测装置/image-20210314204048793.png)\n\n\n\n\n\n这个题目我们就差负载的网络结构没有做出来，不过这部分的分值较大，最后只得了省二等奖。还差一名就是省一，差两名就是国奖。（希望明天上交的分数线不要这样搞我心态，球球了>.<）\n\n最终的作品实物图\n\n![img](【电赛】线路负载及故障检测装置/98A42894CD1A83DF99E066FF81B52447.jpg)\n\n\n\n比赛期间的伙食\n\n![img](【电赛】线路负载及故障检测装置/073EEBE3ADC8768CD77F6E7458FEE2EF.jpg)\n\n\n\n调bug\n\n![img](【电赛】线路负载及故障检测装置/72E10092A5795009CC075D06B096D162.jpg)\n\n\n\n截止时间到\n\n![image-20210314215346801](【电赛】线路负载及故障检测装置/image-20210314215346801.png)\n\n\n\n装箱评测\n\n![img](【电赛】线路负载及故障检测装置/72A042A874BB3868CB583A44600D94D2.jpg)","tags":["线路故障检测","自平衡电桥","正交解调"],"categories":["电赛"]},{"title":"【毕业设计】基于Atlas_200_DK的障碍物识别系统设计与实现","url":"/2021/03/12/嵌入式和硬件/【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/","content":"### 开题答辩——2021/3/12\n\n毕业设计的题目是《基于**Atlas 200 DK**的障碍物识别系统设计与实现》，今天进行了开题答辩，感觉还不错。（就算啥也不会，也要能说的很牛逼才行。只要抓住了评审的点就可以顺利通过）\n\n这个项目的创新点其实就是华为的Atlas 200 DK设备（Atlas 200 DK 开发者套件是一款高性能AI应用开发板，集成了昇腾310 AI处理器，方便用户快速开发、快速验证，可广泛应用于开发者方案验证、高校教育、科学研究等场景。）怎么说也卖3999元呢，希望后面不要有太多的坑。\n\n下一步的计划肯定是先拿到这个开发板，然后把一切环境都配置好。后面会再这里记录一下使用Atlas 200 DK的过程以及碰到的一些坑。\n\n<!--more-->\n\n### 基本操作\n\n主机密码：Huawei@123\n\n连接atlas：ssh HwHiAiUser@192.168.1.2     atlas密码为：Mind@123        \n\n[解决 shell脚本中SCP命令需要输入密码的问题]: https://blog.csdn.net/u012454773/article/details/72779439\n\n启动mindstudio（版本为2.0.0beta2）：~/program/MindStudio-ubuntu/bin/MindStudio.sh\n\n若连接不上atlas，则在命令行输入 rm -rf ~/.ssh/known_hosts 即可\n\n\n\n主从机文件传输：  https://bbs.huaweicloud.com/blogs/168928\n\n```javascript\nscp -r $HOME/samples/cplusplus/level2_simple_inference/2_object_detection/YOLOV3_coco_detection_picture HwHiAiUser@192.168.1.2:/home/HwHiAiUser/testfile\n```\n\n```html\nscp -r HwHiAiUser@192.168.1.2:/home/HwHiAiUser/mask_detection/output ~/Pictures\n```\n\n**开发板断电后需要重新进行联网配置：**\n\n主机中：\n\nsu root \n\necho \"1\" > /proc/sys/net/ipv4/ip_forward #允许报文转换\n\niptables -t nat -A POSTROUTING -o enp0s3 -s 192.168.1.0/24 -j MASQUERADE\n\niptables -A FORWARD -i enp0s12u2 -o enp0s3 -m state --state RELATED,ESTABLISHED -j ACCEPT\n\niptables -A FORWARD -i enp0s12u2 -o enp0s3 -j ACCEPT\n\nssh HwHiAiUser@192.168.1.2\n\nsu root\n\nroute add default gw 192.168.1.223 dev usb0\n\nping e.huawei.com\n\n\n\n\n\ninstall_path 请根据实际情况修改:     /home/Ascend/Ascend/ascend-toolkit/20.1.rc1/x86_64-linux\n\nscp -r /home/Ascend/samples/cplusplus/level2_simple_inference/2_object_detection/YOLOV3_coco_detection_picture \n\n \n\n### 版本信息\n\n我的cann版本为**3.1.0**：   export DDK_PATH=/home/Ascend/Ascend/ascend-toolkit/latest/arm64-linux\n\nFirmware and Drivers版本为**1.0.8 alpha**\n\n主板型号是**IT21VDMB**\n\n\n\natc${install_path}：   /home/Ascend/Ascend/ascend-toolkit/latest\n\n```\nexport PATH=/home/Ascend/Ascend/ascend-toolkit/latest/atc/ccec_compiler/bin:/home/Ascend/Ascend/ascend-toolkit/latest/atc/bin:$PATH\nexport ASCEND_OPP_PATH=/home/Ascend/Ascend/ascend-toolkit/latest/opp\nexport ASCEND_AICPU_PATH=/home/Ascend/Ascend/ascend-toolkit/latest\n```\n\n/home/Ascend/Ascend/ascend-toolkit/20.1.rc1/atc/bin/atc\n\n![image-20210401155249903](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210401155249903.png)\n\n\n\n![image-20210412101856080](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210412101856080.png)\n\n上述命令的含义： https://support.huaweicloud.com/environment-deployment-Atlas200DK202/atlased_04_0033.html\n\n### 具体案例\n\n#### **摄像头**\n\nhttps://gitee.com/ascend/samples/tree/master/cplusplus/level1_single_api/5_200dk_peripheral/ascendcamera  （我的树莓派摄像头版本为V2）\n\ncd $HOME/ascendcamera/out\n\n./main -i -c 1 -o ./output/filename.jpg --overwrite\n\n**presenterserver：** \n\ncd $HOME/ascendcamera/out\n\n./main -v -c 1 -t 60 --fps 20 -w 704 -h 576 -s ip:192.168.1.223:7002/youngyep        (命令行输7002，浏览器输7003)\n\n若要使用presenter server进行实时显示，则需要在开发环境修改ascendcamera中的script/.conf中的presenter_server_ip、presenter_view_ip、presenter_agent_ip修改为虚拟网卡的ip地址：\n\n![image-20210405203242498](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210405203242498.png)\n\n![image-20210405203313368](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210405203313368.png)\n\n将口罩检测和摄像头的代码通过shell脚本结合：\n\n运行环境：cd ~/mask_detection             ./start_detection.sh\n\n开发环境：scp -r HwHiAiUser@192.168.1.2:/home/HwHiAiUsr/mask_detection/output ~/Pictures\n\n\n\n#### 网线连接\n\n参考： https://blog.csdn.net/qq_44527435/article/details/110441603    我的从机端口号为enp0s3\n\n![image-20210414133624189](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210414133624189.png)\n\n\n\n**socket通信**：https://www.cnblogs.com/fnlingnzb-learner/p/8523242.html\n\n在client的代码中填写server的ip地址（我这里是192.168.1.223）\n\n![image-20210414230902559](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210414230902559.png)\n\nc++网络编程中socket函数：https://blog.csdn.net/bian_qing_quan11/article/details/71713647\n\n\n\n#### 交叉编译程序\n\n交叉编译c++：aarch64-linux-gnu-gcc 后接cpp文件\n\n编译c++：g++ hello.cpp -o hello\n\n\n\n#### uart和目标检测二合一\n\n![image-20210419093448974](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210419093448974.png)\n\n\n\n加入框位置和人的判断后结果（停止1表示目标框在图中间三分之一位置，停止2加上了目标是人的限制条件）\n\n![image-20210420114417098](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210420114417098.png)\n\n![image-20210420114546682](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210420114546682.png)\n\n\n\n目标检测总共可识别的类别：\n\n![image-20210422091827753](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210422091827753.png)\n\n\n\n#### python调用uart接口\n\n```python\nfrom periphery import Serial\n\ndef main():\n    print(\"uart connection test\")\n\n    # Open /dev/ttyAMA1 with baudrate 115200\n    ser = Serial(\"/dev/ttyAMA1\", 115200)\n\n    while 1:\n        print(\"Write to UART\")\n        ser.write(b\"Hello from Atlas 200 DK\\n\")\n        \n    # Read up to 32 bytes, with timeout of 2 seconds\n    readdata = ser.read(32, 2).decode('utf-8')\n    print(f'Received reply: {readdata}')\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nAtlas开发板联网后输入 pip install python-periphery ，然后运行 python3 uart.py 即可。\n\n[Python的串口通信（pyserial） - 东小东 - 博客园 (cnblogs.com)](https://www.cnblogs.com/dongxiaodong/p/9992083.html)\n\n\n\n##### 相关链接\n\n1.  Atlas 200 DK 外设接口样例 https://gitee.com/ascend/samples/tree/master/cplusplus/level1_single_api/5_200dk_peripheral uart案例可参考该链接，将STM32串口和单板串口对接，TXD接RXD，串口波特率配置成一致，可以通讯。16-白-RX 18-绿-TX 20-黑-GND（即外侧那排的第8个引脚为白，第9个为绿，第10个为黑）\n\n\n\n\n\n\n\n##### 发送格式\n\n0xAE 0xEA {len} {cmd} {x0} {x1} {y0} {y1} {w0} {w1} 0xEF 0xFE\nlen = total - 4 (例如该情况len=0x08)\ncmd = 0x11\nx0 = (x + speed_offset)*100 / 256\nx1 = (x + speed_offset)*100 & 0xff\n\nspeed_offset = 200\n(x典型取值50)\n\n![image-20210428101449468](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210428101449468.png)\n\n![image-20210516110157569](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516110157569.png\n\n![image-20210516110020248](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516110020248.png)\n\n\n\n### 避障算法设计\n\n##### 检测区域划分\n\n思路一：只取车前方的检测结果（如只取图像中央三分之一处的检测结果）\n\n思路二：和车道线检测结合到一起，只将车道线范围内的目标识别为障碍物\n\n\n\n\n\n### 项目总体定位\n\n重点在于识别出目标障碍物，而不是躲避障碍物。系统的输出为目标的类型及目标框的位置，优点是采用了搭载华为昇腾处理器的开发板Atlas 200 DK，识别速度非常快，可达几十ms一次。为了方便演示，选用人作为目标障碍物，假定人高度为175，通过拟合来测距，当距离小于设定值时发出停止信号。\n\n\n\n## 总体测试\n\n\n\n摄像头正向\n\n![image-20210516095559800](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516095559800.png)\n\n\n\n运行程序：\n\n```\nssh HwHiAiUser@192.168.1.2\ncd /home/HwHiAiUser/testfile/xuezhang_test/python/level2_simple_inference/2_object_detection/YOLOV3_coco_detection_picture/src\npython3 object_detect.py\n```\n\n\n\n程序运行截图\n\n![image-20210516100236140](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516100236140.png)\n\nuart传输的数据\n\n![image-20210516112012898](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516112012898.png)\n\n\n\n![image-20210516111952166](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516111952166.png)\n\n\n","tags":["Atlas","深度学习","神经网络","障碍物识别","目标识别"],"categories":["毕业设计"]},{"title":"考研感悟","url":"/2021/03/02/随笔/考研感悟/","content":"\n#### 2021/3/2\n\n考研分数出来了，考的很糟糕，政治69，英语一71，数学一102，专业课127，总分369。\n\n按照目前群里的统计，400分以上的已经远超往年，我很大概率不会过线了。我爸想让我调剂，我有点想直接去找工作了。\n\n<!--more-->\n\n一时间不知道如何是好，辛辛苦苦准备了差不多一年，最后的结果就是这样吗？本来估分起码有个三百八九十分，结果谁知数学给了我当头一棒，这一棒瞬间把我打成了一个没有梦想的咸鱼。或许自己真的是数学天赋不够吧，毕竟高考也是数学考砸了。付出了那么多的时间，反而还没别人考的好。或许我真该反思一下是不是自己做了太多的无用功。\n\n或许一开始就不应该选择考研这条路。当初凭着一腔热血以及对步入社会的恐惧，选择了考研，硬撑着颈椎病走到了现在，看着别的同学能坐那不动学一天，好生羡慕，而我顶多坐一个小时就必须出去活动一下，不然疼的受不了。跟别人说我脖子疼，大多也是笑笑，只有得过颈椎病的人才能体会我的难处。\n\n当然，这也并不是说为成绩考差找借口，我承认数学没考好是我自己的原因，我也确确实实尽到了我能做的最大努力。只是确实心有不甘，明明是保研边缘的成绩，最后连研都没考上，做了一年的无用题，真的是对不起我自己，也对不起大家对我的厚望。有时候，我常常想，要是我一直成绩就很差那该多好啊，那样大家就不会对我期望那么高了，我找个工资一般的工作也不会有人说什么了，我是不是就可以做我想做的事情了呢？（不过，我到底想做什么呢，哈哈我自己也不知道，我可能只是想混吧）\n\n哎，明明感觉心里有很多东西想说，可是真要我来写，又不知道写什么了，要是研真没考上，找几个兄弟去喝酒吧，或许这样我能把堵在心中的话说出来吧。记得以前我说过，要是什么时候我开始喝酒了，那就证明我向这个世界妥协了。我真的有点累了，虽然确实很想为这个世界做点什么，做点什么能让后人记得的好事。小时候以为自己以后能够很厉害，自己的qq号昵称都是 I am the one and only (我独一无二) 。而现在才发现，自己不过是一个毫不起眼的小角色罢了，特别是在东九下课的时候，上千人蜂拥而出，在这人潮中，所有人都和我差不多，仅仅在这些人中，我就已经被淹没的无影无踪，只能随着人潮慢慢前进。\n\n​\t![image-20210302221734426](考研感悟/image-20210302221734426.png)\n\n今天在一本书中看到上面一段话，有人又成功考上了研，又拿了几个好offer，又谈了恋爱，而我一无所有。\n\n时候不早了，教室只有我一个人了，下次再写吧。\n\n![image-20210302222157963](考研感悟/image-20210302222157963.png)\n\n#### 2021/3/5\n\n这两天感觉好了很多，决定一条路走到黑了，不管找工作的事情了，这让我轻松许多。在小木虫上发布了我的调剂信息后，一个中科院近物所的老师加了我，很希望我去他们那。同时曾老师也帮我找了一些调剂信息，虽然不太行，但还是投了一个西电的。总之是有保底的了，那就加油冲吧！\n\n#### 2021/3/12\n\n今年校线出了，还是330分，不过校线也没啥太大的意义。今天搞完了毕设的开题答辩，可以短期内不用管毕设的事情了，接下来核心任务还是去认真准备复试，如果上天真的给了我这个机会，而我却没有好好珍惜的话，那才是真的难受。既然一切都还有希望，那么在最后揭晓的那一刻来临之前，都要努力去拼一拼啊！还有那么多人为我加油鼓气呢！（在这里真心的感谢为我鼓起和提供帮助的人）我怎么能自己先放弃了呢。明天得认真看笔试了。\n\n关于找工作的事情，可以先看看嵌入式开发的方向，毕竟我本科基本都是在做嵌入式，对于我来说，这个方向所需的准备时间也少一些，没进复试再找工作！现在要分清孰轻孰重，学长说让我好好准备，先别管毕设了，复试逆袭的可能性还很大。\n\n同时，每天晚上开始梳理以前电赛或者课设所做的东西，并整理成md文档发到博客上吧，每天一个项目！\n\n噗，现在看上面3月2日的记录，感觉自己好憨啊，没事儿就别瞎想，做好当下的事情！累了就去运动！加油！\n\n#### 2021/3/13\n\n今天中科院的老师给我发消息，说是下周末他们搞线下复试，问我去不去。我想，就算上交没过线，也不会去他们那里吧，去了的话，我或许会后悔很久很久吧。终究还是想去大城市闯一闯啊。我这人有个毛病，似乎很难多线程，如果有一件什么事情等着我去完成，那我就中途不想管其他事情了。\n\n希望能进复试吧！我相信天无绝人之路，无论结果如何，总会走下去的。听习大大的话，不忘初心啊！\n\n#### 2021/3/29\n\n距离复试完有好几天了，这几天感觉发生了好多好多事啊！\n\n首先，很幸运，我进入了复试，今年上交分数线**356分**，没想到这么多高分的情况下，分数线只比去年高了一分，真可以称的上是奇迹再现了！感谢之前在我感到最黑暗的时候鼓励了我的好朋友们！果然，灯不会在，任何时候为我开，是好是坏，该不该，还没来的不想猜~~ 确实很多时候，做好眼前的事就好，不用去猜那些还没有发生的事情。毕竟，谁也说不清楚未来会怎样。\n\n确定进入复试后，自然是疯狂的学，因为复试线发布是15号，线下复试，21号笔试，22号面试。于是我20号便和我爸一起去了上海。终于是成功来到我心心念念了一年的真正的大城市，由于直接坐地铁到了闵行，当我真正踏上这片土地时，也并未觉得有什么不同，心想，上海就这？哈哈。\n\n当21号来到上交时，看到曾经在我电脑桌面的校门真正的出现在我的眼前，顿时觉得好像有那么一点点不真实。有一说一，上交的建筑风格确实完爆我科。第一天是下午笔试，往年都是一个小时，今年改成了俩小时，不过还好，并没有更改题型，还是50道选择题，那这就没啥好说的了，毕竟选择题怎么都可以做，不会考很深入。笔试考完确实轻松了许多，又赶紧回宾馆准备面试。\n\n面试的简历是非常重要的，好在我很早就在开始写我的简历，而且把简历里面可能问到的每个点都思考了一遍，并写在纸上。我被分在了第二天上午，一大早7点多我就到面试地点了，去的时候教室还一个人都没有，谁知面试顺序是按成绩排的，一直等到中午11点半才到我。\n\n------\n\n（下面的面试过程考虑后面出一个对话版，现在可能比较乱，凑合看看我刺激的面试历程吧！\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——**论我是如何在面试时就斩获4位老师青睐的？**）\n\n一进门，一共5位面试老师（用ABCDE来代称吧，有些问题记不清是哪个老师问的了，就不明写了）和一名学生（负责收我们的资料），老师们正在互相交流，好像很随意，也没有全把目光盯到我身上。我将提前准备好的简历和成绩单发给老师们，然后站到讲台上（不同教室不一样，有的教室学生坐着），鞠躬说道，老师好！然后**A老师**开始用英文提问，第一个问题是让我介绍我的毕设。这个我有所准备，所以答上来了，不过由于很紧张，也是吞吞吐吐的。接下来，谁知他让我介绍我毕设里面的一个设备（Atlas 200 DK），啊这，这谁答的上来啊，问这么细，我知道用中文怎么说，但是让我说英文，我无能为力，憋了半天说了个AI compute machine。A老师步步紧逼，继续问我具体有什么功能，我也记不清当时说了啥，反正嗯嗯啊啊了很久，A老师也没继续问了，英语问答环节到此结束。（其实后来想想，这个环节其实也没多重要，我感觉老师就是想留点时间看看我的简历，我吞吞吐吐的时候老师也并没在意，大多都是盯着我的简历在看。）\n\n接着**A老师**就用中文问起了我的电赛经历，让我介绍一下电赛的项目，这可是我花大力气准备了的，当然丝毫不慌，把功能讲了一遍。老师们好像对这很感兴趣，又继续问我原理，（只要是自己真正做了的项目，遇到懂行的老师是真的好，我就是怕老师不问，只要他问，我都能答上来），这我自然是把原理讲了一通，老师们十分满意，脸上都露出了笑容，我也进入了闲聊模式，放松下来了。然后老师又让我介绍一下软件课设，我也就先介绍了一下功能，然后老师问我为什么数据库要选择mongodb，我很实在的说这是我队友负责的部分，我只负责前端的工作，老师也没有继续深究。\n\n后面老师又看到我下面写的大一大二加入学校的电工基地，做的电赛的培训项目，（我写了好多项目）老师随便抽了一个红外线信号传输的项目，问我怎么实现的，（哈哈，幸好我把每个项目都准备了一遍，而且这些都是我真正做过的项目，自然是手到擒来，还是那句话，就怕他不问），我回答原理和结构后，老师们也很高兴，接着跟我讨论里面为什么要用到我说的方法，我依然答上来了。项目的问题基本就到此结束，后面就真的是纯闲聊了。\n\nA老师问我以后想做什么方向，我说只要不是纯硬件都可以，毕竟硬件有时候太玄了！老师笑着总结我的话：”那意思就是，你大一大二搞了两年硬件搞伤心了，想做软件了，可以这么理解吧？那我看看你软件的课程学的怎么样，哦！python成绩挺高的嘛！“我说python是最后课设用pygame做了个植物大战僵尸的游戏。C老师问，那你觉得verilog是属于硬件还是软件呢？我说，应该也算硬件吧，毕竟它叫硬件编程语言嘛。\n\n然后E老师看到我参加了很多课外活动，问我是怎么分配时间的，我说周六周日都会去实验室学电赛的培训，时间安排挺紧的。B老师看到我的绩点，很疑惑的问我绩点这么高怎么没保研？而且成绩排年级前27%。我笑着说大概要前25%才能保研（上交保研率更高，我这个排名在上交就保研了）。老师也笑着说那刚好差一点啊，很可惜。又看到我初试成绩不高，问我排多少名，我说排114，（一共招120个），老师们又笑着说那你这和你保研一样都是边缘啊！说我就是数学考差了点。我也答道确实如此，数学成绩出乎了我的预料。又问我有没有联系导师，我说联系了一个，但是那个老师说不确定有没有名额。\n\n基本就是这些，老师看了下时间说差不多了，我道谢后下去，**B老师笑着对我说祝我好运**。然后老师们都把简历递给了我，但是到**C老师的时候，他说要把我的简历留一份**在这儿，那我自然是同意，旁边的**B老师**对他说，学校规定是不能留学生简历的！几个老师笑了笑，最后还是留下了简历。我走出教室门的时候，**A老师又跟着我出来，跟我说表现不错，让我记一下他电话**，等会跟我打过来。（哈哈，内心极其激动。）\n\n谁知这还没完，本来面试完就差不多12点多了，我出去和我爸（学校不让家长进去，辛苦我爸一直在学校外等我了）找了家餐馆，刚准备开吃，一个电话打过来，就是**A老师**，问我现在在哪，想跟我谈谈，我说正在跟我爸吃饭，然后他说他1点半有个会，让我3点去他办公室找他交流交流。我接完电话，没吃几口菜，又来一个电话（**B老师**），说：“你还记不记得我，我是那个跟你说了祝你好运的老师，想邀请你加入我们团队……”。我加了他微信，他给我发了一个链接让我报名参加他们晚上的所面（可是我因为复试之前联系的老师（叫**0老师**吧）的所面也是在这个晚上，我看在前面老师跟我打电话聊了很久的份上，先去了他那，这个后面再说）。我再次接完电话，又回去没吃几口饭，一个电话再次响起（这次是**C老师**），我一样接完电话加了微信。最后，总算是清净了一会儿，安安心心地吃完了饭。\n\n下午和A老师聊了一个多小时，主要是他在讲，跟我讲他们实验室的研究方向，以及回答我的一些小问题，总之聊的还是很愉快的。聊完后我走到电院草坪上，静静地躺着，终于可以休息一小会儿了！\n\n紧接着，晚上又去参加**0老师**的所面，不过他居然不在，是其他老师面的我，这个所的老师似乎对我的项目并不感兴趣，甚至有一个女老师居然指着我的电赛项目说：”我看你这也没做过什么实际的东西啊，也就是买几个模块来拼一拼而已“（我TM瞬间不想聊下去了，不仅不懂，还在这随便诋毁别人的项目，估计这老师连电赛是啥都不知道吧）。总之，我对这个研究所的印象极其差，非常后悔来这里面试。也因为来了这面试，我错过了B老师的所面，后面问他时，他说名额已经确定，说我怎么不早点跟他说，感到非常可惜。\n\n说实话，这个所面对我的打击很大，有点不想再面试了。不过，也让我明白，找一个懂你的老师是多么重要，我很幸运，复试面试我的5位老师都很好，都很懂我。（我就跟A老师发消息说想去他那，结果谁知，他又说还会安排明天下午让我我见他手下两个小老板）还是不能自己放弃机会啊！最后还是决定去下一个所试一试。（这个研究所是我随便报的，去聊的也很随意，没啥意思，就不多说）。22号的面试就正是结束了，也大概9点多了。\n\n第二天上午好好睡了觉（其实也没睡太着，多在床上躺了会儿而已），结果刚洗漱完，又接到一个电话，也是我之前随便投的一个研究所，是一个女老师（叫**F老师**吧）打来的电话，问我还记不记得他们所，问我还在不在上海，在的话下午想让我到他们那聊聊，我说我下午约了2点面试。然后她说，你肯定不会聊一下午吧，你聊完了跟我发消息再过来。我看老师这般邀请，自然不能拒绝。\n\n下午去见了A老师手下的两个小老板，有一个我很不喜欢，感觉很严肃，而且说还得再测我一轮，给我一个论文，让我一周内复现。我心想这也太搞了吧，面了这么多次还要测试，感觉不信任我，虽然我很喜欢A老师（他是大老板，不会直接带学生），但是不喜欢这个小老板，还是不太想去他们这了。\n\n跟他们聊完后，又发现B老师跟我打电话，说学校多给了他一个名额，问我想不想去，然后让我加他手下一个博士生，跟博士生用微信聊了会儿，说要跟我保持联系。此时我本来想的是，已经有这么多老师对我感兴趣了，要不不去F老师那面试了吧，可到了差不多4点时，F老师又跟我发微信问我情况，让我别急，他们等我去。我心想，老师这么热情，还是去一下吧，不能自己放弃机会啊。\n\n去到F老师那，老师让我坐着等会儿，过一会儿来了两个男老师，**没想到其中一个就是当初面试我的D老师**。（其实我开始以为会是很多同学都来这排队复试，没想到就我一个）这两位老师基本没问我什么技术问题，就是闲聊，说很看好我，还问我喜欢唱谁的歌，室友情况怎么样等等问题（感觉好像是在做性格测试）。总之，这两个老师是我聊的最开心的。\n\n至此，全部面试结束。我和我爸晚上去外滩看了看，然后第二天坐飞机回家了。\n\n回到家，下午我躺在床上补觉，醒来的时候，发现又有电话没接，群里也炸开了锅。原来是复试没通过的收到了死亡短信，我看了看没收到，应该是过了。电话是F老师打来的，还是问我想不想去他们那儿。最终经过我深思熟虑，**选择了F老师的研究所。**\n\n------\n\n这么多场面试下来，也算是涨了很多经验。也让我深刻地认识到，**曾经的努力都没有白费**（大学没白过就足够了）！这波不亏！以后也要继续努力！\n\n------\n\n哈哈，现在看当最开始写的东西，好丧啊，好傻>.<  还是乐观点好啊！\n\n","tags":["考研"],"categories":["随笔"]},{"title":"常用hexo博客操作","url":"/2021/02/12/软件使用/常用hexo博客操作/","content":"\n### 常用hexo指令\n\n**hexo s** (hexo server) : 启动本地服务器，用于预览主题。默认地址： http://localhost:4000/\n\n**hexo g** (hexo generate) : 生成网站静态文件到默认设置的 public 文件夹。\n\n**hexo d** (hexo deploy) : 自动生成网站静态文件，并部署到设定的仓库。\n\n**hexo clean** : 清除缓存文件 db.json 和已生成的静态文件 public。\n\n**hexo new** + md文件名称 ：会在/source/__posts目录下生成md文件以及创建同名的文件夹。\n\n**hexo new page** + 页面名称 ：新建一个页面，默认地址为 主页地址/页面名称。\n\n<!--more-->\n\n### 设置hexo首页只显示部分摘要（不显示全文）\n\n法一：在文章的`front-matter`中添加`description`，其中`description`中的内容就会被显示在首页上，其余一律不显示。\n\n```\n---\ntitle: 让首页显示部分内容\ndate: 2020-02-23 22:55:10\ndescription: 这是显示在首页的概述，正文内容均会被隐藏。\n---\n```\n\n法二：在需要截断的地方加入：\n\n```\n<!--more-->\n```\n\n首页就会显示这条以上的所有内容，隐藏接下来的所有内容。\n\n\n\n### 好用的插件\n\n隐藏文章的插件https://github.com/printempw/hexo-hide-posts\n\nhexo显示pdfhttps://blog.csdn.net/qq_43701912/article/details/107291518  ——我试了很久都无效\n\n\n\n### hexo博客中提供文件下载链接\n\n在博客的suorce文件夹下放入文件，路径为主页路径+文件在suorce文件夹中的路径。\n\n![image-20210314002727138](常用hexo博客操作/image-20210314002727138.png)\n\n如这样的路径就是https://youngyyp.github.io/download/test1.doc\n\n\n\n### 修改文章`front-matter`模板\n\n修改blog/scaffolds/post.md文件即可\n\n\n\n\n\n## 博客迁移\n\n由于本人最近换了电脑，需要将之前的hexo博客迁移至新电脑，故记录下操作流程。最终结果令人震惊。\n\n### 安装hexo\n\n第一步先在新电脑搭建好环境。\n\n环境搭建参考博客[hexo+github搭建博客(超级详细版，精细入微)_过客~励む-CSDN博客](https://blog.csdn.net/victoryxa/article/details/103733655?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control)\n\n### 注意事项\n\n[使用Hexo-asset-image插件导致静态图片路径出错-Hexo采坑(二)_Java Developer Blog-CSDN博客](https://blog.csdn.net/qq_42009500/article/details/118788129)\n\n### 最终结果\n\n我晕，弄了好久图片显示还是有bug，一气之下就想着能不能**直接把原来电脑的blog文件夹整个复制过来，然后hexo g -d**（当然是在搭建好环境的基础上），居然成功上传，而且不用再重新安装插件，而且没有bug！阿哲！这么简单的流程为什么csdn上的大佬们搞这么复杂呢？","tags":["hexo指令"],"categories":["博客"]},{"title":"卫星姿态估计综述","url":"/2021/01/16/计算机视觉/位姿估计/卫星姿态估计review/","content":"\n# A Review on Satellite 6D Pose Estimation with Deep Learning\n\n\n\n## Abstract\n\n准确可靠地获取目标卫星相对于服务航天器的6D位姿是执行在轨飞行任务的关键。然而，很多目标卫星（包括报废的卫星和碎片）是非合作的，并不能主动提供其状态信息或者特殊标记来协助服务航天器，即使其状态信息可用，也无法确保该数据的准确性。由于深度学习的发展，基于单目视觉的6D姿态估计方法受到重视，因此，本文分析了近几年来基于深度学习的6D姿态估计在卫星等太空中的目标上的应用，并比较了不同方法的性能，揭示了哪些因素使得卫星姿态估计具有挑战性。\n\n\n\n## Introduction\n\n近年来，为了解决地球轨道拥挤问题并延长地球静止卫星的寿命，诸如碎片清除、在轨维修和对接操作等在轨飞行任务概念越来越受到学术界和工业界的关注。执行这些任务的关键是要能准确可靠地获取目标卫星相对于服务航天器的位置和姿态（即6D位姿）。然而，太空中很多目标卫星（包括报废的卫星和碎片）是非合作的，并不能主动提供其状态信息或者特殊标记来协助服务航天器，即使其状态信息可用，也无法确保该数据的准确性。因此，建立一个准确而稳定的卫星姿态估计系统尤为重要，该系统需要能够在无人工干预的情况下自动检测出目标卫星的位姿，从而实时监测卫星状态，以便进行碎片清除、在轨维修和对接等操作。\n\n出于对航天器处理器性质和功率的考虑，使用视觉传感器（如摄像机）进行卫星姿态估计的优先级将大于激光雷达、雷达等其他有源传感器，因为视觉传感器的质量更小且功率要求更低。在小型航天器上，低功耗的单目相机将是实现卫星姿态估计的理想传感器。\n\n为了实现卫星姿态估计，必须利用快速而准确的计算机视觉算法，从一个或一组单目图像计算出目标的相对位置和姿态。早期的姿态估计技术主要集中在手工设计的特征匹配，这些方法的鲁棒性和泛化性较差。此外，太空的环境带来了更为严峻的挑战，例如太空中物体深度变化范围大、缺乏大气散射、信噪比低、照明条件恶劣以及动态的地球背景等。近年来，随着深度卷积神经网络的出现，计算机视觉算法取得了重大突破，与先前的技术相比，基于卷积神经网络(CNN)的深度学习方法取得了更好的效果。\n\n然而，基于CNN的深度学习方法有两个主要的缺点：一是深度学习依赖于大量带标签的数据集，二是其运算量较大。在各种地面应用中，有大量的公开数据集来用于姿态估计，进而训练出最先进的深度学习模型，但是在卫星的姿态估计领域，则缺少真实的带标签的公开数据集。此前大多数单目姿态估计的工作都是在合成图像上进行训练和评估，但在有限的合成图像上表现良好的模型不一定能有效地转移到真实环境中。同时，目前相关的研究对于在卫星所搭载硬件上运行的可行性的讨论较少，大多数CNN需要图形处理器(GPU)来加速运算。然而，由于辐射暴露的问题，GPU在太空领域的应用仍然是一个较新的领域。在许多卫星上，只搭载有低功耗的传统处理器，无法实时运行大型的、最先进的CNN模型。\n\n为了解决以上的问题，欧洲航天局的先进概念小组和斯坦福大学的空间会合实验室举办了卫星姿态估计挑战赛，并在2019年2月发布了第一个公开的可用于航天器姿态估计的机器学习数据集。比赛的结果显示，基于PnP姿态解算的方法更为精确，且单独的目标检测步骤对于提升姿态估计的精度有利。但是这次比赛的数据集主要是合成图像，测试结果也是以合成图像为依据。为了弥补真实和合成图像之间的差距，欧洲航天局的先进概念小组和斯坦福大学的空间会合实验室在2021年11月再次举办了卫星姿态估计挑战赛，同时发布了有更多真实图像的数据集。\n\n本文在第二节回顾了卫星姿态估计相关研究后，在第三节介绍了相关的数据集，最后在第四节对未来进行了展望。\n\n## METHODS ON SATELLITE POSE ESTIMATION\n\n本节我们首先讨论了卫星姿态估计所面临的挑战，并回顾了近三年来卫星姿态估计的主要方法，这些方法可以大致分为两类，即单阶段和两阶段方法。两阶段方法首先识别出目标卫星在图像中的位置，然后将图片裁剪并缩放至固定大小后进行姿态估计，而单阶段方法则不区分目标检测和姿态估计，使用端到端的网络来进行卫星姿态估计。\n\n### Difficulties\n\n卫星姿态估计系统设计中遇到的主要困难包括：\n\n- 物体深度变化范围大：虽然6D位姿估计是CV中的热点领域，但是以往的研究并没有重点关注物体的尺度变化问题，大多集中于研究较近物体的姿态，而在太空中，由于缺乏大气散射，使得目标可以从很远的距离被看到，从而需要充分考虑物体的尺度变化情况。\n- 大气散射的缺乏：太空中缺乏大气散射，这会导致高对比度、欠曝光/过曝光、显著的镜面反射等问题，从而对单目相机所拍图片的质量产生较大影响。(可插图)\n- 数据集的缺乏：由于卫星的真实照片难以获取，目前可用的基准数据集大都采用电脑合成或者在实验室人工环境下使用卫星模型进行拍摄，没有充分强调缺乏大气散射的影响，并且大多描述的是距离非常近的目标\n- 实时性和轻量化要求高：在许多卫星上，只搭载有低功耗的传统处理器，无法实时运行大型的的CNN模型，需要设计轻量化的网络。\n\n### Two-stage methods\n\n两阶段指的是将卫星目标的检测和姿态估计看成两个独立的步骤，先由目标检测网络检测出目标卫星在图像中的位置，在将目标卫星所在区域进行裁剪并缩放至统一大小，然后将缩放后的图片作为姿态估计模板的输入。通过独立的目标检测步骤，能够很好地适应目标在图像不同位置以及不同距离的情况，进而获得较高的精度。\n\n1. UniAdelaide：这个队伍获得了2019年卫星姿态估计比赛的第一名，该团队首先通过多视角三角测量重建卫星模型，并选取了卫星模型上的11个关键点。然后训练了一个CNN目标检测网络用于框选出目标卫星边界，以裁剪卫星所在区域。他们还使用了HRNet架构来对裁剪后的图像进行预定义关键点的回归，在得到关键点的2D-3D对应后，使用稳健的非线性优化来解算目标姿态。（这里可以插图）\n2. Real-time spacecraft pose estimation: 虽然UniAdelaide所设计的两阶段系统能够很准确的识别出卫星的姿态，但是其运行速度较慢，无法满足实时性的要求。为此，The University of Texas at Austin 的研究人员提出了一个轻量化的系统，在保持最先进精度的情况下，大大降低了计算的成本，其网络参数量降为了UniAdelaide的十分之一。与UniAdelaide类似，该系统是一个由目标检测CNN、关键点回归CNN和PNP解算器组成的。它还包括最后的误差预测步骤，该步骤识别不良姿势估计，并用“未检测”来代替它们。此外，整个体系结构使用一种新的合成数据生成方案进行训练，该方案生成具有真实空间图像中存在的许多降级的照片级真实感图像。\n\n### Single-stage methods\n\n单阶段指的是将目标检测和关键点回归看成统一的步骤，用一个端到端的网络来输出预测的关键点位置，这样的联合训练减少了许多不必要的重复计算，使得网络结构更为简单和轻量化。不过也有一些单阶段网络跳过了关键点的回归，直接输出目标的位置和姿态，如UrsoNet。\n\n1. UrsoNet：\n2. EPFL_cvlab：该系统采用单阶段的端到端网络将目标检测和关键点回归看成统一的步骤，用一个端到端的网络来输出预测的关键点位置，鲁棒性更强；采用编码器-解码器体系结构，并在解码器的每一级，建立3D到2D的对应关系；使用了YOLOv3中的Darknet-53作为主干，并采用了特征金字塔网络（FPN）的结构，但与大多数依赖此类网络的方法不同，本系统将每个训练实例分配到多个金字塔级别，以促进多尺度信息的联合使用。最后，使用基于RANSAC的PnP策略从这些对应集合中推断出一个可靠的位姿。同时在训练时，为了解决2D重投影误差方法的缺陷，采用基于3D位置而不是2D投影来最小化训练损失\n\n\n\n### （方法比较？）\n\n\n\n## DATASET \n\n在本节中，我们列举了卫星姿态估计领域目前公开的且质量较高的数据集，包括SPEED,URSO,SPEED+和SwissCube。\n\n1. SPEED:\n\n2. SPEED+:与来自旧SPEED数据集的真实图像相比，姿势的数量、姿势标签的保真度和光线条件的变化都得到了显著改善。（SPEED数据集和SPEED+的具体结构如图所示）\n3. URSO:URSO利用虚幻引擎4(UE4)的功能来渲染逼真的图像，例如基于物理的材质、光晕和镜头光斑。环境中的照明由平行光和聚光灯组成，分别模拟太阳光和地球反照率。为了模拟太阳，我们使用了带有UE4光晕散射卷积的发射材质。地球被模拟成一个高度多边形的球体，纹理上有来自蓝色大理石下一代系列的21600,10800张地球和云图。这将进一步遮罩以从海洋表面获得镜面反射。此外，还使用第三方资源对大气散射进行建模。\n\t我们的场景包括联盟号和龙号航天器模型，几何图形从3D模型存储库导入。\n4. SwissCube\n\n该团队利用虚幻的引擎4（UE4）功能来渲染逼真的图像，例如基于物理的材料，水华和镜头耀斑。他们环境中的照明只是由定向光和聚光灯组成，分别模拟太阳光和地球反照率。环境照明被禁用，为了模拟太阳，他们使用了具有UE4开花散射卷积的发射材料体。地球被建模为高多边形球体，纹理为21600×10800地球。这进一步被掩盖以获得来自海面的镜面反射。另外，第三方资产用于模拟大气散射。他们的场景包括Soyuz和Dragon航天器模型，其几何结构从3D模型库导入\n\n## CONCLUSION AND FUTURE WORK\n\n在本文中，我们对于卫星姿态估计的背景做了详细的介绍，指出了卫星姿态估计面临的问题和挑战，整理了最新的卫星姿态估计方法，并从中选出了4个最具代表性的方法，将它们分为了两阶段和单阶段两种类型，同时回顾了每种方法的特点。然后对于目前卫星姿态估计领域公开且高质量的数据集做出了简要的介绍，这将为未来的研究者提供很大的便利。\n\n总体而言，各种DNN架构能够很好地估计不合作的航天器的姿态，前提是已知目标的3D模型或预定的3D关键点坐标。然而，同样的网络结构在真实图像上的性能相对较差，因为真实图像与用于训练DNN的合成图像具有不同的统计分布。由于在未来的空间任务中部署的任何DNN无疑都将利用合成图像作为主要的训练来源，因此未来必须找到合适的方法来缩小合成图像与真实图像之间的差距。\n\n## Acknowledgment\n\n作者们想要感谢钱久超老师的指导以及潘佳萌学长的帮助\n\n","tags":["Satellite","Pose Estimation"],"categories":["位姿估计"]}]