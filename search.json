[{"title":"【毕业设计】基于Atlas_200_DK的障碍物识别系统设计与实现","url":"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","content":"<h3 id=\"开题答辩——2021-3-12\"><a href=\"#开题答辩——2021-3-12\" class=\"headerlink\" title=\"开题答辩——2021/3/12\"></a>开题答辩——2021/3/12</h3><p>毕业设计的题目是《基于<strong>Atlas 200 DK</strong>的障碍物识别系统设计与实现》，今天进行了开题答辩，感觉还不错。（就算啥也不会，也要能说的很牛逼才行。只要抓住了评审的点就可以顺利通过）</p>\n<p>这个项目的创新点其实就是华为的Atlas 200 DK设备（Atlas 200 DK 开发者套件是一款高性能AI应用开发板，集成了昇腾310 AI处理器，方便用户快速开发、快速验证，可广泛应用于开发者方案验证、高校教育、科学研究等场景。）怎么说也卖3999元呢，希望后面不要有太多的坑。</p>\n<p>下一步的计划肯定是先拿到这个开发板，然后把一切环境都配置好。后面会再这里记录一下使用Atlas 200 DK的过程以及碰到的一些坑。</p>\n<a id=\"more\"></a>\n<h3 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h3><p>主机密码：Huawei@123</p>\n<p>连接atlas：ssh HwHiAiUser@192.168.1.2     atlas密码为：Mind@123        </p>\n<p>启动mindstudio（版本为2.0.0beta2）：~/program/MindStudio-ubuntu/bin/MindStudio.sh</p>\n<p>若连接不上atlas，则在命令行输入 rm -rf ~/.ssh/known_hosts 即可</p>\n<p>主从机文件传输：  <a href=\"https://bbs.huaweicloud.com/blogs/168928\">https://bbs.huaweicloud.com/blogs/168928</a></p>\n<pre class=\" language-lang-javascript\"><code class=\"language-lang-javascript\">scp -r $HOME/samples/cplusplus/level2_simple_inference/2_object_detection/YOLOV3_coco_detection_picture HwHiAiUser@192.168.1.2:/home/HwHiAiUser/testfile\n</code></pre>\n<pre class=\" language-lang-html\"><code class=\"language-lang-html\">scp -r HwHiAiUser@192.168.1.2:/home/HwHiAiUser/mask_detection/output ~/Pictures\n</code></pre>\n<p><strong>开发板断电后需要重新进行联网配置：</strong></p>\n<p>主机中：</p>\n<p>su root </p>\n<p>echo “1” &gt; /proc/sys/net/ipv4/ip_forward #允许报文转换</p>\n<p>iptables -t nat -A POSTROUTING -o enp0s3 -s 192.168.1.0/24 -j MASQUERADE</p>\n<p>iptables -A FORWARD -i enp0s12u2 -o enp0s3 -m state —state RELATED,ESTABLISHED -j ACCEPT</p>\n<p>iptables -A FORWARD -i enp0s12u2 -o enp0s3 -j ACCEPT</p>\n<p>ssh HwHiAiUser@192.168.1.2</p>\n<p>su root</p>\n<p>route add default gw 192.168.1.223 dev usb0</p>\n<p>ping e.huawei.com</p>\n<p>install_path 请根据实际情况修改:     /home/Ascend/Ascend/ascend-toolkit/20.1.rc1/x86_64-linux</p>\n<p>scp -r /home/Ascend/samples/cplusplus/level2_simple_inference/2_object_detection/YOLOV3_coco_detection_picture </p>\n<h3 id=\"版本信息\"><a href=\"#版本信息\" class=\"headerlink\" title=\"版本信息\"></a>版本信息</h3><p>我的cann版本为<strong>3.1.0</strong>：   export DDK_PATH=/home/Ascend/Ascend/ascend-toolkit/latest/arm64-linux</p>\n<p>Firmware and Drivers版本为<strong>1.0.8 alpha</strong></p>\n<p>主板型号是<strong>IT21VDMB</strong></p>\n<p>atc${install_path}：   /home/Ascend/Ascend/ascend-toolkit/latest</p>\n<pre><code>export PATH=/home/Ascend/Ascend/ascend-toolkit/latest/atc/ccec_compiler/bin:/home/Ascend/Ascend/ascend-toolkit/latest/atc/bin:$PATH\nexport ASCEND_OPP_PATH=/home/Ascend/Ascend/ascend-toolkit/latest/opp\nexport ASCEND_AICPU_PATH=/home/Ascend/Ascend/ascend-toolkit/latest\n</code></pre><p>/home/Ascend/Ascend/ascend-toolkit/20.1.rc1/atc/bin/atc</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210401155249903.png\" alt=\"image-20210401155249903\"></p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210412101856080.png\" alt=\"image-20210412101856080\"></p>\n<p>上述命令的含义： <a href=\"https://support.huaweicloud.com/environment-deployment-Atlas200DK202/atlased_04_0033.html\">https://support.huaweicloud.com/environment-deployment-Atlas200DK202/atlased_04_0033.html</a></p>\n<h3 id=\"具体案例\"><a href=\"#具体案例\" class=\"headerlink\" title=\"具体案例\"></a>具体案例</h3><h4 id=\"摄像头\"><a href=\"#摄像头\" class=\"headerlink\" title=\"摄像头\"></a><strong>摄像头</strong></h4><p><a href=\"https://gitee.com/ascend/samples/tree/master/cplusplus/level1_single_api/5_200dk_peripheral/ascendcamera\">https://gitee.com/ascend/samples/tree/master/cplusplus/level1_single_api/5_200dk_peripheral/ascendcamera</a>  （我的树莓派摄像头版本为V2）</p>\n<p>cd $HOME/ascendcamera/out</p>\n<p>./main -i -c 1 -o ./output/filename.jpg —overwrite</p>\n<p><strong>presenterserver：</strong> </p>\n<p>cd $HOME/ascendcamera/out</p>\n<p>./main -v -c 1 -t 60 —fps 20 -w 704 -h 576 -s ip:192.168.1.223:7002/youngyep        (命令行输7002，浏览器输7003)</p>\n<p>若要使用presenter server进行实时显示，则需要在开发环境修改ascendcamera中的script/.conf中的presenter_server_ip、presenter_view_ip、presenter_agent_ip修改为虚拟网卡的ip地址：</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210405203242498.png\" alt=\"image-20210405203242498\"></p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210405203313368.png\" alt=\"image-20210405203313368\"></p>\n<p>将口罩检测和摄像头的代码通过shell脚本结合：</p>\n<p>运行环境：cd ~/mask_detection             ./start_detection.sh</p>\n<p>开发环境：scp -r HwHiAiUser@192.168.1.2:/home/HwHiAiUsr/mask_detection/output ~/Pictures</p>\n<h4 id=\"网线连接\"><a href=\"#网线连接\" class=\"headerlink\" title=\"网线连接\"></a>网线连接</h4><p>参考： <a href=\"https://blog.csdn.net/qq_44527435/article/details/110441603\">https://blog.csdn.net/qq_44527435/article/details/110441603</a>    我的从机端口号为enp0s3</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210414133624189.png\" alt=\"image-20210414133624189\"></p>\n<p><strong>socket通信</strong>：<a href=\"https://www.cnblogs.com/fnlingnzb-learner/p/8523242.html\">https://www.cnblogs.com/fnlingnzb-learner/p/8523242.html</a></p>\n<p>在client的代码中填写server的ip地址（我这里是192.168.1.223）</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210414230902559.png\" alt=\"image-20210414230902559\"></p>\n<p>c++网络编程中socket函数：<a href=\"https://blog.csdn.net/bian_qing_quan11/article/details/71713647\">https://blog.csdn.net/bian_qing_quan11/article/details/71713647</a></p>\n<h4 id=\"交叉编译程序\"><a href=\"#交叉编译程序\" class=\"headerlink\" title=\"交叉编译程序\"></a>交叉编译程序</h4><p>交叉编译c++：aarch64-linux-gnu-gcc 后接cpp文件</p>\n<p>编译c++：g++ hello.cpp -o hello</p>\n<h4 id=\"uart和目标检测二合一\"><a href=\"#uart和目标检测二合一\" class=\"headerlink\" title=\"uart和目标检测二合一\"></a>uart和目标检测二合一</h4><p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210419093448974.png\" alt=\"image-20210419093448974\"></p>\n<p>加入框位置和人的判断后结果（停止1表示目标框在图中间三分之一位置，停止2加上了目标是人的限制条件）</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210420114417098.png\" alt=\"image-20210420114417098\"></p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210420114546682.png\" alt=\"image-20210420114546682\"></p>\n<p>目标检测总共可识别的类别：</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210422091827753.png\" alt=\"image-20210422091827753\"></p>\n<h4 id=\"python调用uart接口\"><a href=\"#python调用uart接口\" class=\"headerlink\" title=\"python调用uart接口\"></a>python调用uart接口</h4><pre class=\" language-lang-python\"><code class=\"language-lang-python\">from periphery import Serial\n\ndef main():\n    print(\"uart connection test\")\n\n    # Open /dev/ttyAMA1 with baudrate 115200\n    ser = Serial(\"/dev/ttyAMA1\", 115200)\n\n    while 1:\n        print(\"Write to UART\")\n        ser.write(b\"Hello from Atlas 200 DK\\n\")\n\n    # Read up to 32 bytes, with timeout of 2 seconds\n    readdata = ser.read(32, 2).decode('utf-8')\n    print(f'Received reply: {readdata}')\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<p>Atlas开发板联网后输入 pip install python-periphery ，然后运行 python3 uart.py 即可。</p>\n<p><a href=\"https://www.cnblogs.com/dongxiaodong/p/9992083.html\">Python的串口通信（pyserial） - 东小东 - 博客园 (cnblogs.com)</a></p>\n<h5 id=\"相关链接\"><a href=\"#相关链接\" class=\"headerlink\" title=\"相关链接\"></a>相关链接</h5><ol>\n<li>Atlas 200 DK 外设接口样例 <a href=\"https://gitee.com/ascend/samples/tree/master/cplusplus/level1_single_api/5_200dk_peripheral\">https://gitee.com/ascend/samples/tree/master/cplusplus/level1_single_api/5_200dk_peripheral</a> uart案例可参考该链接，将STM32串口和单板串口对接，TXD接RXD，串口波特率配置成一致，可以通讯。16-白-RX 18-绿-TX 20-黑-GND（即外侧那排的第8个引脚为白，第9个为绿，第10个为黑）</li>\n</ol>\n<h5 id=\"发送格式\"><a href=\"#发送格式\" class=\"headerlink\" title=\"发送格式\"></a>发送格式</h5><p>0xAE 0xEA {len} {cmd} {x0} {x1} {y0} {y1} {w0} {w1} 0xEF 0xFE<br>len = total - 4 (例如该情况len=0x08)<br>cmd = 0x11<br>x0 = (x + speed_offset)<em>100 / 256<br>x1 = (x + speed_offset)</em>100 &amp; 0xff</p>\n<p>speed_offset = 200<br>(x典型取值50)</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210428101449468.png\" alt=\"image-20210428101449468\"></p>\n<p>![image-20210516110157569](【毕业设计】基于Atlas-200-DK的障碍物识别系统设计与实现/image-20210516110157569.png</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210516110020248.png\" alt=\"image-20210516110020248\"></p>\n<h3 id=\"避障算法设计\"><a href=\"#避障算法设计\" class=\"headerlink\" title=\"避障算法设计\"></a>避障算法设计</h3><h5 id=\"检测区域划分\"><a href=\"#检测区域划分\" class=\"headerlink\" title=\"检测区域划分\"></a>检测区域划分</h5><p>思路一：只取车前方的检测结果（如只取图像中央三分之一处的检测结果）</p>\n<p>思路二：和车道线检测结合到一起，只将车道线范围内的目标识别为障碍物</p>\n<h3 id=\"项目总体定位\"><a href=\"#项目总体定位\" class=\"headerlink\" title=\"项目总体定位\"></a>项目总体定位</h3><p>重点在于识别出目标障碍物，而不是躲避障碍物。系统的输出为目标的类型及目标框的位置，优点是采用了搭载华为昇腾处理器的开发板Atlas 200 DK，识别速度非常快，可达几十ms一次。为了方便演示，选用人作为目标障碍物，假定人高度为175，通过拟合来测距，当距离小于设定值时发出停止信号。</p>\n<h2 id=\"总体测试\"><a href=\"#总体测试\" class=\"headerlink\" title=\"总体测试\"></a>总体测试</h2><p>摄像头正向</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210516095559800.png\" alt=\"image-20210516095559800\"></p>\n<p>运行程序：</p>\n<pre><code>ssh HwHiAiUser@192.168.1.2\ncd /home/HwHiAiUser/testfile/xuezhang_test/python/level2_simple_inference/2_object_detection/YOLOV3_coco_detection_picture/src\npython3 object_detect.py\n</code></pre><p>程序运行截图</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210516100236140.png\" alt=\"image-20210516100236140\"></p>\n<p>uart传输的数据</p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210516112012898.png\" alt=\"image-20210516112012898\"></p>\n<p><img src=\"/2021/03/12/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%E3%80%91%E5%9F%BA%E4%BA%8EAtlas-200-DK%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20210516111952166.png\" alt=\"image-20210516111952166\"></p>\n","categories":["毕业设计"],"tags":["Atlas","深度学习","神经网络","障碍物识别","目标识别"]},{"title":"【电赛】线路负载及故障检测装置","url":"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/","content":"<p>这是2019年的国赛题，赛题链接为 <a href=\"https://youngyyp.github.io/download/2019_c.pdf\">https://youngyyp.github.io/download/2019_c.pdf</a></p>\n<p>我们小组的报告下载链接为 <a href=\"https://youngyyp.github.io/download/2019report.pdf\">https://youngyyp.github.io/download/2019report.pdf</a></p>\n<a id=\"more\"></a>\n<p>题目要求如下图所示：</p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/image-20210314203332601.png\" alt=\"image-20210314203332601\"></p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/image-20210314204048793.png\" alt=\"image-20210314204048793\"></p>\n<p>这个题目我们就差负载的网络结构没有做出来，不过这部分的分值较大，最后只得了省二等奖。还差一名就是省一，差两名就是国奖。（希望明天上交的分数线不要这样搞我心态，球球了&gt;.&lt;）</p>\n<p>最终的作品实物图</p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/98A42894CD1A83DF99E066FF81B52447.jpg\" alt=\"img\"></p>\n<p>比赛期间的伙食</p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/073EEBE3ADC8768CD77F6E7458FEE2EF.jpg\" alt=\"img\"></p>\n<p>调bug</p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/72E10092A5795009CC075D06B096D162.jpg\" alt=\"img\"></p>\n<p>截止时间到</p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/image-20210314215346801.png\" alt=\"image-20210314215346801\"></p>\n<p>装箱评测</p>\n<p><img src=\"/2021/03/13/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/%E3%80%90%E7%94%B5%E8%B5%9B%E3%80%91%E7%BA%BF%E8%B7%AF%E8%B4%9F%E8%BD%BD%E5%8F%8A%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E8%A3%85%E7%BD%AE/72A042A874BB3868CB583A44600D94D2.jpg\" alt=\"img\"></p>\n","categories":["电赛"],"tags":["线路故障检测","自平衡电桥","正交解调"]},{"title":"SLAM十四讲代码bug及解决","url":"/2021/11/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/","content":"<h3 id=\"第三章编译报错：\"><a href=\"#第三章编译报错：\" class=\"headerlink\" title=\"第三章编译报错：\"></a>第三章编译报错：</h3><p>error: ‘decay_t’ is not a member of ‘std’; did you mean ‘decay’?</p>\n<p><a href=\"https://blog.csdn.net/qq_15987811/article/details/122937722\">https://blog.csdn.net/qq_15987811/article/details/122937722</a> 修改编译的C++版本为c++14</p>\n<pre><code>set(CMAKE_CXX_FLAGS \"-std=c++14\")\n</code></pre><h3 id=\"第四章安装Sophus报错\"><a href=\"#第四章安装Sophus报错\" class=\"headerlink\" title=\"第四章安装Sophus报错\"></a>第四章安装Sophus报错</h3><p>error: implicitly-declared ‘Eigen::Map<sophus::so2<double> &gt;::Map(const Eig</sophus::so2<double></p>\n<p><a href=\"https://blog.csdn.net/weixin_41698305/article/details/116547361\">https://blog.csdn.net/weixin_41698305/article/details/116547361</a></p>\n<h3 id=\"第五章\"><a href=\"#第五章\" class=\"headerlink\" title=\"第五章\"></a>第五章</h3><p>安装opencv</p>\n<p>直接通过apt-get一键安装<a href=\"https://blog.csdn.net/weixin_43909881/article/details/94013882\">https://blog.csdn.net/weixin_43909881/article/details/94013882</a></p>\n<pre><code>sudo apt-get update\nsudo apt-get install libcv-dev\nsudo apt-get install libopencv-dev\n</code></pre><h3 id=\"视觉SLAM十四讲（第二版）环境安装心得体会https-blog-csdn-net-weixin-43863574-article-details-107080443\"><a href=\"#视觉SLAM十四讲（第二版）环境安装心得体会https-blog-csdn-net-weixin-43863574-article-details-107080443\" class=\"headerlink\" title=\"视觉SLAM十四讲（第二版）环境安装心得体会https://blog.csdn.net/weixin_43863574/article/details/107080443\"></a>视觉SLAM十四讲（第二版）环境安装心得体会<a href=\"https://blog.csdn.net/weixin_43863574/article/details/107080443\">https://blog.csdn.net/weixin_43863574/article/details/107080443</a></h3><h3 id=\"编译基本过程\"><a href=\"#编译基本过程\" class=\"headerlink\" title=\"编译基本过程\"></a>编译基本过程</h3><pre><code>mkdir build \n\ncd build\n\ncmake -D CMAKE_BUILD_TYPE=Release .. #此处的参数是为了带调试信息\n\nmake -j20  #10核cpu最高设置20\n\nsudo make install  #将程序安装至系统中。如果原始码编译无误，且执行结果正确，便可以把程序安装至系统预设的可执行文件存放路径。默认/usr/local/bin\n</code></pre><h3 id=\"第6章\"><a href=\"#第6章\" class=\"headerlink\" title=\"第6章\"></a>第6章</h3><h4 id=\"安装-Ceres\"><a href=\"#安装-Ceres\" class=\"headerlink\" title=\"安装 Ceres\"></a>安装 Ceres</h4><p>安装前需要安装以下依赖:</p>\n<pre class=\" language-lang-bash\"><code class=\"language-lang-bash\">sudo apt install -y \\\n    liblapack-dev \\\n    libsuitesparse-dev \\\n    libcxsparse3 \\\n    libgflags-dev \\\n    libgoogle-glog-dev \\\n    libgtest-dev\n</code></pre>\n<p>之后即可正常安装：</p>\n<pre class=\" language-lang-bash\"><code class=\"language-lang-bash\">git clone https://github.com/ceres-solver/ceres-solver.git\ncd ceres-solver\nmkdir build && cd build\ncmake -DCMAKE_BUILD_TYPE=Release ..\nmake -j2\nsudo make install\n</code></pre>\n<h4 id=\"安装-G2O\"><a href=\"#安装-G2O\" class=\"headerlink\" title=\"安装 G2O\"></a>安装 G2O</h4><p>安装前需要安装以下依赖:</p>\n<pre class=\" language-lang-bash\"><code class=\"language-lang-bash\">sudo apt install -y \\\n    qt5-qmake \\\n    qt5-default \\\n    libqglviewer-dev-qt5\\\n    libsuitesparse-dev \\\n    libcxsparse3 \\\n    libcholmod3\n</code></pre>\n<p>在安装G2O前，ubuntu20.04如果安装了anaconda可能会有qt5的版本冲突，我在g2o的github issue上找到了解决办法<a href=\"https://github.com/Shubodh/MR-project1-pgo/issues/5\">https://github.com/Shubodh/MR-project1-pgo/issues/5</a></p>\n<ul>\n<li><p>Make sure to do <code>conda deactivate</code> before starting anything. Remove any environment, even <code>base</code>.</p>\n</li>\n<li><p>Type <code>export NO_CONDA_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin</code></p>\n</li>\n<li><p>Now go to your g2o repository, make a folder <code>build</code> and do <code>cd build</code></p>\n</li>\n<li><p>Instead of the <code>cmake ../</code> mentioned by <a href=\"https://github.com/Shubodh\">@Shubodh</a> sir, use <code>PATH=$NO_CONDA_PATH cmake ../</code></p>\n</li>\n<li><p>Instead of the command <code>make</code> mentioned by <a href=\"https://github.com/Shubodh\">@Shubodh</a> sir, use <code>make -j8</code> to use threading for building. It is faster</p>\n</li>\n</ul>\n<p>安装 G2O：</p>\n<pre class=\" language-lang-bash\"><code class=\"language-lang-bash\">git clone https://github.com/RainerKuemmerle/g2o.git\ncd g2o\nmkdir build && cd build\ncmake -DCMAKE_BUILD_TYPE=Release ..\nmake -j2\nsudo make install\n</code></pre>\n<h3 id=\"第7章\"><a href=\"#第7章\" class=\"headerlink\" title=\"第7章\"></a>第7章</h3><p>SLAM十四讲，第七章程序ch7报错， error: ‘CV_LOAD_IMAGE_COLOR’ was not declared in this scope</p>\n<p><a href=\"https://blog.csdn.net/CxC2333/article/details/107848500\">https://blog.csdn.net/CxC2333/article/details/107848500</a></p>\n<p>error while loading shared libraries: libg2o_core.so: cannot open shared object file: No such file or directory</p>\n<p><a href=\"https://blog.csdn.net/LittleEmperor/article/details/80840198\">https://blog.csdn.net/LittleEmperor/article/details/80840198</a></p>\n<p><img src=\"/2021/11/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/data/blog/AI/SLAM十四讲.assets/2022-02-23 18-38-54 的屏幕截图.png\" alt=\"2022-02-23 18-38-54 的屏幕截图\"></p>\n<h3 id=\"射影变换\"><a href=\"#射影变换\" class=\"headerlink\" title=\"射影变换\"></a>射影变换</h3><p><a href=\"https://blog.csdn.net/frozenspring/article/details/77132296?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-77132296-blog-100829246.pc_relevant_paycolumn_v3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-77132296-blog-100829246.pc_relevant_paycolumn_v3&amp;utm_relevant_index=4\">MVG读书笔记——三维空间中的射影几何(一）_炽霜的博客-CSDN博客</a><em>**</em></p>\n","categories":["SLAM"],"tags":["算法"]},{"title":"常用hexo博客操作","url":"/2021/02/12/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/%E5%B8%B8%E7%94%A8hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/","content":"<h3 id=\"常用hexo指令\"><a href=\"#常用hexo指令\" class=\"headerlink\" title=\"常用hexo指令\"></a>常用hexo指令</h3><p><strong>hexo s</strong> (hexo server) : 启动本地服务器，用于预览主题。默认地址： <a href=\"http://localhost:4000/\">http://localhost:4000/</a></p>\n<p><strong>hexo g</strong> (hexo generate) : 生成网站静态文件到默认设置的 public 文件夹。</p>\n<p><strong>hexo d</strong> (hexo deploy) : 自动生成网站静态文件，并部署到设定的仓库。</p>\n<p><strong>hexo clean</strong> : 清除缓存文件 db.json 和已生成的静态文件 public。</p>\n<p><strong>hexo new</strong> + md文件名称 ：会在/source/__posts目录下生成md文件以及创建同名的文件夹。</p>\n<p><strong>hexo new page</strong> + 页面名称 ：新建一个页面，默认地址为 主页地址/页面名称。</p>\n<a id=\"more\"></a>\n<h3 id=\"设置hexo首页只显示部分摘要（不显示全文）\"><a href=\"#设置hexo首页只显示部分摘要（不显示全文）\" class=\"headerlink\" title=\"设置hexo首页只显示部分摘要（不显示全文）\"></a>设置hexo首页只显示部分摘要（不显示全文）</h3><p>法一：在文章的<code>front-matter</code>中添加<code>description</code>，其中<code>description</code>中的内容就会被显示在首页上，其余一律不显示。</p>\n<pre><code>---\ntitle: 让首页显示部分内容\ndate: 2020-02-23 22:55:10\ndescription: 这是显示在首页的概述，正文内容均会被隐藏。\n---\n</code></pre><p>法二：在需要截断的地方加入：</p>\n<pre><code>&lt;!--more--&gt;\n</code></pre><p>首页就会显示这条以上的所有内容，隐藏接下来的所有内容。</p>\n<h3 id=\"好用的插件\"><a href=\"#好用的插件\" class=\"headerlink\" title=\"好用的插件\"></a>好用的插件</h3><p>隐藏文章的插件<a href=\"https://github.com/printempw/hexo-hide-posts\">https://github.com/printempw/hexo-hide-posts</a></p>\n<p>hexo显示pdf<a href=\"https://blog.csdn.net/qq_43701912/article/details/107291518\">https://blog.csdn.net/qq_43701912/article/details/107291518</a>  ——我试了很久都无效</p>\n<h3 id=\"hexo博客中提供文件下载链接\"><a href=\"#hexo博客中提供文件下载链接\" class=\"headerlink\" title=\"hexo博客中提供文件下载链接\"></a>hexo博客中提供文件下载链接</h3><p>在博客的suorce文件夹下放入文件，路径为主页路径+文件在suorce文件夹中的路径。</p>\n<p><img src=\"/2021/02/12/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/%E5%B8%B8%E7%94%A8hexo%E5%8D%9A%E5%AE%A2%E6%93%8D%E4%BD%9C/image-20210314002727138.png\" alt=\"image-20210314002727138\"></p>\n<p>如这样的路径就是<a href=\"https://youngyyp.github.io/download/test1.doc\">https://youngyyp.github.io/download/test1.doc</a></p>\n<h3 id=\"修改文章front-matter模板\"><a href=\"#修改文章front-matter模板\" class=\"headerlink\" title=\"修改文章front-matter模板\"></a>修改文章<code>front-matter</code>模板</h3><p>修改blog/scaffolds/post.md文件即可</p>\n<h2 id=\"博客迁移\"><a href=\"#博客迁移\" class=\"headerlink\" title=\"博客迁移\"></a>博客迁移</h2><p>由于本人最近换了电脑，需要将之前的hexo博客迁移至新电脑，故记录下操作流程。最终结果令人震惊。</p>\n<h3 id=\"安装hexo\"><a href=\"#安装hexo\" class=\"headerlink\" title=\"安装hexo\"></a>安装hexo</h3><p>第一步先在新电脑搭建好环境。</p>\n<p>环境搭建参考博客<a href=\"https://blog.csdn.net/victoryxa/article/details/103733655?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control\">hexo+github搭建博客(超级详细版，精细入微)_过客~励む-CSDN博客</a></p>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><p><a href=\"https://blog.csdn.net/qq_42009500/article/details/118788129\">使用Hexo-asset-image插件导致静态图片路径出错-Hexo采坑(二)_Java Developer Blog-CSDN博客</a></p>\n<h3 id=\"最终结果\"><a href=\"#最终结果\" class=\"headerlink\" title=\"最终结果\"></a>最终结果</h3><p>我晕，弄了好久图片显示还是有bug，一气之下就想着能不能<strong>直接把原来电脑的blog文件夹整个复制过来，然后hexo g -d</strong>（当然是在搭建好环境的基础上），居然成功上传，而且不用再重新安装插件，而且没有bug！阿哲！这么简单的流程为什么csdn上的大佬们搞这么复杂呢？</p>\n","categories":["博客"],"tags":["hexo指令"]},{"title":"23岁生日感想","url":"/2022/08/18/%E9%9A%8F%E7%AC%94/23%E5%B2%81%E7%94%9F%E6%97%A5%E6%84%9F%E6%83%B3/","content":"<p>转眼间在交大已经过了一年了，</p>\n"},{"title":"21年一战上海交通大学电子系819初试及复试逆袭经验贴","url":"/2021/04/01/%E9%9A%8F%E7%AC%94/%E4%B8%80%E6%88%98%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E7%B3%BB819%E5%88%9D%E8%AF%95%E5%8F%8A%E5%A4%8D%E8%AF%95%E9%80%86%E8%A2%AD%E7%BB%8F%E9%AA%8C%E8%B4%B4/","content":"<h3 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h3><p>我的成绩排名并不太好，写此篇单纯是想记录一下自己的经历，仅供参考。</p>\n<p>参考书什么的，我会简单一提，主要讲下对于819专业课的理解。我更多的想写一写我在考研的过程中的一些感悟和小建议，在我看来这比单纯的列写推荐书目以及进度安排更为重要。</p>\n<p>首先，我希望学弟学妹们在考研之前认真的想一想以下几个问题：</p>\n<ol>\n<li>我为什么要考研？</li>\n<li>为什么不选择工作或出国呢？</li>\n<li>我是一个能沉得住心的人吗？或者说，考研这枯燥的几个月甚至是一年，我能坚持下来吗？</li>\n<li>你能接受考研失败吗？</li>\n</ol>\n<p>其次，我会说说我自己对这几个问题的理解，以及我的一些小感悟和小建议。</p>\n<p>最后，我会讲述我的<strong>复试经历</strong>。（复试面试时一共5位老师，其中有<strong>4位</strong>老师在面试完后都给我打电话邀请我加入团队）</p>\n<a id=\"more\"></a>\n<h3 id=\"个人背景介绍（21届考研）\"><a href=\"#个人背景介绍（21届考研）\" class=\"headerlink\" title=\"个人背景介绍（21届考研）\"></a>个人背景介绍（21届考研）</h3><p>本科：武汉某985，加权排名年级前27%，参加过两年电赛培训。</p>\n<p>初试成绩：总分370– （21年分数线355），出乎意料的数学爆炸（只有一百零几分），所以可以说初试考的很糟糕，且初试成绩排名处于录取人数（120人）的边缘。</p>\n<p>复试：总分160+（满分200分），复试成绩使我排名上涨40+。</p>\n<hr>\n<h3 id=\"参考书推荐\"><a href=\"#参考书推荐\" class=\"headerlink\" title=\"参考书推荐\"></a>参考书推荐</h3><p><strong>专业课：</strong>专业课我考的还不错，我的推荐书目可能有点不一样。信号与系统（ss)  当然还是推荐上交本科的教材——胡光锐老师编写的《信号与系统》（作为课本）以及配套的习题《信号与系统习题精解与考研指导》（俗称白皮书）。不过数字信号处理（dsp）我其实不太推荐奥本海默的书，反而我强推上交本科的教材——刘兴钊老师的《数字信号处理》 （作为课本）。</p>\n<p>奥本海默的书实在是写的太细了，其实对于最近几年的命题趋势来说，没必要这么细。我一开始也是看的奥本的书，啃的很难受，而且上面很多不会考的内容。同时奥本书后的的习题，我刷的时候感觉就是一直抄答案（个人认为时间较紧的话刷一遍重点题就行了，不过记得做好笔记）。直到我有一天翻开了<strong>刘兴钊</strong>老师的书，里面的内容都是活生生的考点啊！而且内容简洁明了，我觉得更适合中国学生，看着本书的效率完爆看奥本的书。而且我做了真题后发现，真题中很多题目都是直接来自于这本书的例题中，同时课后的习题质量也很高，做一遍选填题后对概念的理解会更深入。而且最近几年dsp考试的题型都更偏向于概念，选填题增多，刘的书更符合这个趋势。</p>\n<p>总结，其实819并不是特别难，说难其实是因为奥本的书以及习题特别难啃，但是在我看来，可以以刘的书为主力，奥本的书为辅助。如果不追求130以上的分数，看刘的书绝对是更好的选择，特别是对于时间紧张的同学。</p>\n<p><strong>数学：</strong>不建议大家一直纠结于买谁的书好，多刷题才是王道，而且不要以为以前很少考的今年就不考，复习一定要全面！（我就是吃了这个亏）。具体推荐书目大家看看别的帖子吧，别太纠结。</p>\n<p><strong>英语、政治：</strong>相信我，如果时间紧的话，这两门没必要花太多时间，最后的分数差不了多少。在保证数学专业课能拿高分的情况下再多看看英语，政治最后突击一下分都差不多。（当然，这两门你想打80+的话还是得多下功夫的，但是实际情况是你可能比别人多看很久，最后却只比别人高几分或者甚至还比别人低。大家一般都在70分左右）</p>\n<p>ps：我是有颈椎问题的，初试效率可能比其他人低很多。不过没经历过颈椎病的人估计也无法感同身受，就不多说了，总之，<strong>注意身体</strong>！！！！</p>\n<hr>\n<h3 id=\"问题解答以及一些建议\"><a href=\"#问题解答以及一些建议\" class=\"headerlink\" title=\"问题解答以及一些建议\"></a>问题解答以及一些建议</h3><p>先说说我对下面这几个问题的理解吧。</p>\n<ol>\n<li><p><strong>我为什么要考研？</strong></p>\n<p> 是因为大家都考研吗？还是因为父母的期待？还是因为自己对于职场的恐惧，想给自己一点缓冲的时间？还是因为自己对学术很感兴趣，想进一步深造？还是觉得自己本科过的太混了或者觉得本科学校不够好？还是觉得考研后工资更高？我相信每个人对这个问题都有不同的答案，不管是什么样的回答，希望这个答案能足以坚持你走完这条并不平坦的路。</p>\n</li>\n<li><p><strong>为什么不选择工作或出国呢？</strong></p>\n<p> 其实很多人在考研途中又去选择了工作，或者边找工作边考研。如果能更早的明确以后的方向，岂不是更好？</p>\n<p> 如果你的专业是互联网行业相关（别的我不了解），认真准备一年或者半年是绝对可以找到一份不错的工作的，而且研究生的工资并不会高特别多，如果你中途退出或者因为没有认真准备而没过初试，转而去找工作，为什么不一开始就选择找工作呢？</p>\n<p> 如果家庭条件允许，出国其实也是一个很好的选择，同等努力下，出国可以进入更好的学校。（我个人对国外不向往，热爱祖国，当然家庭条件也不允许哈哈~）</p>\n</li>\n<li><p><strong>我是一个能沉得住心的人吗？或者说，考研这枯燥的几个月甚至是一年，我能坚持下来吗？</strong></p>\n<p> 当你备考时，会陆陆续续听到周围许多同学都找到了工作，有的甚至工资很高，你会继续坚持吗？</p>\n<p> 当你备考时，找到工作的和保研了的人都在和小伙伴愉快的玩耍，你会继续坚持吗？</p>\n<p> 当你备考时，找工作和出国的同学都拿到了offer，而你的未来还很迷茫，你会继续坚持吗？</p>\n<p> 当你备考时，遇到了一位心爱的ta，你会继续坚持吗？</p>\n<p> ……</p>\n</li>\n<li><p><strong>你能接受考研失败吗？</strong></p>\n<p> 我曾经以为我能接受，但我现在觉得我的确接受不了。若你运气不太好（从另一方面也可以说运气比较好了），初试差很多分，考完就知道自己绝对考不上，那么你马上开始准备找工作（1月份开始），春招的时候（3月份最集中）你还有很大的可能找到一份不错的工作。若你运气很差，初试过了复试又被刷了，这时可能已经到了4月份中旬左右，春招已经基本接近尾声，那么，你的机会还有多少？对于我这个不想二战的人来说，我的确很难接受。那么你怎么想？</p>\n</li>\n</ol>\n<p><strong>我的一些小建议：</strong></p>\n<ol>\n<li><p>尽早确定学校，最好不要中途换</p>\n</li>\n<li><p>找几个一起考研的小伙伴，互相监督，互相鼓励，更容易坚持下去哦~</p>\n</li>\n<li><p>学的好不如选的好，选择学校和专业也是一场博弈，非常重要！</p>\n</li>\n<li><p>知乎很好，但不要贪多哦！每个人的说法都各有不同，还是得跟着自己的节奏来</p>\n</li>\n<li><p>考研其实也没那么难，别被一些文章和公众号营造的焦虑困扰。</p>\n</li>\n<li><p>如果觉得自己没啥奖项和项目经历，尽量选择本校或者复试占比小的学校（上交初试500分，复试200分。复试中面试占80分，笔试占100分，英语占20分）</p>\n</li>\n<li><p>千万不要以为初试完了就完事大吉了，复试同样重要！特别是简历要精心打磨，上面的每一个点都要能回答。</p>\n</li>\n<li><p><strong>身体第一！</strong>千万要注意身体啊！！！少久坐！多运动！一定不要因为考研而不坚持运动了啊！不要熬夜！</p>\n</li>\n<li><p>初试更重要的是<strong>坚持</strong>，复试则与你本科前几年所学的知识和项目关系更大，不过不同学校不太一样，有的甚至复试的面试分数基本没啥差距，这也是一个很重要的因素。</p>\n</li>\n</ol>\n<hr>\n<h3 id=\"复试经历\"><a href=\"#复试经历\" class=\"headerlink\" title=\"复试经历\"></a>复试经历</h3><p>3月初可能是我最迷茫最黑暗的时候，那时初试分数刚出来，群里做了个成绩调查，统计结果是400分以上的比以往任何一年都多，大家都预测今年分数线会上涨很多（关于这个，几乎每年都会有人在群里喊分数线必涨，很搞人心态）。我估摸着自己可能处于分数线边缘，非常难受。一边复习一边看调剂信息，同时还想着是不是得准备找工作了，还有毕业设计的老师也在催进度。四重折磨让我压力山大，爸妈和我的很多朋友来鼓励我，很感谢他们。最后我还是决定赌一把，相信光~</p>\n<p>我算是很幸运，比复试线高出了十几分。这让我信心大增，本科的项目经历也为我在复试中增色不少。最终复试逆袭，<strong>排名上涨了40多名</strong>。同时在复试面试结束后，有<strong>4位</strong>面试老师都给我打电话邀请我加入。想看具体的<strong>复试经历</strong>请见我的另一篇博客——《考研感悟》<a href=\"https://youngyyp.github.io/2021/03/02/随笔/考研感悟/\">考研感悟 | young’s blog (youngyyp.github.io)</a></p>\n","categories":["随笔"],"tags":["考研"]},{"title":"日常随笔","url":"/2021/07/04/%E9%9A%8F%E7%AC%94/%E6%97%A5%E5%B8%B8%E9%9A%8F%E7%AC%94/","content":"<h4 id=\"2021-7-4\"><a href=\"#2021-7-4\" class=\"headerlink\" title=\"2021/7/4\"></a>2021/7/4</h4><p>好久没有静下来写点什么了，这几天放假回家，一直都在和朋友们玩儿，散步，吃饭，聊天，看鬼片，拍照，打麻将，游泳，遛狗，弹琴，唱歌。好像挺充实的，但心中却又总觉得空落落的，为什么呢，可能是心中有好多话压抑着没有言说吧。此刻，当我独自一人，戴着耳机，听着舒缓的音乐，敲击着键盘，写下心中所想，才觉得轻松了许多。</p>\n<p>的确像许多人说的那样，我好像总是把自己的事情埋在心里。可能是语言表达能力不够吧，也可能是没有到那种情绪吧。和朋友们一起玩的时候，常常他们玩的很嗨的时候，我内心却毫无波澜。他们或笑，或跳，或讲故事，或聊综艺，而我只是静静地听着，好像他们越兴奋，我离他们越远。不知道这算不算所谓的社恐，不过对于有些特别好的朋友不会出现这样的情况。</p>\n<p>总有人问我，你不打游戏，也不看综艺，也不追剧，你每天都在干什么。是啊，我每天在干什么呢。我想说，我打游戏，而且打的也不少，只是越长大越觉得，缺少了小时候那种对游戏不知名的热爱，现在在我看来打游戏只是一种社交；我也会看剧，只是看的没有许多狂热看剧爱好者看的多，我喜欢好剧，喜欢那种能让人回想好久的剧，而不是紧跟最新更新的剧。若你认为除了这些，就没有其他的事情可做了，或者说就没有其他的娱乐可做了，那你挺惨的。对于我来说，我有着更多喜欢做的事情。我喜欢和朋友一起运动，我喜欢一个人在房间的时候唱歌弹琴，我喜欢在清晨背上包，到图书馆安安静静地看一本书，我喜欢在夕阳落下之前去健身房健身，我喜欢在黄昏之时出去遛狗，我喜欢在月亮高悬之时散步聊天。</p>\n"},{"title":"verilog学习笔记","url":"/2021/07/16/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/verilog%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","content":"<h2 id=\"Modelsim使用技巧\"><a href=\"#Modelsim使用技巧\" class=\"headerlink\" title=\"Modelsim使用技巧\"></a>Modelsim使用技巧</h2><h3 id=\"基本操作流程\"><a href=\"#基本操作流程\" class=\"headerlink\" title=\"基本操作流程\"></a>基本操作流程</h3><h4 id=\"图形界面操作\"><a href=\"#图形界面操作\" class=\"headerlink\" title=\"图形界面操作\"></a>图形界面操作</h4><ol>\n<li>新建工程（建议给每个工程单独建文件夹）</li>\n<li>调试的时候直接在library中的work目录下对testbench文件右键simulate</li>\n</ol>\n<h4 id=\"命令行操作\"><a href=\"#命令行操作\" class=\"headerlink\" title=\"命令行操作\"></a>命令行操作</h4><p>①创建一个工程和工程库;<br>②加载设计文件（包括你编写好的testbench）；<br>③编译源文件；<br>④运行仿真，并查看结果；<br>⑤最后进行工程调试。<br>上面为利用modelsim的仿真步骤，利用do文件可以完成以上1—4步。</p>\n<p>例程<br>（1） sim_run.bat文件用于批处理<br>@set modelsim=C:\\modeltech64_10.7\\win64\\vsim.exe<br>@if exist vsim.wlf del vsim.wlf<br>%modelsim% -do sim_top.do</p>\n<p>（“％环境变量名％”用法的含义是取指定环境变量的值）<br>首先设置modelsim的执行路径，可以是vsim.exe也可以是modelsim.exe<br>然后删除历史运行的vsim.wlf文件，以便后面进行更新<br>最后用DOS命令调用modelsim工具，并在modelsim工具中执行sim._top.do文件<br>(2)sim_top.do文件<br>vlib work<br>//建立work库</p>\n<p>vmap work work<br>//将目前逻辑库work和实际工作库work相映射对应<br>vlog -f vlog-rtl.list </p>\n<h1 id=\"将vlog-rtl-list文件中所有文件编译\"><a href=\"#将vlog-rtl-list文件中所有文件编译\" class=\"headerlink\" title=\"将vlog-rtl.list文件中所有文件编译\"></a>将vlog-rtl.list文件中所有文件编译</h1><p>vopt +acc=npr tb_top -o tb_top_opt</p>\n<h1 id=\"仿真时优化tb-top输出tb-top-opt\"><a href=\"#仿真时优化tb-top输出tb-top-opt\" class=\"headerlink\" title=\"仿真时优化tb_top输出tb_top_opt\"></a>仿真时优化tb_top输出tb_top_opt</h1><p>vsim tb_top_opt</p>\n<h1 id=\"仿真tb-top-opt\"><a href=\"#仿真tb-top-opt\" class=\"headerlink\" title=\"仿真tb_top_opt\"></a>仿真tb_top_opt</h1><p>(也可以直接 vsim -novopt work.tb_top)</p>\n<p>do wave.do</p>\n<h1 id=\"运行wave-do，添加wave的信息\"><a href=\"#运行wave-do，添加wave的信息\" class=\"headerlink\" title=\"运行wave.do，添加wave的信息\"></a>运行wave.do，添加wave的信息</h1><p>run 20ms</p>\n<h1 id=\"运行20ms-（run-all：运行全过程）\"><a href=\"#运行20ms-（run-all：运行全过程）\" class=\"headerlink\" title=\"运行20ms （run -all：运行全过程）\"></a>运行20ms （run -all：运行全过程）</h1><p>（3）vlog-rtl.list<br>../rtl/*.v</p>\n<h1 id=\"上级文件夹rtl的所有v文件\"><a href=\"#上级文件夹rtl的所有v文件\" class=\"headerlink\" title=\"上级文件夹rtl的所有v文件\"></a>上级文件夹rtl的所有v文件</h1><p>../../calc_engine/*.v</p>\n<h1 id=\"上上级文件夹calc-engine文件夹的所有v文件\"><a href=\"#上上级文件夹calc-engine文件夹的所有v文件\" class=\"headerlink\" title=\"上上级文件夹calc_engine文件夹的所有v文件\"></a>上上级文件夹calc_engine文件夹的所有v文件</h1><p>../sim/tb/*.v<br>上级sim/tb文件夹内的所有v文件<br>（4）wave.do<br>通过添加并设置波形参数，保存为wave.do文件</p>\n<p>更多详情可参考 <a href=\"http://blog.sina.com.cn/s/blog_7e2e98ad0101gxx7.html\">http://blog.sina.com.cn/s/blog_7e2e98ad0101gxx7.html</a></p>\n<p> log -r /<em> 对所有的信号记录，运行这个命令后即使在仿真前没有把信号加入wave窗口，仿真完成后直接加入wave窗口就可以查看波形，比较方便，但是缺点是当工程较大和仿真时间很长时仿真速度较慢，占用内存也较大。<br> vsim  -voptargs=”+acc”<br>+acc意思是设计中所有的信号都提供入口，可以观察，而log命令的意思就是在运行仿真的时候信号不用加入wave窗口在开始仿真后直接可以看波形，意思就是他记录了所有的信号波形<br>观察其它窗口的结果，用view </em>命令显示 。view *命令可以观察包括signals、wave、dataflow等窗口文件，也可以分别打开。例如用view signals来观察信号变量。</p>\n<h2 id=\"verilog语法\"><a href=\"#verilog语法\" class=\"headerlink\" title=\"verilog语法\"></a>verilog语法</h2><ul>\n<li><p>always块中被赋值的变量只能为reg</p>\n</li>\n<li><p>不可在不同的always块中对同一个reg赋值</p>\n</li>\n<li><p>assign 语句对已经定义的wire赋值，wire也可在被定义的时候赋值，wire变量只能被赋值一次</p>\n</li>\n<li><p>组合逻辑用阻塞赋值，非组合逻辑用非阻塞赋值。——<strong>组合逻辑</strong>电路在逻辑功能上的特点是任意时刻的输出仅仅取决于该时刻的输入，与电路原来的状态无关。<strong>时序逻辑</strong>电路在逻辑功能上的特点是任意时刻的输出不仅取决于当时的输入信号，而且还取决于电路原来的状态，或者说，还与以前的输入有关</p>\n<pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">  组合逻辑：always @(*)  //组合always块相当于assign语句，因此组合电路存在两种表达方法\n  时序逻辑：always @(posedge clk)\n</code></pre>\n</li>\n<li><p>在<strong>时序逻辑</strong>中，不完整的 if…else… 结构并<strong>不会生成锁存器</strong>，而组合逻辑中不完整的 if…else… 结构就会生成锁存器</p>\n</li>\n<li><p>`timescale 1 ns / 1 ps 该指令用于定义时延、仿真的单位和精度</p>\n</li>\n<li><p>`default_nettype wand 用于默认设置为线网类型 </p>\n</li>\n<li><p>`default_nettype none 会关闭隐式声明功能，有助于查bug</p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/qq397381823/article/details/87521885\">verilog设计过程寄存器使用#1的问题_烟花一时的博客-CSDN博客</a></p>\n</li>\n<li><p>{1’b0,1’b0,1’b0,1’b0,1’b0,1’b0,1’b0,1’b0}可以简单写成{8{1’b0}}</p>\n</li>\n<li><p>input、inout 类型不能声明为 reg 数据类型，因为 reg 类型是用于保存数值的，而输入端口只能反映与其相连的外部信号的变化，不能保存这些信号的值。</p>\n<p>  output 可以声明为 wire 或 reg 数据类型。</p>\n</li>\n<li><h6 id=\"端口连接规则\"><a href=\"#端口连接规则\" class=\"headerlink\" title=\"端口连接规则:\"></a>端口连接规则:</h6><p>  <strong>输入端口</strong></p>\n<p>  模块例化时，从模块外部来讲， input 端口可以连接 wire 或 reg 型变量。这与模块声明是不同的，从<strong>模块内部</strong>来讲，<strong><em>input 端口必须是 wire 型变量</em></strong>。</p>\n<p>  <strong>输出端口</strong></p>\n<p>  模块例化时，<strong>从模块外部来讲，output 端口必须连接 wire 型变量</strong>。这与模块声明是不同的，从<strong>模块内部</strong>来讲，output 端口可以是 wire 或 reg 型变量。</p>\n<p>  <strong>输入输出端口</strong></p>\n<p>  模块例化时，从模块外部来讲，inout 端口必须连接 wire 型变量。这与模块声明是相同的。</p>\n<p>  <strong>悬空端口</strong></p>\n<p>  模块例化时，如果某些信号不需要与外部信号进行连接交互，我们可以将其悬空，即端口例化处保留空白即可，上述例子中有提及。</p>\n<p>  output 端口正常悬空时，我们甚至可以在例化时将其删除。</p>\n<p>  input 端口正常悬空时，悬空信号的逻辑功能表现为高阻状态（逻辑值为 z）。但是，例化时一般不能将悬空的 input 端口删除，否则编译会报错。</p>\n</li>\n<li><pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">  reg [7:0] mem [255:0];//定义在向量名之前的是向量的位宽，定义在向量名之后的维度可以理解为向量数组的长度\n</code></pre>\n</li>\n<li><pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">  always @(*) begin\n      casez (in[3:0])\n          4'bzzz1: out = 0;   // in[3:1]输入什么都可以\n          4'bzz1z: out = 1;\n          4'bz1zz: out = 2;\n          4'b1zzz: out = 3;\n          default: out = 0;\n      endcase\n  end\n  //case项是按顺序检查的(实际上，它更像是生成一个巨大的真值表然后生成超大的门)。注意有输入(例如，4'b1111)匹配多个case项。选择第一个匹配(因此4'b1111匹配第一个case项，out = 0)。\n</code></pre>\n</li>\n<li><pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">  for(i=0;i<3;i=i+1) begin \n        ……  //循环中的 i 最高可到 2\n  end\n\n  for(初始化表达式1; 布尔表达式2; 步进表达式4){\n           循环体3\n  }//执行顺序：1234-->234-->234-->不满足为止。\n</code></pre>\n</li>\n</ul>\n<ul>\n<li><p>端口声明规范，例子如下</p>\n<pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">  module ax_pwm\n  #(\n      parameter N = 32 //pwm bit width \n  )\n  (\n      input         clk,\n      input         rst,\n      input[N - 1:0]period,\n      input[N - 1:0]duty,\n      output        pwm_out \n      );\n   //在声明时统一使用wire，避免模块连接的时候出现问题\n  reg[N - 1:0] period_r;\n  reg[N - 1:0] duty_r;\n  reg[N - 1:0] period_cnt;\n  reg pwm_r;//若要对输出的值进行修改，则再定义一个reg变量，使用assign语句对wire赋值\n  assign pwm_out = pwm_r;\n</code></pre>\n</li>\n</ul>\n<p>仿真时的文件操作</p>\n<ul>\n<li><pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">  //下面进行文件读操作\n  integer file;\n  initial begin   \n      file = $fopen(\"../data/tof.txt\",\"r\");//只读方式打开文件\n      if (!file)\n          $display(\"Could not open file!\");\n      else begin\n          $display(\"Open file success!\");\n      end\n  end\n  -------------------------------------------\n  always @(posedge clk or negedge rstn) begin\n      if(~rstn)begin\n          ……\n      end\n      else begin\n          $fscanf(file,\"%d\",data_i);//将file文件按行读取至data_i,每个时钟读取一次\n      end\n  end\n  -------------------------------------------\n  //下面进行文件写操作\n  wire [DW-1:0] data_o;\n  integer file_out;\n\n  initial begin\n      file_out = $fopen(\"data_out.txt\", \"w\");\n      if (!file)\n          $display(\"Could not open file_out!\");\n      else begin\n          $display(\"Open file_out success!\");\n      end\n  end\n  always @(posedge clk) begin\n      if (……) begin\n          $fwrite(file_out, \"%d\\n\", data_o);//将data_o逐行写入\n      end    \n      else if (……) begin\n          $fclose(file_out);//关闭文件\n          $finish;//关闭仿真\n      end\n  end\n</code></pre>\n</li>\n</ul>\n<h3 id=\"三段式状态机\"><a href=\"#三段式状态机\" class=\"headerlink\" title=\"三段式状态机\"></a>三段式状态机</h3><p><img src=\"/2021/07/16/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/verilog%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/70.jpeg\" alt=\"这里写图片描述\"></p>\n<pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">采用三段式的写法：\n1.状态触发\n2.状态转移\n3.结果输出\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n//第一个always块，时序逻辑，描述现态转移到次态\nalways @ (posedge clk or negedge rst) begin\n    if(rst_n) \n        current_state<=S0;\n    else\n        current_state<=next_state;\nend\n\n//第二个always块，组合逻辑，描述状态转移的条件\nalways @ (current_state) begin  //或者always@(*)\n    next_state = x; //初始化，使得系统复位后能进入正确的状态（可省）\n    case(current_state)\n    S0：begin\n        if(condition1)        next_state = S1;//状态转移\n        else if (condition2)  next_state = S2;//状态转移\n        else next_state <= S0;\n        end\n    S1：begin\n        if(condition3)       next_state = S3;//状态转移\n        else if (condition4) next_state =S4; \n        …\n        end\n    ……\n    default: begin\n            if(condition0)   next_state = S0;//状态转移\n           end\n    endcase\nend\n//第三个always块，时序逻辑，描述输出。这部分也可采用assign输出\nalways @ (posedge clk or negedge rst) begin\nif(rst)\n    out0；\nelse\n    case(current_state)\n    S0: out0;\n    S1: out1;\n    ….\n    default:Out0;\n    endcase\nend\n//////////////////////////////////////////////////////////////////////////////\n</code></pre>\n<h3 id=\"generate语句\"><a href=\"#generate语句\" class=\"headerlink\" title=\"generate语句\"></a>generate语句</h3><pre class=\" language-lang-verilog\"><code class=\"language-lang-verilog\">//四位全加器的实现\nmodule top_module (\n    input [3:0] x,\n    input [3:0] y, \n    output [4:0] sum);\n\n    wire [3:0] cout;\n    assign sum[4]=cout[3];\n\n    fa fa0(\n        .a(x[0]),\n        .b(y[0]),\n        .cin(1'b0),\n        .cout(cout[0]),\n        .sum(sum[0])\n    );\n\n    genvar i;\n    generate \n        for(i=1;i<=3;i=i+1)\n        begin:adds\n               fa fa1(\n            .a(x[i]),\n            .b(y[i]),\n            .cin(cout[i-1]),\n            .cout(cout[i]),\n            .sum(sum[i])\n            );\n        end\n    endgenerate\nendmodule\n\nmodule fa (\n    input a,b,cin,\n    output cout,sum\n);\n    assign {cout,sum} = a+b+cin;    \nendmodule\n</code></pre>\n<h2 id=\"数电知识\"><a href=\"#数电知识\" class=\"headerlink\" title=\"数电知识\"></a>数电知识</h2><p><a href=\"https://blog.csdn.net/weixin_43342105/article/details/104515921\">逻辑代数最大项和最小项的概念表达式的转换及关系_阿锋不知道丶的博客-CSDN博客</a></p>\n<p><a href=\"https://blog.csdn.net/hahasusu/article/details/88244155\">卡诺图化简法<em>hahasusu的专栏-CSDN博客</em>卡诺图</a>   </p>\n<p><a href=\"https://zhidao.baidu.com/question/427622855038635732.html\">卡诺图圈零的时候 表达式怎么写圈1表达式是与或式 圈零是或与式？_百度知道 (baidu.com)</a></p>\n","categories":["FPGA"],"tags":["verilog","modelsim"]},{"title":"【FPGA】图像处理","url":"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/","content":"<h2 id=\"图像的中值滤波\"><a href=\"#图像的中值滤波\" class=\"headerlink\" title=\"图像的中值滤波\"></a>图像的中值滤波</h2><p>统计排序滤波器是非线性滤波器的一种，这里我们学习较为常见的<strong>中值滤波</strong>，并以掩膜大小为3*3进行举例，来说明使用FPGA设计二维中值滤波器的流程。</p>\n<h3 id=\"一、行缓存\"><a href=\"#一、行缓存\" class=\"headerlink\" title=\"一、行缓存\"></a>一、行缓存</h3><p>在FPGA数字图像处理中，行缓存的使用非常频繁，例如我们需要图像矩阵操作的时候就需要进行缓存，例如图像的均值滤波，中值滤波，高斯滤波以及sobel边缘查找等都需要行缓存设计。</p>\n<p><img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210723151332772.png\" alt=\"image-20210723151332772\"><img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210723151214106.png\" alt=\"image-20210723151214106\" style=\"zoom: 80%;\"></p>\n<p>图像数据以数据流的方式，先从左到右，然后从上到下将每一个像素数据输出。若要获取如上图中的3*3矩阵，则必须对行数据进行缓存，如下图所示，采用两个FIFO即可解决行缓存的问题，下图即可同时获取到第一个掩膜所覆盖的9个数据。基本思路是，第一行数据依次输入进来写入fifo1，当写到第一行最后一个数据时，开始从fifo1依次读出数据然后写入fifo2，依次类推。通过行缓存的方式使前三行的输出对齐。</p>\n<p><img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/line_buffer.png\" alt=\"line_buffer.png\"></p>\n<h3 id=\"二、并行全比较排序\"><a href=\"#二、并行全比较排序\" class=\"headerlink\" title=\"二、并行全比较排序\"></a>二、并行全比较排序</h3><p>排序是一种重要的数据运算，传统的排序方法主要靠软件串行方式实现，包括冒泡法、选择法、计数法等，这些算法大多采用循环比较，运算费时，实时性差。不能满足工程上越来越高的实时性要求。并行全比较实时排序算法是基于序列中任意两个数并行比较实现。由于全部数字同时进行比较处理，将会占用大量的处理空间，因此此算法也可称为“<strong>以空间换时间</strong>”排序算法。</p>\n<p>假设有一数组{20，80，40，40，60，70}，定为A0=20、A1=80、 A2=40、A3=40、A4=60、A5=70，要求对该数组按从大到小的顺序排列。排序按以下过程进行 ： </p>\n<p><strong>(1) 两两比较，并累积分数。</strong>考虑到可能出现相同的数，规定<strong>下标越大的数优先级越高</strong>，即：</p>\n<pre class=\" language-lang-c\"><code class=\"language-lang-c\">若m>n,则选用  （Am>=An）? 1：0 ;\n若m<n，则选用  (Am>An)? 1:0 ;\n</code></pre>\n<p><img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210723155419045.png\" alt=\"image-20210723155419045\"></p>\n<p><strong>(2) 按积分排序。</strong>由上图可知，积分越大，表明对应的数越大。</p>\n<h4 id=\"3-3数据的中值\"><a href=\"#3-3数据的中值\" class=\"headerlink\" title=\"3*3数据的中值\"></a>3*3数据的中值</h4><p>若直接对3*3掩膜所覆盖的9个数据进行并行全比较排序，资源消耗较大，这里可以分别对每一行的3个数据进行并行全比较排序，获得每一行的中值，再将这3个中值再进行一次排序，取出中值。（这样取出来的不一定是真正的中值，但比较近似，算法复杂度也不高）</p>\n<h2 id=\"TOF的工作原理和数据处理流程\"><a href=\"#TOF的工作原理和数据处理流程\" class=\"headerlink\" title=\"TOF的工作原理和数据处理流程\"></a>TOF的工作原理和数据处理流程</h2><h3 id=\"一、什么是TOF相机\"><a href=\"#一、什么是TOF相机\" class=\"headerlink\" title=\"一、什么是TOF相机\"></a>一、什么是TOF相机</h3><p>双目相机是一种被动接收自然光的传感器，通过接收自然光利用三角测距的方式描述三维环境。本文的TOF相机则是一种主动发射红外光的传感器，通过发射光脉冲并接收打到物体反射回来的脉冲信号，最后计算光脉冲的飞行时间得到目标物体的距离。</p>\n<p>与激光雷达不同，TOF相机不是以逐点扫描的方式感知环境信息，而是以面阵的方式得到深度图。</p>\n<h3 id=\"二、TOF相机的技术原理\"><a href=\"#二、TOF相机的技术原理\" class=\"headerlink\" title=\"二、TOF相机的技术原理\"></a>二、TOF相机的技术原理</h3><p>TOF相机有两种成像方法，分别为脉冲法和连续波调制法，下面分别介绍其技术原理和优缺点。</p>\n<ol>\n<li><p><strong>脉冲法</strong>：通过记录脉冲波从发出到返回的时间来计算相机到物体的距离，即<strong>D=0.5×c×Δt</strong>（其中c表示光在空气中传播的速度，Δt表示脉冲信号从相机到目标往返的时间）</p>\n<p> <img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210727103401146.png\" alt=\"image-20210727103401146\" style=\"zoom:150%;\"></p>\n<p> 通过一个<strong>高频率</strong>的时钟驱动计数器对收发脉冲之间的时间进行计数，使得计数时钟的周期必须远小于发送脉冲和接收脉冲之间的时间才能够保证足够的精度。但是如果要达到毫米级别的测量，对控制时钟，发射单元等电子元器件的精度都是一项挑战。这就是为什么无人驾驶中应用的激光雷达传感器常基于脉冲法，因为该方法比较适合<strong>中远距离</strong>的测量。</p>\n<p> <strong>脉冲法</strong>测量方式简单，占空比窄检测距离远；但是它易受环境光和元器件精度影响，测量精度相对较低。</p>\n</li>\n<li><p><strong>连续波调制法</strong>：使用调制光照射场景, 并测量通过场景中的物体反射后返回光的相位延迟。得到相位延迟后, 再使用正交采样技术测量间接得到距离，即<strong>D=0.5×c×φ×T/2π</strong>（其中c表示光在空气中传播的速度，T表示调制周期，ϕ表示发射和接收波形的相位差）</p>\n<p> <img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210727103341053.png\" alt=\"image-20210727103341053\"></p>\n<p> 该方法比较适合<strong>中短距离</strong>的测量，精度往往可以达到毫米级，在机器人等应用中使用的TOF相机大多采用的是连续波调制的方法。<strong>相位差</strong>可以消除测量器件带来的固定偏差但是采样次数多，导致测量耗时帧率低。</p>\n</li>\n</ol>\n<h3 id=\"三、如何将相位偏移转换为距离？\"><a href=\"#三、如何将相位偏移转换为距离？\" class=\"headerlink\" title=\"三、如何将相位偏移转换为距离？\"></a>三、如何将相位偏移转换为距离？</h3><p>两种方法的距离求解公式都是D=c×t。那么t是如何得到的呢？脉冲法比较直接，就不多叙述。</p>\n<p>连续波调制法通过测量相位差来计算时间。采用<strong>四次曝光</strong>原理来测量相位差，即传感器上的每个像素以相等的间隔对场景反射的光进行四次采样。</p>\n<p>在连续波调制过程中通常将连续波近似为正弦波划分成4个窗口进行采样，并且采样时间间隔相同。设定四次曝光的采样相位分别为：0°，90°，180°，270°。如下图所示：</p>\n<p><img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210727141830496.png\" alt=\"image-20210727141830496\" style=\"zoom: 67%;\"></p>\n<p><img src=\"/2021/07/23/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%92%8C%E7%A1%AC%E4%BB%B6/FPGA/%E3%80%90FPGA%E3%80%91%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/image-20210727144035880.png\" alt=\"image-20210727144035880\"></p>\n<p>物体与传感器的距离和相位差可定义如下：</p>\n<script type=\"math/tex; mode=display\">\nDmax = c/2f  \\tag{1}</script><script type=\"math/tex; mode=display\">\nφ=acrtan[(Q_3-Q_1)/(Q_0-Q_2)] \\tag{2}</script><script type=\"math/tex; mode=display\">\nD=cφ/4πf \\tag{3}</script><p>其中Dmax是最大测量距离，D是目标距离，f是ToF发射波的频率，c是光速，Q0-Q3分别为0°-270°的采样值。</p>\n<p>公式2的具体推导过程见参考文献[1]。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><p><a href=\"https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017269863.nh&amp;v=MjQ4NDIyNkdiRytGOW5LckpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3VmWU9Sb0Z5bmdWcnpNVkY=&amp;uid=WEEvREcwSlJHSldSdmVqM1BLVW9SbUxZeG1WK0ZTTVUyWWFNVTVibm1CUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!\">[1]王胤. 应用于三维成像飞行时间法建模及其误差分析[D].湘潭大学,2017.</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/5dqn_vrW86plQRPEGY12Dw\">[2]一文详解深度相机之TOF成像,新机器视觉</a></p>\n<p><a href=\"https://ieeexplore.ieee.org/document/8448475\">[3]Y. Fang, X. Wang, Y. Su, K. Zhang and B. Su, “The Accuracy Analysis of TOF Camera Based on ANOVA,” 2018 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW), 2018, pp. 1-2, doi: 10.1109/ICCE-China.2018.8448475.</a></p>\n","categories":["FPGA"],"tags":["图像处理"]},{"title":"卫星姿态估计综述","url":"/2021/01/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1review/","content":"<h1 id=\"A-Review-on-Satellite-6D-Pose-Estimation-with-Deep-Learning\"><a href=\"#A-Review-on-Satellite-6D-Pose-Estimation-with-Deep-Learning\" class=\"headerlink\" title=\"A Review on Satellite 6D Pose Estimation with Deep Learning\"></a>A Review on Satellite 6D Pose Estimation with Deep Learning</h1><h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>准确可靠地获取目标卫星相对于服务航天器的6D位姿是执行在轨飞行任务的关键。然而，很多目标卫星（包括报废的卫星和碎片）是非合作的，并不能主动提供其状态信息或者特殊标记来协助服务航天器，即使其状态信息可用，也无法确保该数据的准确性。由于深度学习的发展，基于单目视觉的6D姿态估计方法受到重视，因此，本文分析了近几年来基于深度学习的6D姿态估计在卫星等太空中的目标上的应用，并比较了不同方法的性能，揭示了哪些因素使得卫星姿态估计具有挑战性。</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>近年来，为了解决地球轨道拥挤问题并延长地球静止卫星的寿命，诸如碎片清除、在轨维修和对接操作等在轨飞行任务概念越来越受到学术界和工业界的关注。执行这些任务的关键是要能准确可靠地获取目标卫星相对于服务航天器的位置和姿态（即6D位姿）。然而，太空中很多目标卫星（包括报废的卫星和碎片）是非合作的，并不能主动提供其状态信息或者特殊标记来协助服务航天器，即使其状态信息可用，也无法确保该数据的准确性。因此，建立一个准确而稳定的卫星姿态估计系统尤为重要，该系统需要能够在无人工干预的情况下自动检测出目标卫星的位姿，从而实时监测卫星状态，以便进行碎片清除、在轨维修和对接等操作。</p>\n<p>出于对航天器处理器性质和功率的考虑，使用视觉传感器（如摄像机）进行卫星姿态估计的优先级将大于激光雷达、雷达等其他有源传感器，因为视觉传感器的质量更小且功率要求更低。在小型航天器上，低功耗的单目相机将是实现卫星姿态估计的理想传感器。</p>\n<p>为了实现卫星姿态估计，必须利用快速而准确的计算机视觉算法，从一个或一组单目图像计算出目标的相对位置和姿态。早期的姿态估计技术主要集中在手工设计的特征匹配，这些方法的鲁棒性和泛化性较差。此外，太空的环境带来了更为严峻的挑战，例如太空中物体深度变化范围大、缺乏大气散射、信噪比低、照明条件恶劣以及动态的地球背景等。近年来，随着深度卷积神经网络的出现，计算机视觉算法取得了重大突破，与先前的技术相比，基于卷积神经网络(CNN)的深度学习方法取得了更好的效果。</p>\n<p>然而，基于CNN的深度学习方法有两个主要的缺点：一是深度学习依赖于大量带标签的数据集，二是其运算量较大。在各种地面应用中，有大量的公开数据集来用于姿态估计，进而训练出最先进的深度学习模型，但是在卫星的姿态估计领域，则缺少真实的带标签的公开数据集。此前大多数单目姿态估计的工作都是在合成图像上进行训练和评估，但在有限的合成图像上表现良好的模型不一定能有效地转移到真实环境中。同时，目前相关的研究对于在卫星所搭载硬件上运行的可行性的讨论较少，大多数CNN需要图形处理器(GPU)来加速运算。然而，由于辐射暴露的问题，GPU在太空领域的应用仍然是一个较新的领域。在许多卫星上，只搭载有低功耗的传统处理器，无法实时运行大型的、最先进的CNN模型。</p>\n<p>为了解决以上的问题，欧洲航天局的先进概念小组和斯坦福大学的空间会合实验室举办了卫星姿态估计挑战赛，并在2019年2月发布了第一个公开的可用于航天器姿态估计的机器学习数据集。比赛的结果显示，基于PnP姿态解算的方法更为精确，且单独的目标检测步骤对于提升姿态估计的精度有利。但是这次比赛的数据集主要是合成图像，测试结果也是以合成图像为依据。为了弥补真实和合成图像之间的差距，欧洲航天局的先进概念小组和斯坦福大学的空间会合实验室在2021年11月再次举办了卫星姿态估计挑战赛，同时发布了有更多真实图像的数据集。</p>\n<p>本文在第二节回顾了卫星姿态估计相关研究后，在第三节介绍了相关的数据集，最后在第四节对未来进行了展望。</p>\n<h2 id=\"METHODS-ON-SATELLITE-POSE-ESTIMATION\"><a href=\"#METHODS-ON-SATELLITE-POSE-ESTIMATION\" class=\"headerlink\" title=\"METHODS ON SATELLITE POSE ESTIMATION\"></a>METHODS ON SATELLITE POSE ESTIMATION</h2><p>本节我们首先讨论了卫星姿态估计所面临的挑战，并回顾了近三年来卫星姿态估计的主要方法，这些方法可以大致分为两类，即单阶段和两阶段方法。两阶段方法首先识别出目标卫星在图像中的位置，然后将图片裁剪并缩放至固定大小后进行姿态估计，而单阶段方法则不区分目标检测和姿态估计，使用端到端的网络来进行卫星姿态估计。</p>\n<h3 id=\"Difficulties\"><a href=\"#Difficulties\" class=\"headerlink\" title=\"Difficulties\"></a>Difficulties</h3><p>卫星姿态估计系统设计中遇到的主要困难包括：</p>\n<ul>\n<li>物体深度变化范围大：虽然6D位姿估计是CV中的热点领域，但是以往的研究并没有重点关注物体的尺度变化问题，大多集中于研究较近物体的姿态，而在太空中，由于缺乏大气散射，使得目标可以从很远的距离被看到，从而需要充分考虑物体的尺度变化情况。</li>\n<li>大气散射的缺乏：太空中缺乏大气散射，这会导致高对比度、欠曝光/过曝光、显著的镜面反射等问题，从而对单目相机所拍图片的质量产生较大影响。(可插图)</li>\n<li>数据集的缺乏：由于卫星的真实照片难以获取，目前可用的基准数据集大都采用电脑合成或者在实验室人工环境下使用卫星模型进行拍摄，没有充分强调缺乏大气散射的影响，并且大多描述的是距离非常近的目标</li>\n<li>实时性和轻量化要求高：在许多卫星上，只搭载有低功耗的传统处理器，无法实时运行大型的的CNN模型，需要设计轻量化的网络。</li>\n</ul>\n<h3 id=\"Two-stage-methods\"><a href=\"#Two-stage-methods\" class=\"headerlink\" title=\"Two-stage methods\"></a>Two-stage methods</h3><p>两阶段指的是将卫星目标的检测和姿态估计看成两个独立的步骤，先由目标检测网络检测出目标卫星在图像中的位置，在将目标卫星所在区域进行裁剪并缩放至统一大小，然后将缩放后的图片作为姿态估计模板的输入。通过独立的目标检测步骤，能够很好地适应目标在图像不同位置以及不同距离的情况，进而获得较高的精度。</p>\n<ol>\n<li>UniAdelaide：这个队伍获得了2019年卫星姿态估计比赛的第一名，该团队首先通过多视角三角测量重建卫星模型，并选取了卫星模型上的11个关键点。然后训练了一个CNN目标检测网络用于框选出目标卫星边界，以裁剪卫星所在区域。他们还使用了HRNet架构来对裁剪后的图像进行预定义关键点的回归，在得到关键点的2D-3D对应后，使用稳健的非线性优化来解算目标姿态。（这里可以插图）</li>\n<li>Real-time spacecraft pose estimation: 虽然UniAdelaide所设计的两阶段系统能够很准确的识别出卫星的姿态，但是其运行速度较慢，无法满足实时性的要求。为此，The University of Texas at Austin 的研究人员提出了一个轻量化的系统，在保持最先进精度的情况下，大大降低了计算的成本，其网络参数量降为了UniAdelaide的十分之一。与UniAdelaide类似，该系统是一个由目标检测CNN、关键点回归CNN和PNP解算器组成的。它还包括最后的误差预测步骤，该步骤识别不良姿势估计，并用“未检测”来代替它们。此外，整个体系结构使用一种新的合成数据生成方案进行训练，该方案生成具有真实空间图像中存在的许多降级的照片级真实感图像。</li>\n</ol>\n<h3 id=\"Single-stage-methods\"><a href=\"#Single-stage-methods\" class=\"headerlink\" title=\"Single-stage methods\"></a>Single-stage methods</h3><p>单阶段指的是将目标检测和关键点回归看成统一的步骤，用一个端到端的网络来输出预测的关键点位置，这样的联合训练减少了许多不必要的重复计算，使得网络结构更为简单和轻量化。不过也有一些单阶段网络跳过了关键点的回归，直接输出目标的位置和姿态，如UrsoNet。</p>\n<ol>\n<li>UrsoNet：</li>\n<li>EPFL_cvlab：该系统采用单阶段的端到端网络将目标检测和关键点回归看成统一的步骤，用一个端到端的网络来输出预测的关键点位置，鲁棒性更强；采用编码器-解码器体系结构，并在解码器的每一级，建立3D到2D的对应关系；使用了YOLOv3中的Darknet-53作为主干，并采用了特征金字塔网络（FPN）的结构，但与大多数依赖此类网络的方法不同，本系统将每个训练实例分配到多个金字塔级别，以促进多尺度信息的联合使用。最后，使用基于RANSAC的PnP策略从这些对应集合中推断出一个可靠的位姿。同时在训练时，为了解决2D重投影误差方法的缺陷，采用基于3D位置而不是2D投影来最小化训练损失</li>\n</ol>\n<h3 id=\"（方法比较？）\"><a href=\"#（方法比较？）\" class=\"headerlink\" title=\"（方法比较？）\"></a>（方法比较？）</h3><h2 id=\"DATASET\"><a href=\"#DATASET\" class=\"headerlink\" title=\"DATASET\"></a>DATASET</h2><p>在本节中，我们列举了卫星姿态估计领域目前公开的且质量较高的数据集，包括SPEED,URSO,SPEED+和SwissCube。</p>\n<ol>\n<li><p>SPEED:</p>\n</li>\n<li><p>SPEED+:与来自旧SPEED数据集的真实图像相比，姿势的数量、姿势标签的保真度和光线条件的变化都得到了显著改善。（SPEED数据集和SPEED+的具体结构如图所示）</p>\n</li>\n<li>URSO:URSO利用虚幻引擎4(UE4)的功能来渲染逼真的图像，例如基于物理的材质、光晕和镜头光斑。环境中的照明由平行光和聚光灯组成，分别模拟太阳光和地球反照率。为了模拟太阳，我们使用了带有UE4光晕散射卷积的发射材质。地球被模拟成一个高度多边形的球体，纹理上有来自蓝色大理石下一代系列的21600,10800张地球和云图。这将进一步遮罩以从海洋表面获得镜面反射。此外，还使用第三方资源对大气散射进行建模。<br> 我们的场景包括联盟号和龙号航天器模型，几何图形从3D模型存储库导入。</li>\n<li>SwissCube</li>\n</ol>\n<p>该团队利用虚幻的引擎4（UE4）功能来渲染逼真的图像，例如基于物理的材料，水华和镜头耀斑。他们环境中的照明只是由定向光和聚光灯组成，分别模拟太阳光和地球反照率。环境照明被禁用，为了模拟太阳，他们使用了具有UE4开花散射卷积的发射材料体。地球被建模为高多边形球体，纹理为21600×10800地球。这进一步被掩盖以获得来自海面的镜面反射。另外，第三方资产用于模拟大气散射。他们的场景包括Soyuz和Dragon航天器模型，其几何结构从3D模型库导入</p>\n<h2 id=\"CONCLUSION-AND-FUTURE-WORK\"><a href=\"#CONCLUSION-AND-FUTURE-WORK\" class=\"headerlink\" title=\"CONCLUSION AND FUTURE WORK\"></a>CONCLUSION AND FUTURE WORK</h2><p>在本文中，我们对于卫星姿态估计的背景做了详细的介绍，指出了卫星姿态估计面临的问题和挑战，整理了最新的卫星姿态估计方法，并从中选出了4个最具代表性的方法，将它们分为了两阶段和单阶段两种类型，同时回顾了每种方法的特点。然后对于目前卫星姿态估计领域公开且高质量的数据集做出了简要的介绍，这将为未来的研究者提供很大的便利。</p>\n<p>总体而言，各种DNN架构能够很好地估计不合作的航天器的姿态，前提是已知目标的3D模型或预定的3D关键点坐标。然而，同样的网络结构在真实图像上的性能相对较差，因为真实图像与用于训练DNN的合成图像具有不同的统计分布。由于在未来的空间任务中部署的任何DNN无疑都将利用合成图像作为主要的训练来源，因此未来必须找到合适的方法来缩小合成图像与真实图像之间的差距。</p>\n<h2 id=\"Acknowledgment\"><a href=\"#Acknowledgment\" class=\"headerlink\" title=\"Acknowledgment\"></a>Acknowledgment</h2><p>作者们想要感谢钱久超老师的指导以及潘佳萌学长的帮助</p>\n","categories":["位姿估计"],"tags":["Satellite","Pose Estimation"]},{"title":"特征匹配","url":"/2022/04/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/","content":"<h1 id=\"相关链接\"><a href=\"#相关链接\" class=\"headerlink\" title=\"相关链接\"></a>相关链接</h1><p><a href=\"https://blog.csdn.net/weixin_43605641/article/details/122572446\">(34条消息) 特征点检测与匹配相关论文梳理（持续更新）<em>秋山丶雪绪的博客-CSDN博客</em>特征匹配论文</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_43605641\">(34条消息) 秋山丶雪绪的博客_CSDN博客-特征点检测与匹配,计算机系统结构,Python代码记录领域博主</a></p>\n<p><a href=\"https://blog.csdn.net/qq_42708183/article/details/109133806\">2020最强匹配综述—《Image Matching from Handcrafted to Deep Features: A Survey》阅读总结_喵呜喵喵喵的博客-CSDN博客</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s?__biz=MzI3NDIyMjcyNg==&amp;mid=2652161460&amp;idx=1&amp;sn=4d2ee4e6973593e670ce460cc8eb590b&amp;chksm=f0f73e5dc780b74b1a4b68d6909708c261a539383720e245c2133f48753a8e871f98fd221912&amp;scene=21#wechat_redirect\">CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！ (qq.com)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/387962855\">CVPR 2021 | 图像匹配挑战赛总结 (SuperPoint + SuperGlue 缝缝补补还能再战一年) - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://aijishu.com/a/1060000000224837\">CVPR 2021 Image Matching 挑战赛双冠算法：揭秘AR导航背后的技术 - 极术社区 - 连接开发者与智能计算生态 (aijishu.com)</a></p>\n<p><a href=\"https://github.com/ducha-aiki/pydegensac\">ducha-aiki/pydegensac: Advanced RANSAC (DEGENSAC) with bells and whistles for H and F estimation (github.com)</a>（ransac的改进方案，效果更好）</p>\n<p><a href=\"https://github.com/hpatches/hpatches-dataset\">hpatches/hpatches-dataset： HPatches： Homography-patchs dataset. (github.com)</a></p>\n<p><a href=\"https://blog.csdn.net/u010440456/article/details/81483145\">非常详细的sift算法原理解析_可时间倒数了的博客-CSDN博客_sift</a></p>\n<p><a href=\"https://blog.csdn.net/zddblog/article/details/7521424?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2-7521424-blog-81483145.pc_relevant_multi_platform_whitelistv1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2-7521424-blog-81483145.pc_relevant_multi_platform_whitelistv1&amp;utm_relevant_index=3\">SIFT算法详解_zddhub的博客-CSDN博客_sift</a></p>\n<p><a href=\"https://wenku.baidu.com/view/1e08afaea3116c175f0e7cd184254b35eefd1aa4.html\">关于softmax、argmax、softargmax - 百度文库 (baidu.com)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/539740663\">CVPR 2022 图像匹配挑战赛回顾 - 知乎 (zhihu.com)</a></p>\n<h1 id=\"线特征匹配\"><a href=\"#线特征匹配\" class=\"headerlink\" title=\"线特征匹配\"></a>线特征匹配</h1><p>LBD，LSD 算法</p>\n<p>An efficient and robust line segment matching approach based on LBD descriptor and pairwise geometric consistency</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h2><p><a href=\"https://github.com/lh9171338/Line-Segment-Detection-Papers\">lh9171338/Line-Segment-Detection-Papers: Line segment detection papers (github.com)</a></p>\n<h1 id=\"双目深度图（立体匹配）\"><a href=\"#双目深度图（立体匹配）\" class=\"headerlink\" title=\"双目深度图（立体匹配）\"></a>双目深度图（立体匹配）</h1><p>参考代码 <strong><a href=\"https://github.com/aliyasineser/stereoDepth\">stereoDepth</a></strong>  在该代码的基础上加一个mask，输出mask内像素的平均距离</p>\n<p>这个是只算了圆心点的距离，不过可以借鉴里面的mask<a href=\"https://github.com/niconielsen32/ComputerVision/blob/master/StereoVision/Python/main.py\">ComputerVision/main.py at master · niconielsen32/ComputerVision (github.com)</a></p>\n<h3 id=\"立体标定与立体校正\"><a href=\"#立体标定与立体校正\" class=\"headerlink\" title=\"立体标定与立体校正\"></a>立体标定与立体校正</h3><p><a href=\"https://blog.csdn.net/u011574296/article/details/73826420\">(30条消息) 【立体视觉】双目立体标定与立体校正<em>ZealCV的博客-CSDN博客</em>立体校正</a></p>\n<p><a href=\"https://aijishu.com/a/1060000000139727\">来聊聊双目视觉的基础知识（视察深度、标定、立体匹配） - 极术社区 - 连接开发者与智能计算生态 (aijishu.com)</a></p>\n<h1 id=\"Learning-Feature-Descriptors-using-Camera-Pose-Supervision\"><a href=\"#Learning-Feature-Descriptors-using-Camera-Pose-Supervision\" class=\"headerlink\" title=\"Learning Feature Descriptors using Camera Pose Supervision\"></a>Learning Feature Descriptors using Camera Pose Supervision</h1><p><img src=\"/2022/04/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/image-20220530163756268.png\" alt=\"image-20220530163756268\"></p>\n<p>caps预训练模型在mepdepth测试集测试结果</p>\n<p><img src=\"/2022/04/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/image-20220531171555853.png\" alt=\"image-20220531171555853\"></p>\n<p>训练测试</p>\n<pre class=\" language-lang-python\"><code class=\"language-lang-python\">/home/young/anaconda3/envs/sgp/bin/python /home/young/code/SGP/code/perception2d/sgp.py --config /home/young/code/SGP/code/perception2d/config_sgp_sample.yml\nlabel root caps_pseudo_label/bs will be overwritten to enter teaching mode\nDataset size: 3337\n0it [00:00, ?it/s][ WARN:0@1.508] global /io/opencv_contrib/modules/xfeatures2d/misc/python/shadow_sift.hpp (13) SIFT_create DEPRECATED: cv.xfeatures2d.SIFT_create() is deprecated due SIFT tranfer to the main repository. https://github.com/opencv/opencv/issues/16736\n3337it [10:10,  5.47it/s]\nlabel root caps_pseudo_label/bs exists, entering learning mode.\ntensorboard log files are stored in caps_logs/bs/caps_sgp\nNo ckpts found, training from scratch...\n/home/young/anaconda3/envs/sgp/lib/python3.8/site-packages/torch/nn/functional.py:3981: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n  warnings.warn(\n/home/young/anaconda3/envs/sgp/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /tmp/pip-req-build-ex__3qls/aten/src/ATen/native/BinaryOps.cpp:467.)\n  return torch.floor_divide(self, other)\ncaps_sgp | Step: 1, Loss: 0.16486\ncaps_sgp | Step: 2, Loss: 0.08825\ncaps_sgp | Step: 3, Loss: 0.19555\ncaps_sgp | Step: 4, Loss: 0.10034\ncaps_sgp | Step: 5, Loss: 0.08762\nsaving ckpts caps_outputs/bs/caps_sgp/000005.pth...\nlabel root caps_pseudo_label/00 does not exist, entering teaching mode.\n0it [00:00, ?it/s]Reloading from caps_outputs/bs/caps_sgp/000005.pth, starting at step=5\n3337it [16:32,  3.36it/s]\nlabel root caps_pseudo_label/00 exists, entering learning mode.\ntensorboard log files are stored in caps_logs/00/caps_sgp\nNo ckpts found, training from scratch...\ncaps_sgp | Step: 1, Loss: 0.16329\ncaps_sgp | Step: 2, Loss: 0.13774\ncaps_sgp | Step: 3, Loss: 0.12036\ncaps_sgp | Step: 4, Loss: 0.11186\ncaps_sgp | Step: 5, Loss: 0.11013\nsaving ckpts caps_outputs/00/caps_sgp/000005.pth...\nlabel root caps_pseudo_label/01 does not exist, entering teaching mode.\nReloading from caps_outputs/00/caps_sgp/000005.pth, starting at step=5\n3337it [16:32,  3.36it/s]\nlabel root caps_pseudo_label/01 exists, entering learning mode.\ntensorboard log files are stored in caps_logs/01/caps_sgp\nNo ckpts found, training from scratch...\ncaps_sgp | Step: 1, Loss: 0.21352\ncaps_sgp | Step: 2, Loss: 0.08665\ncaps_sgp | Step: 3, Loss: 0.08282\ncaps_sgp | Step: 4, Loss: 0.12540\ncaps_sgp | Step: 5, Loss: 0.09589\nsaving ckpts caps_outputs/01/caps_sgp/000005.pth...\n\nProcess finished with exit code 0\n</code></pre>\n<h1 id=\"论文中提到的问题\"><a href=\"#论文中提到的问题\" class=\"headerlink\" title=\"论文中提到的问题\"></a>论文中提到的问题</h1><ul>\n<li>特征检测器比描述符更难学习，因为对于描述符的定义更为明确，而人们通常不清楚哪些点是有趣的，难以进行人为标记</li>\n<li>特征点的性质：可区分和可重复<ul>\n<li>1.能够发现图像中局部的具备区分度的信息；同一性约束（identity constraint）<br>  2.当场景经过多种变换后，仍能够重复检测出一致的特征；协变约束（covariant constraint ）</li>\n</ul>\n</li>\n<li>虽然机器学习显然有助于照明不变性，但对于视点不变性，传统方法仍然具有惊人的竞争力。</li>\n<li>监督学习不能很好地作用于关键点的检测，因为在这种情况下，监督学习的本质可以归结为复制现有的检测器，而不是发现更好、更容易的关键点。更有效的方向是自监督/无监督的学习</li>\n<li>将网络的某部分用传统方法代替，能极大减少网络的参数量</li>\n<li>数据集的质量&gt;数量？</li>\n</ul>\n<h2 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h2><p><a href=\"https://www.aiuai.cn/aifarm1697.html\">度量学习 - 损失函数汇总[译] - AI备忘录 (aiuai.cn)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/80761087?from_voters_page=true\">深度学习loss清单-未完待续 - 知乎 (zhihu.com)</a></p>\n<h1 id=\"CVPR-2022-Image-matching-challenge\"><a href=\"#CVPR-2022-Image-matching-challenge\" class=\"headerlink\" title=\"CVPR 2022 Image matching challenge\"></a>CVPR 2022 Image matching challenge</h1><p>有用的点：</p>\n<ul>\n<li>对匹配聚类除去外点后框选bbox，再进行匹配</li>\n</ul>\n<p>无用的点：</p>\n<ul>\n<li>使用分割过滤移动对象</li>\n<li></li>\n</ul>\n<h1 id=\"The-MegaDepth-Dataset\"><a href=\"#The-MegaDepth-Dataset\" class=\"headerlink\" title=\"The MegaDepth Dataset\"></a>The MegaDepth Dataset</h1><ul>\n<li><p>colmap的sfm和mvs重建，得到原始深度图</p>\n</li>\n<li><p>采用更谨慎的mvs方法，更倾向于少而准确的真实数据，未计算出真实值的点输出为0</p>\n</li>\n<li>采用语义分割来增强和过滤深度图，过滤了天空部分的深度，以及移动物体的深度。</li>\n</ul>\n","categories":["位姿估计"]},{"title":"考研感悟","url":"/2021/03/02/%E9%9A%8F%E7%AC%94/%E8%80%83%E7%A0%94%E6%84%9F%E6%82%9F/","content":"<h4 id=\"2021-3-2\"><a href=\"#2021-3-2\" class=\"headerlink\" title=\"2021/3/2\"></a>2021/3/2</h4><p>考研分数出来了，考的很糟糕，政治69，英语一71，数学一102，专业课127，总分369。</p>\n<p>按照目前群里的统计，400分以上的已经远超往年，我很大概率不会过线了。我爸想让我调剂，我有点想直接去找工作了。</p>\n<a id=\"more\"></a>\n<p>一时间不知道如何是好，辛辛苦苦准备了差不多一年，最后的结果就是这样吗？本来估分起码有个三百八九十分，结果谁知数学给了我当头一棒，这一棒瞬间把我打成了一个没有梦想的咸鱼。或许自己真的是数学天赋不够吧，毕竟高考也是数学考砸了。付出了那么多的时间，反而还没别人考的好。或许我真该反思一下是不是自己做了太多的无用功。</p>\n<p>或许一开始就不应该选择考研这条路。当初凭着一腔热血以及对步入社会的恐惧，选择了考研，硬撑着颈椎病走到了现在，看着别的同学能坐那不动学一天，好生羡慕，而我顶多坐一个小时就必须出去活动一下，不然疼的受不了。跟别人说我脖子疼，大多也是笑笑，只有得过颈椎病的人才能体会我的难处。</p>\n<p>当然，这也并不是说为成绩考差找借口，我承认数学没考好是我自己的原因，我也确确实实尽到了我能做的最大努力。只是确实心有不甘，明明是保研边缘的成绩，最后连研都没考上，做了一年的无用题，真的是对不起我自己，也对不起大家对我的厚望。有时候，我常常想，要是我一直成绩就很差那该多好啊，那样大家就不会对我期望那么高了，我找个工资一般的工作也不会有人说什么了，我是不是就可以做我想做的事情了呢？（不过，我到底想做什么呢，哈哈我自己也不知道，我可能只是想混吧）</p>\n<p>哎，明明感觉心里有很多东西想说，可是真要我来写，又不知道写什么了，要是研真没考上，找几个兄弟去喝酒吧，或许这样我能把堵在心中的话说出来吧。记得以前我说过，要是什么时候我开始喝酒了，那就证明我向这个世界妥协了。我真的有点累了，虽然确实很想为这个世界做点什么，做点什么能让后人记得的好事。小时候以为自己以后能够很厉害，自己的qq号昵称都是 I am the one and only (我独一无二) 。而现在才发现，自己不过是一个毫不起眼的小角色罢了，特别是在东九下课的时候，上千人蜂拥而出，在这人潮中，所有人都和我差不多，仅仅在这些人中，我就已经被淹没的无影无踪，只能随着人潮慢慢前进。</p>\n<p>​    <img src=\"/2021/03/02/%E9%9A%8F%E7%AC%94/%E8%80%83%E7%A0%94%E6%84%9F%E6%82%9F/image-20210302221734426.png\" alt=\"image-20210302221734426\"></p>\n<p>今天在一本书中看到上面一段话，有人又成功考上了研，又拿了几个好offer，又谈了恋爱，而我一无所有。</p>\n<p>时候不早了，教室只有我一个人了，下次再写吧。</p>\n<p><img src=\"/2021/03/02/%E9%9A%8F%E7%AC%94/%E8%80%83%E7%A0%94%E6%84%9F%E6%82%9F/image-20210302222157963.png\" alt=\"image-20210302222157963\"></p>\n<h4 id=\"2021-3-5\"><a href=\"#2021-3-5\" class=\"headerlink\" title=\"2021/3/5\"></a>2021/3/5</h4><p>这两天感觉好了很多，决定一条路走到黑了，不管找工作的事情了，这让我轻松许多。在小木虫上发布了我的调剂信息后，一个中科院近物所的老师加了我，很希望我去他们那。同时曾老师也帮我找了一些调剂信息，虽然不太行，但还是投了一个西电的。总之是有保底的了，那就加油冲吧！</p>\n<h4 id=\"2021-3-12\"><a href=\"#2021-3-12\" class=\"headerlink\" title=\"2021/3/12\"></a>2021/3/12</h4><p>今年校线出了，还是330分，不过校线也没啥太大的意义。今天搞完了毕设的开题答辩，可以短期内不用管毕设的事情了，接下来核心任务还是去认真准备复试，如果上天真的给了我这个机会，而我却没有好好珍惜的话，那才是真的难受。既然一切都还有希望，那么在最后揭晓的那一刻来临之前，都要努力去拼一拼啊！还有那么多人为我加油鼓气呢！（在这里真心的感谢为我鼓起和提供帮助的人）我怎么能自己先放弃了呢。明天得认真看笔试了。</p>\n<p>关于找工作的事情，可以先看看嵌入式开发的方向，毕竟我本科基本都是在做嵌入式，对于我来说，这个方向所需的准备时间也少一些，没进复试再找工作！现在要分清孰轻孰重，学长说让我好好准备，先别管毕设了，复试逆袭的可能性还很大。</p>\n<p>同时，每天晚上开始梳理以前电赛或者课设所做的东西，并整理成md文档发到博客上吧，每天一个项目！</p>\n<p>噗，现在看上面3月2日的记录，感觉自己好憨啊，没事儿就别瞎想，做好当下的事情！累了就去运动！加油！</p>\n<h4 id=\"2021-3-13\"><a href=\"#2021-3-13\" class=\"headerlink\" title=\"2021/3/13\"></a>2021/3/13</h4><p>今天中科院的老师给我发消息，说是下周末他们搞线下复试，问我去不去。我想，就算上交没过线，也不会去他们那里吧，去了的话，我或许会后悔很久很久吧。终究还是想去大城市闯一闯啊。我这人有个毛病，似乎很难多线程，如果有一件什么事情等着我去完成，那我就中途不想管其他事情了。</p>\n<p>希望能进复试吧！我相信天无绝人之路，无论结果如何，总会走下去的。听习大大的话，不忘初心啊！</p>\n<h4 id=\"2021-3-29\"><a href=\"#2021-3-29\" class=\"headerlink\" title=\"2021/3/29\"></a>2021/3/29</h4><p>距离复试完有好几天了，这几天感觉发生了好多好多事啊！</p>\n<p>首先，很幸运，我进入了复试，今年上交分数线<strong>356分</strong>，没想到这么多高分的情况下，分数线只比去年高了一分，真可以称的上是奇迹再现了！感谢之前在我感到最黑暗的时候鼓励了我的好朋友们！果然，灯不会在，任何时候为我开，是好是坏，该不该，还没来的不想猜~~ 确实很多时候，做好眼前的事就好，不用去猜那些还没有发生的事情。毕竟，谁也说不清楚未来会怎样。</p>\n<p>确定进入复试后，自然是疯狂的学，因为复试线发布是15号，线下复试，21号笔试，22号面试。于是我20号便和我爸一起去了上海。终于是成功来到我心心念念了一年的真正的大城市，由于直接坐地铁到了闵行，当我真正踏上这片土地时，也并未觉得有什么不同，心想，上海就这？哈哈。</p>\n<p>当21号来到上交时，看到曾经在我电脑桌面的校门真正的出现在我的眼前，顿时觉得好像有那么一点点不真实。有一说一，上交的建筑风格确实完爆我科。第一天是下午笔试，往年都是一个小时，今年改成了俩小时，不过还好，并没有更改题型，还是50道选择题，那这就没啥好说的了，毕竟选择题怎么都可以做，不会考很深入。笔试考完确实轻松了许多，又赶紧回宾馆准备面试。</p>\n<p>面试的简历是非常重要的，好在我很早就在开始写我的简历，而且把简历里面可能问到的每个点都思考了一遍，并写在纸上。我被分在了第二天上午，一大早7点多我就到面试地点了，去的时候教室还一个人都没有，谁知面试顺序是按成绩排的，一直等到中午11点半才到我。</p>\n<hr>\n<p>（下面的面试过程考虑后面出一个对话版，现在可能比较乱，凑合看看我刺激的面试历程吧！</p>\n<p>​                                                                                                    ——<strong>论我是如何在面试时就斩获4位老师青睐的？</strong>）</p>\n<p>一进门，一共5位面试老师（用ABCDE来代称吧，有些问题记不清是哪个老师问的了，就不明写了）和一名学生（负责收我们的资料），老师们正在互相交流，好像很随意，也没有全把目光盯到我身上。我将提前准备好的简历和成绩单发给老师们，然后站到讲台上（不同教室不一样，有的教室学生坐着），鞠躬说道，老师好！然后<strong>A老师</strong>开始用英文提问，第一个问题是让我介绍我的毕设。这个我有所准备，所以答上来了，不过由于很紧张，也是吞吞吐吐的。接下来，谁知他让我介绍我毕设里面的一个设备（Atlas 200 DK），啊这，这谁答的上来啊，问这么细，我知道用中文怎么说，但是让我说英文，我无能为力，憋了半天说了个AI compute machine。A老师步步紧逼，继续问我具体有什么功能，我也记不清当时说了啥，反正嗯嗯啊啊了很久，A老师也没继续问了，英语问答环节到此结束。（其实后来想想，这个环节其实也没多重要，我感觉老师就是想留点时间看看我的简历，我吞吞吐吐的时候老师也并没在意，大多都是盯着我的简历在看。）</p>\n<p>接着<strong>A老师</strong>就用中文问起了我的电赛经历，让我介绍一下电赛的项目，这可是我花大力气准备了的，当然丝毫不慌，把功能讲了一遍。老师们好像对这很感兴趣，又继续问我原理，（只要是自己真正做了的项目，遇到懂行的老师是真的好，我就是怕老师不问，只要他问，我都能答上来），这我自然是把原理讲了一通，老师们十分满意，脸上都露出了笑容，我也进入了闲聊模式，放松下来了。然后老师又让我介绍一下软件课设，我也就先介绍了一下功能，然后老师问我为什么数据库要选择mongodb，我很实在的说这是我队友负责的部分，我只负责前端的工作，老师也没有继续深究。</p>\n<p>后面老师又看到我下面写的大一大二加入学校的电工基地，做的电赛的培训项目，（我写了好多项目）老师随便抽了一个红外线信号传输的项目，问我怎么实现的，（哈哈，幸好我把每个项目都准备了一遍，而且这些都是我真正做过的项目，自然是手到擒来，还是那句话，就怕他不问），我回答原理和结构后，老师们也很高兴，接着跟我讨论里面为什么要用到我说的方法，我依然答上来了。项目的问题基本就到此结束，后面就真的是纯闲聊了。</p>\n<p>A老师问我以后想做什么方向，我说只要不是纯硬件都可以，毕竟硬件有时候太玄了！老师笑着总结我的话：”那意思就是，你大一大二搞了两年硬件搞伤心了，想做软件了，可以这么理解吧？那我看看你软件的课程学的怎么样，哦！python成绩挺高的嘛！“我说python是最后课设用pygame做了个植物大战僵尸的游戏。C老师问，那你觉得verilog是属于硬件还是软件呢？我说，应该也算硬件吧，毕竟它叫硬件编程语言嘛。</p>\n<p>然后E老师看到我参加了很多课外活动，问我是怎么分配时间的，我说周六周日都会去实验室学电赛的培训，时间安排挺紧的。B老师看到我的绩点，很疑惑的问我绩点这么高怎么没保研？而且成绩排年级前27%。我笑着说大概要前25%才能保研（上交保研率更高，我这个排名在上交就保研了）。老师也笑着说那刚好差一点啊，很可惜。又看到我初试成绩不高，问我排多少名，我说排114，（一共招120个），老师们又笑着说那你这和你保研一样都是边缘啊！说我就是数学考差了点。我也答道确实如此，数学成绩出乎了我的预料。又问我有没有联系导师，我说联系了一个，但是那个老师说不确定有没有名额。</p>\n<p>基本就是这些，老师看了下时间说差不多了，我道谢后下去，<strong>B老师笑着对我说祝我好运</strong>。然后老师们都把简历递给了我，但是到<strong>C老师的时候，他说要把我的简历留一份</strong>在这儿，那我自然是同意，旁边的<strong>B老师</strong>对他说，学校规定是不能留学生简历的！几个老师笑了笑，最后还是留下了简历。我走出教室门的时候，<strong>A老师又跟着我出来，跟我说表现不错，让我记一下他电话</strong>，等会跟我打过来。（哈哈，内心极其激动。）</p>\n<p>谁知这还没完，本来面试完就差不多12点多了，我出去和我爸（学校不让家长进去，辛苦我爸一直在学校外等我了）找了家餐馆，刚准备开吃，一个电话打过来，就是<strong>A老师</strong>，问我现在在哪，想跟我谈谈，我说正在跟我爸吃饭，然后他说他1点半有个会，让我3点去他办公室找他交流交流。我接完电话，没吃几口菜，又来一个电话（<strong>B老师</strong>），说：“你还记不记得我，我是那个跟你说了祝你好运的老师，想邀请你加入我们团队……”。我加了他微信，他给我发了一个链接让我报名参加他们晚上的所面（可是我因为复试之前联系的老师（叫<strong>0老师</strong>吧）的所面也是在这个晚上，我看在前面老师跟我打电话聊了很久的份上，先去了他那，这个后面再说）。我再次接完电话，又回去没吃几口饭，一个电话再次响起（这次是<strong>C老师</strong>），我一样接完电话加了微信。最后，总算是清净了一会儿，安安心心地吃完了饭。</p>\n<p>下午和A老师聊了一个多小时，主要是他在讲，跟我讲他们实验室的研究方向，以及回答我的一些小问题，总之聊的还是很愉快的。聊完后我走到电院草坪上，静静地躺着，终于可以休息一小会儿了！</p>\n<p>紧接着，晚上又去参加<strong>0老师</strong>的所面，不过他居然不在，是其他老师面的我，这个所的老师似乎对我的项目并不感兴趣，甚至有一个女老师居然指着我的电赛项目说：”我看你这也没做过什么实际的东西啊，也就是买几个模块来拼一拼而已“（我TM瞬间不想聊下去了，不仅不懂，还在这随便诋毁别人的项目，估计这老师连电赛是啥都不知道吧）。总之，我对这个研究所的印象极其差，非常后悔来这里面试。也因为来了这面试，我错过了B老师的所面，后面问他时，他说名额已经确定，说我怎么不早点跟他说，感到非常可惜。</p>\n<p>说实话，这个所面对我的打击很大，有点不想再面试了。不过，也让我明白，找一个懂你的老师是多么重要，我很幸运，复试面试我的5位老师都很好，都很懂我。（我就跟A老师发消息说想去他那，结果谁知，他又说还会安排明天下午让我我见他手下两个小老板）还是不能自己放弃机会啊！最后还是决定去下一个所试一试。（这个研究所是我随便报的，去聊的也很随意，没啥意思，就不多说）。22号的面试就正是结束了，也大概9点多了。</p>\n<p>第二天上午好好睡了觉（其实也没睡太着，多在床上躺了会儿而已），结果刚洗漱完，又接到一个电话，也是我之前随便投的一个研究所，是一个女老师（叫<strong>F老师</strong>吧）打来的电话，问我还记不记得他们所，问我还在不在上海，在的话下午想让我到他们那聊聊，我说我下午约了2点面试。然后她说，你肯定不会聊一下午吧，你聊完了跟我发消息再过来。我看老师这般邀请，自然不能拒绝。</p>\n<p>下午去见了A老师手下的两个小老板，有一个我很不喜欢，感觉很严肃，而且说还得再测我一轮，给我一个论文，让我一周内复现。我心想这也太搞了吧，面了这么多次还要测试，感觉不信任我，虽然我很喜欢A老师（他是大老板，不会直接带学生），但是不喜欢这个小老板，还是不太想去他们这了。</p>\n<p>跟他们聊完后，又发现B老师跟我打电话，说学校多给了他一个名额，问我想不想去，然后让我加他手下一个博士生，跟博士生用微信聊了会儿，说要跟我保持联系。此时我本来想的是，已经有这么多老师对我感兴趣了，要不不去F老师那面试了吧，可到了差不多4点时，F老师又跟我发微信问我情况，让我别急，他们等我去。我心想，老师这么热情，还是去一下吧，不能自己放弃机会啊。</p>\n<p>去到F老师那，老师让我坐着等会儿，过一会儿来了两个男老师，<strong>没想到其中一个就是当初面试我的D老师</strong>。（其实我开始以为会是很多同学都来这排队复试，没想到就我一个）这两位老师基本没问我什么技术问题，就是闲聊，说很看好我，还问我喜欢唱谁的歌，室友情况怎么样等等问题（感觉好像是在做性格测试）。总之，这两个老师是我聊的最开心的。</p>\n<p>至此，全部面试结束。我和我爸晚上去外滩看了看，然后第二天坐飞机回家了。</p>\n<p>回到家，下午我躺在床上补觉，醒来的时候，发现又有电话没接，群里也炸开了锅。原来是复试没通过的收到了死亡短信，我看了看没收到，应该是过了。电话是F老师打来的，还是问我想不想去他们那儿。最终经过我深思熟虑，<strong>选择了F老师的研究所。</strong></p>\n<hr>\n<p>这么多场面试下来，也算是涨了很多经验。也让我深刻地认识到，<strong>曾经的努力都没有白费</strong>（大学没白过就足够了）！这波不亏！以后也要继续努力！</p>\n<hr>\n<p>哈哈，现在看当最开始写的东西，好丧啊，好傻&gt;.&lt;  还是乐观点好啊！</p>\n","categories":["随笔"],"tags":["考研"]},{"title":"卫星姿态估计","url":"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/","content":"<h2 id=\"Satellite-Pose-Estimation-Challenge-Dataset-Competition-Design-and-Results\"><a href=\"#Satellite-Pose-Estimation-Challenge-Dataset-Competition-Design-and-Results\" class=\"headerlink\" title=\"Satellite Pose Estimation Challenge: Dataset,Competition Design and Results\"></a>Satellite Pose Estimation Challenge: Dataset,Competition Design and Results</h2><p>背景：为了解决地球轨道拥堵问题和延长地球静止轨道卫星的寿命，清除碎片和在轨服务等任务概念越来越受到学术界和工业界的关注。执行这些任务的一个关键是目标航天器相对于服务航天器的位置和姿态(即姿态)的可用性。相对于其他方法，使用基于单目视觉的方法对质量和功率的要求很小，且系统结构更为简单。</p>\n<p>难点：</p>\n<ol>\n<li><p>用于航天器位姿估计的数据集是缺乏的。主要原因是很难获得数以千计的具有精确注释的姿态标签的目标航天器的星载图像。<br> 此外，由于缺乏通用数据集，无法系统地评估和比较不同位姿估计算法的性能。</p>\n</li>\n<li><p>目标距离和背景是主要的挑战</p>\n</li>\n</ol>\n<p>与直接位姿估计方法相比，基于透视n点(PNP)解算器的位姿估计方法具有更高的精确度。</p>\n<p>相关工作：</p>\n<p>数据集：</p>\n<ol>\n<li>SPEED：第一个公开可用的用于航天器姿态估计的机器学习数据集，最初于2019年2月发布。包括合成和真实数据集两部分（真实数据集是用相机拍的1：1模型）</li>\n</ol>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101091807417.png\" alt=\"image-20211101091807417\" style=\"zoom:50%;\"></p>\n<ol>\n<li><p>URSO：采用虚幻4引擎进行仿真的合成卫星图像数据集</p>\n</li>\n<li><p>SPEED+:</p>\n<p> <img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101111534454.png\" alt=\"image-20211101111534454\"></p>\n</li>\n<li><p>BOP数据集？</p>\n</li>\n</ol>\n<p>又出现了一个吊打其他所有的SPARK数据集</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220214152033019.png\" alt=\"image-20220214152033019\"></p>\n<p>比赛：Kelvins Pose Estimation Challenge(KPEC)   欧洲航天局和斯坦福大学合作组织的比赛</p>\n<p><a href=\"https://kelvins.esa.int/satellite-pose-estimation-challenge/\">Kelvins - Pose Estimation Challenge - Home (esa.int)</a></p>\n<p><a href=\"https://kelvins.esa.int/pose-estimation-2021/home/\">Kelvins - Pose Estimation 2021 - Home (esa.int)</a></p>\n<p>2021误差估计方法：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101113240159.png\" alt=\"image-20211101113240159\"></p>\n<p>凯尔文姿势估计比赛的入门工具包：</p>\n<p><a href=\"https://gitlab.com/EuropeanSpaceAgency/speed-utils\">EuropeanSpaceAgency / speed-utils · GitLab</a></p>\n<p><a href=\"https://github.com/janblumenkamp/esa-kelvin-pose-estimation\">janblumenkamp/esa-kelvin-pose-estimation (github.com)</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101164942148.png\" alt=\"image-20211101164942148\" style=\"zoom:70%;\"></p>\n<h2 id=\"Deep-Learning-for-Spacecraft-Pose-Estimation-from-Photorealistic-Rendering-（2020-ICRA）\"><a href=\"#Deep-Learning-for-Spacecraft-Pose-Estimation-from-Photorealistic-Rendering-（2020-ICRA）\" class=\"headerlink\" title=\"Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering （2020 ICRA）\"></a>Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering （2020 ICRA）</h2><p>此论文的最佳解决方案在ESA位姿挑战[5]的合成数据集上获得了<strong>第三名</strong>，在真实数据集上获得了第二名。（不依赖于pnp的最佳方案）</p>\n<p>该方案：基于resnet的架构直接回归3D位置，该团队使用基于高斯混合模型的软分类来估计角度</p>\n<p>前两名的方案：2D关键点回归；图像裁剪+缩放和鲁棒的PnP</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211022093553834.png\" alt=\"image-20211022093553834\"></p>\n<p>该网络采用带有预先训练的权重的ResNet架构作为网络主干（backbone），为了保持空间特征分辨率，去掉了原网络的最后一个全连接层和全局平均池化层，只在第二层留下了一个池化层。全局池化层被替换为了stride为2，的3<em>3卷积层（bottleneck layer），用来压缩CNN的特征。该网络的缺点是网络本身<em>*不能处理多个对象</em></em>。</p>\n<p><strong>3D位置(Location)估计</strong>是一个简单的含有两个全连接层的分支。</p>\n<p>不是最小化绝对欧几里得距离，而是最小化相对误差，即下式的第1项</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211104204920303.png\" alt=\"image-20211104204920303\" style=\"zoom:50%;\"></p>\n<p><strong>旋转角度估计</strong>包括两种不同的方法，硬分类和软分类，硬分类输出一个四元数；软分类分两部分：第一个部分输出一组四元数，第二部分输出对应每个四元数的概率</p>\n<p><strong>sim-to-real augmentation pipeline</strong> 将5张带标签的真实数据集进行扩充，得到了很好的效果</p>\n<p>此论文方法的精度依赖于大量的参数（500M），且与前两名的得分还有较大差距</p>\n<p>此论文的实验揭示了几个网络超参数选择和不同估计旋转方向方法的影响。结果表明，基于软分类的方向估计方法比直接回归方法具有更好的估计效果。</p>\n<p>存在的问题：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105130442853.png\" alt=\"image-20211105130442853\" style=\"zoom:40%;\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211111094705105.png\" alt=\"image-20211111094705105\"></p>\n<p>展望：一个很有前途的方向是使用递归神经网络和使用URSO生成的视频序列来解决跟踪问题。作为未来的工作，作者还计划将URSO扩展到SLAM，以定位几何形状未知的目标。</p>\n<p>其他：</p>\n<p>在benchmark for 6d object pose estimation这篇论文中提出<strong>目前基于点对特征的方法表现最好，优于模板匹配法、基于学习的方法和基于3D局部特征的方法</strong>   (20年基于学习的方法表现更好)</p>\n<p>不过通过避免直接估计姿势，而是使用CNN来回归预定义3D关键点的2D投影，并最终使用稳健的PNP解决方案(例如嵌入在RANSAC中)来估计姿势也可取得很好的效果</p>\n<h2 id=\"A-Review-on-Object-Pose-Recovery-from-3D-Bounding-Box-Detectors-to-Full-6D-Pose-Estimators\"><a href=\"#A-Review-on-Object-Pose-Recovery-from-3D-Bounding-Box-Detectors-to-Full-6D-Pose-Estimators\" class=\"headerlink\" title=\"A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators\"></a>A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators</h2><h2 id=\"Vision-based-attitude-estimation-for-spacecraft-docking-operation-through-deep-learning-algorithm\"><a href=\"#Vision-based-attitude-estimation-for-spacecraft-docking-operation-through-deep-learning-algorithm\" class=\"headerlink\" title=\"Vision-based attitude estimation for spacecraft docking operation through deep learning algorithm\"></a>Vision-based attitude estimation for spacecraft docking operation through deep learning algorithm</h2><p>这篇感觉就在urso那篇论文上改了下网络和损失函数，没太多创新点，但是对urso论文中一些概念解释还不错</p>\n<p>而且实验的结果也不好，对于旋转的角度误差大的离谱</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211104204858051.png\" alt=\"image-20211104204858051\" style=\"zoom:60%;\"></p>\n<h2 id=\"POSE-ESTIMATION-FOR-NON-COOPERATIVE-SPACECRAFT-RENDEZVOUS-USING-NEURAL-NETWORKS\"><a href=\"#POSE-ESTIMATION-FOR-NON-COOPERATIVE-SPACECRAFT-RENDEZVOUS-USING-NEURAL-NETWORKS\" class=\"headerlink\" title=\"POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS\"></a>POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS</h2><p>这篇论文提出SPN网络以及发布了<strong>SPEED</strong>数据集</p>\n<p><strong>SPN</strong>网络总体结构如下：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105162747501.png\" alt=\"image-20211105162747501\"></p>\n<p>下图解释了参考系、相对位置和相对姿态的定义：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105162830993.png\" alt=\"image-20211105162830993\" style=\"zoom:40%;\"></p>\n<p>下图说明了SPN方法中所使用的卷积神经网络：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105163010801.png\" alt=\"image-20211105163010801\"></p>\n<p>该网络的branch1用到了《Faster R-CNN: Towards Real-Time Object Detection with <strong>Region Proposal Networks</strong> 》，输出目标的矩形框</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105185913215.png\" alt=\"image-20211105185913215\" style=\"zoom:40%;\"></p>\n<p>k是指每个特征点对应的先验框anchors的数量</p>\n<p>在每个滑动窗口位置，我们同时预测 k 个候选矩形框，那么 reg layer 有 4k 个输出用于表示 k 个矩形框的坐标位置及尺寸大小信息。 cls layer 输出 2k 个概率用于表示每个矩形框包含/不包含物体的概率信息</p>\n<p><a href=\"https://cloud.tencent.com/developer/article/1436729\">重温目标检测—Faster R-CNN - 云+社区 - 腾讯云 (tencent.com)</a></p>\n<p><a href=\"https://blog.csdn.net/qq314000558/article/details/82082911\">Region Proposal Networks 详解_qq314000558的专栏-CSDN博客</a></p>\n<p>branch2和３使用了一种混合分类与回归的方法，来确定相对旋转角度。(也可称为基于软分类的<strong>方向估计</strong>)</p>\n<p>方向估计使用两个头分支：一个进行硬分类，给出一组预定义的四元数，以找到距离真值最近N个的四元数，然后第二个分支估计这N个四元数的权重，最终的方向由加权平均四元数给出。</p>\n<p>branch2 执行分类任务。设置m（1000）个预定义旋转角度，输出每个角度是前n（3）个最接近真值的角度的概率。branch2的输出为向量v，其维度为m*1。</p>\n<p>branch3使用branch2的输出作为输入，执行回归任务，输出向量w，其维度也为m*1。输出brach2每个结果的权重，但后面只取了前n个概率最大结果的权重（即最大的n个vj所对应的结果）使用，其余m-n个权重输出并未使用。然后将这n个权重通过softmax函数</p>\n<p>由brach1输出的二维边界框、brach2和3所得到的相对旋转角度、几何约束相结合，使用高斯-牛顿算法估计相对位置。（基于包围框检测的<strong>位置估计</strong>）</p>\n<h2 id=\"Satellite-Pose-Estimation-with-Deep-Landmark-Regression-and-Nonlinear-Pose-Refinement（第一名）\"><a href=\"#Satellite-Pose-Estimation-with-Deep-Landmark-Regression-and-Nonlinear-Pose-Refinement（第一名）\" class=\"headerlink\" title=\"Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement（第一名）\"></a>Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement（第一名）</h2><p>姿态估计分为三步：<strong>目标检测、关键点回归、PnP求解</strong></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211108213040358.png\" alt=\"image-20211108213040358\"></p>\n<p>这篇文章的采用了HRNet</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109112921791.png\" alt=\"image-20211109112921791\"></p>\n<h2 id=\"Segmentation-driven-Satellite-Pose-Estimation（第二名）\"><a href=\"#Segmentation-driven-Satellite-Pose-Estimation（第二名）\" class=\"headerlink\" title=\"Segmentation-driven Satellite Pose Estimation（第二名）\"></a>Segmentation-driven Satellite Pose Estimation（第二名）</h2><p><a href=\"https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf\">ppt——EPFL_CVLAB</a></p>\n<p><a href=\"https://github.com/cvlab-epfl/segmentation-driven-pose\">GitHub - cvlab-epfl/segmentation-driven-pose: Segmentation-driven 6D Object Pose Estimation. CVPR 2019.</a>根据他们实验室的这篇论文做的</p>\n<p>这个实验室的另外一篇6D姿态估计论文<a href=\"https://github.com/cvlab-epfl/single-stage-pose\">GitHub - cvlab-epfl/single-stage-pose: Single-Stage 6D Object Pose Estimation, CVPR 2020</a>：</p>\n<p>之前很多工作，都是先检测出2D图片上的一些关键点，然后建立2D-3D的correspondings，最后通过基于 <strong><em>RANSAC</em></strong> 的 <strong><em>Pnp</em></strong> 算法，求出最后的6D姿态。这篇文章主要的创新点是把基于 <strong><em>RANSAC</em></strong> 的 <strong><em>Pnp</em></strong> 算法集成到了网络之中，形成了一个End-to-end的网络。</p>\n<h2 id=\"REAL-TIME-FLIGHT-READY-NON-COOPERATIVE-SPACECRAFT-POSE-ESTIMATION-USING-MONOCULAR-IMAGERY\"><a href=\"#REAL-TIME-FLIGHT-READY-NON-COOPERATIVE-SPACECRAFT-POSE-ESTIMATION-USING-MONOCULAR-IMAGERY\" class=\"headerlink\" title=\"REAL-TIME, FLIGHT-READY, NON-COOPERATIVE SPACECRAFT POSE ESTIMATION USING MONOCULAR IMAGERY\"></a>REAL-TIME, FLIGHT-READY, NON-COOPERATIVE SPACECRAFT POSE ESTIMATION USING MONOCULAR IMAGERY</h2><p>在保证精度的同时又十分轻量级</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211108211901970.png\" alt=\"image-20211108211901970\"></p>\n<p>姿态估计分为三步：<strong>目标检测、关键点回归、PnP求解</strong></p>\n<p>目标检测将感兴趣区域裁剪后输出给关键点回归网络。关键点回归网络对航天器模型上预定的三维表面关键点的二维位置进行回归，然后利用PnP求得姿态。</p>\n<h2 id=\"Segmentation-driven-6D-Object-Pose-Estimation（CVPR-2019-by-cvlab）\"><a href=\"#Segmentation-driven-6D-Object-Pose-Estimation（CVPR-2019-by-cvlab）\" class=\"headerlink\" title=\"Segmentation-driven 6D Object Pose Estimation（CVPR 2019 by cvlab）\"></a>Segmentation-driven 6D Object Pose Estimation（CVPR 2019 by cvlab）</h2><p><a href=\"https://github.com/cvlab-epfl/segmentation-driven-pose\">cvlab-epfl/segmentation-driven-pose: Segmentation-driven 6D Object Pose Estimation. CVPR 2019. (github.com)</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220120102831564.png\" alt=\"image-20220120102831564\"></p>\n<h2 id=\"Single-Stage-6D-Object-Pose-Estimation（CVPR-2020-by-cvlab）\"><a href=\"#Single-Stage-6D-Object-Pose-Estimation（CVPR-2020-by-cvlab）\" class=\"headerlink\" title=\"Single-Stage 6D Object Pose Estimation（CVPR 2020 by cvlab）\"></a>Single-Stage 6D Object Pose Estimation（CVPR 2020 by cvlab）</h2><p>传统方法采用网络回归关键点，再加上ransac+pnp(但是这部分不属于神经网络的一部分)。</p>\n<p>本文将ransac+pnp融入到深度神经网络之中。</p>\n<h2 id=\"Wide-Depth-Range-6D-Object-Pose-Estimation-in-Space-CVPR-2021-by-cvlab\"><a href=\"#Wide-Depth-Range-6D-Object-Pose-Estimation-in-Space-CVPR-2021-by-cvlab\" class=\"headerlink\" title=\"Wide-Depth-Range 6D Object Pose Estimation in Space(CVPR 2021 by cvlab)\"></a>Wide-Depth-Range 6D Object Pose Estimation in Space(CVPR 2021 by cvlab)</h2><p><a href=\"https://github.com/cvlab-epfl/wide-depth-range-pose\">cvlab-epfl/wide-depth-range-pose: Wide-Depth-Range 6D Object Pose Estimation in Space, CVPR 2021 (github.com)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/440557881\">洛桑理工CVPR-21：太空中大深度范围6D物体位姿估计 - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/APbqBaxLoZfcrjOi69jQLA\">洛桑理工CVPR-21：太空中大深度范围6D物体位姿估计 (qq.com)</a></p>\n<h3 id=\"0-摘要\"><a href=\"#0-摘要\" class=\"headerlink\" title=\"0. 摘要\"></a>0. 摘要</h3><p>空间6D姿态估计带来了独特的挑战，这在地面环境中并不常见。最显著的区别之一是<strong>缺乏大气散射</strong>，这使得物体可以从很远的距离看到，同时使<strong>照明条件复杂化</strong>。目前可用的基准数据集没有充分强调这一方面，并且大多描述了非常接近的目标。</p>\n<p>在处理<strong>大深度范围变化</strong>下的姿态估计之前的工作依赖于<strong>两个阶段</strong>的方法，首先估计尺度，然后在调整大小的图像块上进行姿态估计。相反，我们提出了一种<strong>单级</strong>分层端到端可训练网络，该网络对规模变化更具鲁棒性。我们证明，它不仅在合成图像以类似于在空间拍摄的图像上，而且在标准基准上都优于现有方法。</p>\n<h3 id=\"1-介绍\"><a href=\"#1-介绍\" class=\"headerlink\" title=\"1. 介绍\"></a>1. 介绍</h3><p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206214920301.png\" alt=\"image-20211206214920301\" style=\"zoom:40%;\"></p>\n<p>目前面临的挑战：目标尺度和方向变化大（需要不同的图像特征）、空间材料镜面反射、图像某些部分的过度/不足曝光以及其他部分的细节缺失</p>\n<p>19年的卫星姿态估计比赛中最好的方法采用的是两阶段的方法：探测器发现一个围绕目标的轴对齐的盒子，该盒子被重新采样到一个统一的大小，最后由一个6D姿态估计器进行处理。但这个方法在几个方面仍有缺点，如下：</p>\n<ol>\n<li>目标检测和姿态估计被视为两个单独的过程，这排除了联合的训练</li>\n<li>其次，它只向正在使用的编解码器结构的最后一层提供监督信号，而不是向解码金字塔的所有级别提供监督信号，这将增加鲁棒性。</li>\n<li>两个过程都执行了许多相似的特征提取计算，这导致了不必要的重复工作</li>\n<li>这些方法依赖于基于深度学习的6D对象姿势估计的主要方法，包括训练网络以最小化预定义3D关键点的2D重投影误差，该误差无法处理较大的深度范围变化：如图2所示，重投影误差受单个关键点到相机的距离的强烈影响，不明确考虑这一点会降低性能</li>\n</ol>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206221242701.png\" alt=\"image-20211206221242701\" style=\"zoom:40%;\"></p>\n<p>为了解决这些缺点，我们引入了一个单一阶段的分层端到端可训练网络，如图3所示，该网络可产生鲁棒且不区分比例的6D姿势。</p>\n<p>这大多数仅从最后一层估计姿势的网络不同。为了跨尺度使用信息，该网络逐步缩小学习到的特征，为结果金字塔的每个层级导出3D到2D对应，最后使用基于RANSAC的PnP策略从这些对应集合推断出单个可靠姿势。</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206221605058.png\" alt=\"image-20211206221605058\" style=\"zoom:50%;\"></p>\n<p>为了跨尺度使用信息，它逐步缩小学习到的特征，为结果金字塔的每个层级导出3D到2D对应，最后使用基于RANSAC的PnP策略从这些对应集合推断出单个可靠姿势。这与大多数仅从最后一层估计姿势的网络不同。为了解决图2中的问题，我们<strong>基于3D位置而不是2D投影</strong>来最小化训练损失，从而使该方法对目标距离保持不变。我们使用特征金字塔网络（FPN）[24]作为主干，但与大多数依赖此类网络的方法不同，我们将每个训练实例分配到多个金字塔级别，以促进多尺度信息的联合使用。</p>\n<p>简言之，我们的贡献是一种新的6D姿势估计架构，它可以可靠地处理具有<strong>挑战性条件下的大规模变化</strong>。我们将证明，在已建立的<strong>SPEED</strong>数据集上，它<strong>优于所有最先进的方法，同时速度也快得多</strong>。此外，我们还介绍了一个<strong>更大规模的卫星姿态估计数据集</strong>，该数据集具有比SPEED更真实、更复杂的图像，并且我们表明，我们的方法在这个更具挑战性的场景中提供了相同的好处。最后，我们证明了我们的方法即使在<strong>深度变化较小的图像</strong>上（例如具有挑战性的遮挡LINEMOD数据集）也优于最新技术。</p>\n<h3 id=\"2-相关工作\"><a href=\"#2-相关工作\" class=\"headerlink\" title=\"2. 相关工作\"></a>2. 相关工作</h3><p>一般来说标准6D姿态估计流程为：首先建立三维到二维的对应关系，然后使用PnP解算器计算姿势</p>\n<p>之前的其他方法的设计都是为了在标准的计算机视觉基准上有效，其特点是尺度变化较小，当描绘对象的深度范围在不同的图像中发生显著变化时，它们的性能较差。部分尝试处理缩放问题的工作多采用两阶段的方法，使得目标检测和姿态估计分离开来，使得网络结构大大复杂化，引入大量冗余操作，进而导致实时性不高。</p>\n<p>我们的<strong>主要贡献</strong>是利用单个网络固有的层次结构，在各个层次上共享权重，以处理尺度变化问题。我们证明了这一点既健壮又高效。</p>\n<p>分层处理，如图像金字塔，是多尺度图像理解的经典理念。最近，这一想法已经通过特征金字塔网络（FPN）转化为深度学习领域，现在它是许多目标检测框架的标准组件，我们将此想法用于6D姿态估计。然而，与大多数将每个金字塔级别显式关联到单个预定义比例的对象检测方法不同，我们引入了一种<strong>动态采样策略</strong>，其中每个训练实例利用所有金字塔级别，尽管权重不同。这使我们能够在推理时融合来自不同层面的预测，从而得到更稳健的6D姿势估计。</p>\n<p>我们将实验重点放在星载物体的6D姿态估计上，因为在这种情况下，对缩放的鲁棒性非常重要，特别是在接近需要运动同步的非合作目标（如空间垃圾）时。空间工程界有自己关于6D姿态估计的文献。虽然它的发展方式类似于计算机视觉的进步，但它主要关注手工制作的方法，只有少数作品提出了基于深度学习的方法。其主要原因是缺乏大量的空间物体注释数据。</p>\n<p><strong>第二个贡献</strong>是提出了一个基于物理渲染创建的<strong>SwissCube数据集</strong>。该数据集中的图像是使用基于物理的光谱光传输模拟创建的，该模拟涉及一个立方体卫星的精确参考3D模型，该模型考虑了太阳、地球、恒星等的影响。</p>\n<p>之前由ESA提出的SPEED数据集有以下的缺点：</p>\n<ol>\n<li>没提供卫星的三维模型，重建模型有误差</li>\n<li>通过基于非物理的渲染技术合成的，不能反映空间照明的复杂性</li>\n<li>深度分布不均匀，只有很少的图像描述了距离相机很远的卫星。</li>\n</ol>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207095921055.png\" alt=\"image-20211207095921055\" style=\"zoom:50%;\"></p>\n<h3 id=\"3-方法\"><a href=\"#3-方法\" class=\"headerlink\" title=\"3. 方法\"></a>3. 方法</h3><p>使用特征金字塔在多尺度上回归预定义3D点的2D投影</p>\n<h4 id=\"3-1-网络金字塔结构（Pyramid-Network-Architecture）\"><a href=\"#3-1-网络金字塔结构（Pyramid-Network-Architecture）\" class=\"headerlink\" title=\"3.1 网络金字塔结构（Pyramid Network Architecture）\"></a>3.1 网络金字塔结构（Pyramid Network Architecture）</h4><p>大多数6D姿势估计深度网络依赖于编码器-解码器体系结构。因此，为了处理6D对象姿态估计的大范围变化，我们<strong>使用编码器网络固有的层次结构，而不是依赖于额外的对象检测网络</strong>，它提取不同尺度的特征。具体而言，我们在框架中使用Darknet-53[34]作为主干，并采用与FPN[24]中设计的用于目标检测的网络架构相同的网络架构，该网络架构由k=5级特征图组成，{F1、F2、F3、F4、F5}，<strong>每个特征图具有越来越大的感受野</strong>。</p>\n<p>我们从金字塔的<strong>每一层回归对象3D关键点的2D位置，而不是仅从特征图F5计算单个姿势估计</strong>。为此，我们依赖于[11]中的分段驱动方法，在每个特征地图的每个空间位置生成特征向量，以输出3D关键点的2D投影（表示为相对于相应单元中心的偏移量）和每个对象类的对象性分数。因此，每个单元的特征向量是一个C×（2×8+1）维向量，由8个2D关键点的偏移位置（见下图）和1个objectness(物体存在于该cell的概率)。要对分割掩码进行编码，所有特征单元都需要参与对象性预测，包括不包含目标对象的单元。相反，如下所述，只有选定的单元参与姿势回归器的训练。</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208101923121.png\" alt=\"image-20211208101923121\" style=\"zoom:50%;\"></p>\n<p><a href=\"https://blog.csdn.net/u011984148/article/details/112504800\">(29条消息) 理解物体检测中的Objectness_AI公园-CSDN博客</a></p>\n<h4 id=\"3-2-集合感知采样（Ensemble-Aware-Sampling）\"><a href=\"#3-2-集合感知采样（Ensemble-Aware-Sampling）\" class=\"headerlink\" title=\"3.2 集合感知采样（Ensemble-Aware Sampling）\"></a>3.2 集合感知采样（Ensemble-Aware Sampling）</h4><p><a href=\"https://www.jianshu.com/p/86f39172b68a\">特征向量（Feature Vectors） - 简书 (jianshu.com)</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211209163724502.png\" alt=\"image-20211209163724502\" style=\"zoom:40%;\"></p>\n<p>处理尺度变化过大问题的通用方法为：分治策略——将训练集分为不重复的组，之后在训练过程中将不同的pyramid层分配到不同尺度。此策略在目标检测中是够用的，因为可以在不同的层输出最优结果。但是在6D估计中却不能够联合多层信息提升鲁棒性，单个层的输出会引入较高的噪声。</p>\n<p>具体采样过程如下，需要根据某一个训练实例的尺寸大小动态确定在每一层的采样数量。λ控制着每层的激活cell数量，λ=0时在所有层取相同的cell数量；λ&gt;20过大时退化为FPN结构中的“hard assignment”</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/v2-1fad96cf5150c0d3a42a8580cdead3c9_720w.jpg\" alt=\"img\"></p>\n<h4 id=\"3-3-三维空间中的损失函数-（Loss-Function-in-3D-Space）\"><a href=\"#3-3-三维空间中的损失函数-（Loss-Function-in-3D-Space）\" class=\"headerlink\" title=\"3.3 三维空间中的损失函数 （Loss Function in 3D Space）\"></a>3.3 三维空间中的损失函数 （Loss Function in 3D Space）</h4><p>在采样过程中选择的特征向量用于回归3D包围盒的8个角的2D投影。在回归2D关键点位置时，大多数方法采用的损失函数为：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207194451711.png\" alt=\"image-20211207194451711\" style=\"zoom:50%;\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207194548714.png\" alt=\"image-20211207194548714\" style=\"zoom:50%;\"></p>\n<p>但正如图2所示，这种损失函数是次优的，特别是在存在大深度变化的情况下，因为(a)它更强调某些关键点而不是其他关键点(不同关键点误差对损失函数的影响不同)，(b)并且还取决于物体的相对位置（物体不同位置的误差影响也不同）。</p>\n<p>为了解决以上问题，本文在3D空间引入损失函数（其实就是把预测的2D关键点根据位姿还原到3D，然后再和真实值比较）</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/49981234\">Focal loss论文详解 - 知乎 (zhihu.com)</a> </p>\n<h4 id=\"3-4-多尺度融合推理-（Inference-via-Multi-Scale-Fusion）\"><a href=\"#3-4-多尺度融合推理-（Inference-via-Multi-Scale-Fusion）\" class=\"headerlink\" title=\"3.4 多尺度融合推理 （Inference via Multi-Scale Fusion）\"></a>3.4 多尺度融合推理 （Inference via Multi-Scale Fusion）</h4><p>Ensemble-awaresampling（智能采样策略）使得每个层都能够生成有效pose信息，因此可以在所有层中使用阈值=0.3过滤objectness score。所有结果可以统一使用RANSAC+PnP方法融合，也可以使用基于学习的方法融合（Hu-Single stage 6D pose）。</p>\n<p>本文为简单使用RANSAC+PnP方法。首先估计物体尺寸S，选取所有层中objectness score最高的特征向量，通过8个box角计算S。明确尺寸后，从每个层中选取Nk个高分特征cell，以此来构造如下2D-3D对应关系。最后通过RANSAC+PnP方法完成姿势估计。</p>\n<p><strong>推理总结</strong>：依然使用智能采样策略，通过最高分确定尺寸S，之后正常使用采样策略从各个层中选取特征数量，全部用于姿势估计。</p>\n<h3 id=\"4-实验\"><a href=\"#4-实验\" class=\"headerlink\" title=\"4.实验\"></a>4.实验</h3><p>首先在SPEED数据集上测试，采用的评估指标为比赛的指标。</p>\n<p>然后介绍SwissCube数据集，采用ADI-0.1d指标，以评估不同深度范围的性能。并在真实图像进行了测试(请注意，这些真实的图像不是在太空中捕获的，而是在实验室环境中使用目标的模型和OptiTrack运动捕获系统来获取一些图像的地面真实姿势信息。)。</p>\n<p>最后为了证明该方法的通用性，在描述小深度变化的Occluded-LINEMOD标准数据集上对其进行了评估。</p>\n<h4 id=\"4-1-Evaluation-on-the-SPEED-Dataset\"><a href=\"#4-1-Evaluation-on-the-SPEED-Dataset\" class=\"headerlink\" title=\"4.1 Evaluation on the SPEED Dataset\"></a>4.1 Evaluation on the SPEED Dataset</h4><p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208103411496.png\" alt=\"image-20211208103411496\" style=\"zoom:50%;\"></p>\n<h4 id=\"4-2-Evaluation-on-the-SwissCube-Dataset\"><a href=\"#4-2-Evaluation-on-the-SwissCube-Dataset\" class=\"headerlink\" title=\"4.2 Evaluation on the SwissCube Dataset\"></a>4.2 Evaluation on the SwissCube Dataset</h4><p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208104306785.png\" alt=\"image-20211208104306785\" style=\"zoom:50%;\"></p>\n<p>SwissCube Dataset 删除了进入地球阴影的部分，因为这部分的图像基本纯黑。一共有500个场景，每个场景100帧图像。</p>\n<p>相机距离在1d-10d之间，d为SwissCube 卫星的直径（不含天线），分为近、中、远三类，分别对应距离[1d-4d], [4d-7d],  [7d-10d]</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208105825232.png\" alt=\"image-20211208105825232\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208105838151.png\" alt=\"image-20211208105838151\" style=\"zoom:50%;\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208110509594.png\" alt=\"image-20211208110509594\" style=\"zoom:50%;\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208110521554.png\" alt=\"image-20211208110521554\" style=\"zoom:50%;\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208140720710.png\" alt=\"image-20211208140720710\" style=\"zoom:50%;\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208141002214.png\" alt=\"image-20211208141002214\" style=\"zoom:50%;\"></p>\n<h4 id=\"4-3-Results-on-Real-Images\"><a href=\"#4-3-Results-on-Real-Images\" class=\"headerlink\" title=\"4.3. Results on Real Images\"></a>4.3. Results on Real Images</h4><h4 id=\"4-4-Evaluation-on-Occluded-LINEMOD\"><a href=\"#4-4-Evaluation-on-Occluded-LINEMOD\" class=\"headerlink\" title=\"4.4. Evaluation on Occluded-LINEMOD\"></a>4.4. Evaluation on Occluded-LINEMOD</h4><h3 id=\"代码实测结果\"><a href=\"#代码实测结果\" class=\"headerlink\" title=\"代码实测结果\"></a>代码实测结果</h3><p>python test.py —config_file ./configs/swisscube.yaml —num_workers 0 —weight_file ‘./swisscube_pretrained.pth’ —running_device ‘cuda’</p>\n<p>python train.py —config_file ./configs/swisscube.yaml —num_workers 0 —weight_file ‘./swisscube_pretrained.pth’ —running_device ‘cuda’</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211210103613085.png\" alt=\"image-20211210103613085\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211210103622008.png\" alt=\"image-20211210103622008\"></p>\n<p>作者提供的模型精度远高于论文中的精度</p>\n<p>backbone的输出：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211214103709819.png\" alt=\"image-20211214103709819\" style=\"zoom:50%;\"></p>\n<p>fpn的输出：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211214103620809.png\" alt=\"image-20211214103620809\" style=\"zoom:60%;\"></p>\n<p>PoseHead的输出：</p>\n<p>测试时总model的输出pred：由score, cls_id, R, T组成 </p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211215103709168.png\" alt=\"image-20211215103709168\"></p>\n<p>speed数据集模型重建：</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105160244996.png\" alt=\"image-20220105160244996\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105160316940.png\" alt=\"image-20220105160316940\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105165415536.png\" alt=\"image-20220105165415536\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105165427442.png\" alt=\"image-20220105165427442\"></p>\n<h2 id=\"申请书要求\"><a href=\"#申请书要求\" class=\"headerlink\" title=\"申请书要求\"></a>申请书要求</h2><p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109163033078.png\" alt=\"image-20211109163033078\"></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109163041316.png\" alt=\"image-20211109163041316\"></p>\n<h2 id=\"相关调研\"><a href=\"#相关调研\" class=\"headerlink\" title=\"相关调研\"></a>相关调研</h2><h3 id=\"马鸿英学长：\"><a href=\"#马鸿英学长：\" class=\"headerlink\" title=\"马鸿英学长：\"></a><strong>马鸿英</strong>学长：</h3><p>天线姿态估计</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143216549.png\" alt=\"image-20211116143216549\" style=\"zoom:33%;\"></p>\n<p>目标：只用测量出偏角θ</p>\n<p>方法：最小化重投影误差</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143457174.png\" alt=\"image-20211116143457174\" style=\"zoom: 33%;\"></p>\n<p>这个方法在《POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS》这篇卫星姿态估计论文中使用。</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143633683.png\" alt=\"image-20211116143633683\" style=\"zoom: 33%;\"></p>\n<h3 id=\"张澜涛学长：\"><a href=\"#张澜涛学长：\" class=\"headerlink\" title=\"张澜涛学长：\"></a>张澜涛学长：</h3><p>他们目前还在做锥套的检测，飞机的姿态估计还没怎么开始做，目前打算用某个关键点检测的方法，两相机各看到一些固定的关键点，三角测量这些关键点得到关键点坐标，算点集空间变换。</p>\n<p>经讨论觉得卫星姿态估计和飞机的姿态估计几乎差不多，后面我们可以一起做，只不过他们的项目对于检测速度的要求特别高，而我目前看到的一篇轻量化的卫星姿态估计的网络最快也只能达到6.6Hz（不过这个得看在什么设备上跑）。</p>\n<h2 id=\"2021官方入门工具包\"><a href=\"#2021官方入门工具包\" class=\"headerlink\" title=\"2021官方入门工具包\"></a>2021官方入门工具包</h2><p><a href=\"https://gitlab.com/EuropeanSpaceAgency/speedplus-utils\">EuropeanSpaceAgency / speedplus-utils · GitLab</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211117102805335.png\" alt=\"image-20211117102805335\"></p>\n","categories":["位姿估计"],"tags":["Satellite","Pose Estimation"]},{"title":"位姿估计基础及论文","url":"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/","content":"<h2 id=\"3D旋转的表示\"><a href=\"#3D旋转的表示\" class=\"headerlink\" title=\"3D旋转的表示\"></a>3D旋转的表示</h2><ol>\n<li><p>旋转矩阵</p>\n</li>\n<li><p>欧拉角</p>\n<p> 使用动态欧拉角会出现万向锁现象；静态欧拉角不存在万向锁的问题。通时还要注意不同的旋转顺序所得的结果是不同的。</p>\n<p> <a href=\"https://www.bilibili.com/video/BV1Bt4y1v7R1?from=search&amp;seid=2218520216726804689&amp;spm_id_from=333.337.0.0\">3d旋转欧拉角与万向锁！Cocos Creator 3D!_哔哩哔哩_bilibili</a></p>\n</li>\n<li><p>四元数</p>\n<p> <a href=\"https://www.bilibili.com/video/BV1SW411y7W1/?spm_id_from=autoNext\">四元数的可视化_哔哩哔哩_bilibili</a></p>\n<p> <a href=\"https://eater.net/quaternions/video/intro\">Visualizing quaternions | 3blue1brown + Ben Eater</a>交互式体验四元数</p>\n<p> <img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20211104112131946.png\" alt=\"image-20211104112131946\"><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20211104112421258.png\" alt=\"image-20211104112421258\"><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20211104112437943.png\" alt=\"image-20211104112437943\"></p>\n<p> 四元数就相当于后三个数定义了一个单位向量作为旋转轴，第一个数代表旋转的角度</p>\n<p> 设旋转角为θ(从轴的正向箭头处看下去，θ为正则向逆时针旋转，为负则向顺时针旋转)，<strong>q = cos(θ/2) + sin(θ/2)(xi + yj + zk) = q0 + q1i + q2j +q3k</strong></p>\n</li>\n</ol>\n<p><a href=\"https://blog.csdn.net/lql0716/article/details/72597719\">旋转矩阵、欧拉角、四元数理论及其转换关系<em>aibotlab的博客-CSDN博客</em>欧拉角转四元数</a></p>\n<h2 id=\"PnP\"><a href=\"#PnP\" class=\"headerlink\" title=\"PnP\"></a>PnP</h2><p>待</p>\n<h2 id=\"软分类和硬分类\"><a href=\"#软分类和硬分类\" class=\"headerlink\" title=\"软分类和硬分类\"></a>软分类和硬分类</h2><p>软分类：使用的是概率模型，输出不同类对应的概率，最后的分类结果取概率最大的类，如多SVM组合分类；</p>\n<p>硬分类：使用的是非概率模型，分类结果就是决策函数的决策结果；</p>\n<h2 id=\"BOP-Challenge-2020-on-6D-Object-Localization\"><a href=\"#BOP-Challenge-2020-on-6D-Object-Localization\" class=\"headerlink\" title=\"BOP Challenge 2020 on 6D Object Localization\"></a>BOP Challenge 2020 on 6D Object Localization</h2><p>In the BOP Challenge 2019, methods using the <strong>depth image channel</strong>, whichwere mostly based on the <strong>point pair features (PPF’s)</strong> [10], clearly outperformed methods relying <strong>only on the RGB channels</strong>, all of which were based on deep neural networks (DNN’s).  （19年及以前PPF方法优于深度学习方法）</p>\n<p>20年的比赛中有五个DNN方法超过了19年的冠军(采用PPF)，第三名的方法只使用了RGB通道，完全没用深度通道。</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20211123085515345.png\" alt=\"image-20211123085515345\"></p>\n<p>BOP数据集说明：</p>\n<p><a href=\"https://blog.csdn.net/u014712806/article/details/112339410\">BOP数据集格式说明_屠龙之术-CSDN博客_bop文件</a></p>\n<p>PBR全称(Physicallly-Based Rendering)</p>\n<h2 id=\"CosyPose-Consistent-multi-view-multi-object-6D-pose-estimation\"><a href=\"#CosyPose-Consistent-multi-view-multi-object-6D-pose-estimation\" class=\"headerlink\" title=\"CosyPose: Consistent multi-view multi-object 6D pose estimation\"></a>CosyPose: Consistent multi-view multi-object 6D pose estimation</h2><p>这个代码没跑出来<a href=\"https://github.com/ylabbe/cosypose\">ylabbe/cosypose: Code for “CosyPose: Consistent multi-view multi-object 6D pose estimation”, ECCV 2020. (github.com)</a></p>\n<h2 id=\"HybridPose-6D-Object-Pose-Estimation-under-Hybrid-Representations（CVPR-2020）\"><a href=\"#HybridPose-6D-Object-Pose-Estimation-under-Hybrid-Representations（CVPR-2020）\" class=\"headerlink\" title=\"HybridPose: 6D Object Pose Estimation under Hybrid Representations（CVPR 2020）\"></a>HybridPose: 6D Object Pose Estimation under Hybrid Representations（<strong>CVPR 2020</strong>）</h2><p><a href=\"https://blog.csdn.net/john_bh/article/details/103998704\">6D位姿估计 HybridPose: 6D Object Pose Estimation under Hybrid Representations_不忘初心~-CSDN博客</a></p>\n<p><a href=\"https://github.com/chensong1995/HybridPose\">chensong1995/HybridPose: HybridPose: 6D Object Pose Estimation under Hybrid Representation (CVPR 2020) (github.com)</a>这个star多，正在跑</p>\n<p>复现遇到的bug：</p>\n<p><a href=\"https://blog.csdn.net/qq_15158911/article/details/107887490\">(25条消息) win10安装visual studio C++ build tools 提示安装包丢失或毁坏_与君共勉-CSDN博客_buildtools_msbuild.msi</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20211123112129454.png\" alt=\"image-20211123112129454\"></p>\n<h2 id=\"DPOD-6D-Pose-Object-Detector-and-Refiner\"><a href=\"#DPOD-6D-Pose-Object-Detector-and-Refiner\" class=\"headerlink\" title=\"DPOD: 6D Pose Object Detector and Refiner\"></a>DPOD: 6D Pose Object Detector and Refiner</h2><p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/Screenshot from 2021-12-05 20-02-59.png\" alt=\"Screenshot from 2021-12-05 20-02-59\"></p>\n<p>这个bug解决不了，放弃了</p>\n<h2 id=\"CDPN-Coordinates-based-Disentangled-Pose-Network-for-Real-time-RGB-based-6-DoF-Object-Pose-Estimation\"><a href=\"#CDPN-Coordinates-based-Disentangled-Pose-Network-for-Real-time-RGB-based-6-DoF-Object-Pose-Estimation\" class=\"headerlink\" title=\"CDPN: Coordinates-based Disentangled Pose Network for Real-time RGB-based 6-DoF Object Pose Estimation\"></a>CDPN: Coordinates-based Disentangled Pose Network for Real-time RGB-based 6-DoF Object Pose Estimation</h2><p><a href=\"https://github.com/LZGMatrix/CDPN_ICCV2019_ZhigangLi\">https://github.com/LZGMatrix/CDPN_ICCV2019_ZhigangLi</a></p>\n<p>正在尝试</p>\n<h2 id=\"Pix2Pose-Pixel-Wise-Coordinate-Regression-of-Objects-for-6D-Pose-Estimation（ICCV2019）\"><a href=\"#Pix2Pose-Pixel-Wise-Coordinate-Regression-of-Objects-for-6D-Pose-Estimation（ICCV2019）\" class=\"headerlink\" title=\"Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation（ICCV2019）\"></a>Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation（ICCV2019）</h2><p>由于遮挡和对称性等问题，仅使用<strong>RGB图像</strong>估计物体的6D姿态仍然具有挑战性。如果没有专家知识或专业的扫描设备，也<strong>很难构建具有精确纹理的三维模型</strong>。为了解决这些问题，我们提出了一种新的位姿估计方法Pix2Pose，它可以在没有纹理模型的情况下预测每个目标像素的三维坐标。设计了一种自动编码器结构来估计三维坐标和每个像素的期望误差。然后将这些像素级预测用于多个阶段，形成2D-3D对应关系，用RANSAC迭代的PnP算法直接计算姿态。我们的方法通过利用最近在生成性对抗训练中的成果来精确地恢复被遮挡的部分，从而对遮挡具有鲁棒性。此外，提出了一种新的损耗函数变压器损耗，通过将预测引导到最接近的对称姿态来处理对称目标，对包含对称和遮挡目标的三个不同基准数据集的计算表明，我们的方法优于仅使用RGB图像的最新方法。</p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/AI/姿态估计基础/image-20220308091537677.png\" alt=\"image-20220308091537677\" style=\"zoom:50%;\"></p>\n<p>tensorflow  &amp;  训练步骤复杂<a href=\"https://github.com/kirumang/Pix2Pose\">https://github.com/kirumang/Pix2Pose</a></p>\n<h2 id=\"Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction（CVPR2018）\"><a href=\"#Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction（CVPR2018）\" class=\"headerlink\" title=\"Real-Time Seamless Single Shot 6D Object Pose Prediction（CVPR2018）\"></a>Real-Time Seamless Single Shot 6D Object Pose Prediction（CVPR2018）</h2><p>论文链接：<a href=\"https://arxiv.org/abs/1711.08848\">https://arxiv.org/abs/1711.08848</a><br>代码链接：<a href=\"https://github.com/Microsoft/singleshotpose\">https://github.com/Microsoft/singleshotpose</a></p>\n<p><strong>主要思想</strong>：我们提出了一种单阶段方法来同时检测RGB图像中的一个物体并预测其6D姿态，不需要多个阶段或检查多个假设。不像最近提出的一些单阶段技术，它只预测一个近似6D的姿势，然后必须细化，我们是足够精确的，不需要额外的后处理。它的速度非常快，在Titan X（帕斯卡）GPU上每秒50帧，因此更适合实时处理。我们的方法的关键部分是一个新的CNN架构，直接预测对象的3D边界框的投影顶点的2D图像位置，然后用PnP算法估计物体的6D姿态。我们的单目标和多目标姿态估计方法在LINEMOD和OCCLUSION数据集上明显优于其他最近基于CNN的方法。</p>\n<p>主要贡献： 论文的主要贡献是一个新的网络架构，即一个快速和准确的单阶段6D姿势预测网络，不需要任何后处理。它以无缝和自然的方式扩展了用于二维检测的单阶段CNN结构去执行6D检测任务。实现基于YOLO，但该方法适用于其他单阶段检测器，如SSD及其变体。</p>\n<p>代码环境太老了</p>\n<p>新版代码  <a href=\"https://github.com/a2824256/singleshotpose_imp\">https://github.com/a2824256/singleshotpose_imp</a>   （据说自制模型位姿估计异常） </p>\n<p>Single Shot 6D Object Pose Prediction代码复现—测试<a href=\"https://codeantenna.com/a/nuYk97dWkZ\">https://codeantenna.com/a/nuYk97dWkZ</a></p>\n<h2 id=\"SSD-6D-Making-RGB-Based-3D-Detection-and-6D-Pose-Estimation-Great-Again\"><a href=\"#SSD-6D-Making-RGB-Based-3D-Detection-and-6D-Pose-Estimation-Great-Again\" class=\"headerlink\" title=\"SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again\"></a>SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again</h2><p>主要思想：提出了一种新的基于RGB数据的三维模型实例检测和6D姿态估计方法。为此，我们扩展了流行的SSD范式，以覆盖完整的6D姿势空间，并仅对合成模型数据进行训练。我们的方法可以与当前最先进的方法在多个具有挑战性的RGBD数据集上竞争或超越。此外，我们的方法在10Hz左右，要比相关的其它方法快很多倍。</p>\n<p>主要贡献：</p>\n<p>（1） 一个仅利用合成三维模型信息的训练阶段<br>（2） 模型位姿空间的分解，便于对称性的训练和处理<br>（3） SSD的一种扩展，产生2D检测并推断出正确的6D姿势</p>\n<p>论文链接：<a href=\"https://arxiv.org/abs/1711.10006v1\">https://arxiv.org/abs/1711.10006v1</a><br>代码链接：<a href=\"https://github.com/wadimkehl/ssd-6d\">https://github.com/wadimkehl/ssd-6d</a>    没有提供训练代码</p>\n<h2 id=\"BundleTrack-6D-Pose-Tracking-for-Novel-Objects-without-Instance-or-Category-Level-3D-Models\"><a href=\"#BundleTrack-6D-Pose-Tracking-for-Novel-Objects-without-Instance-or-Category-Level-3D-Models\" class=\"headerlink\" title=\"BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models\"></a>BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models</h2><p>rgbd输入</p>\n<p><strong>特别之处</strong>：</p>\n<ul>\n<li>不需要被跟踪物体的3D模型</li>\n<li>稳定，不受明显遮挡的影响</li>\n<li>10Hz</li>\n<li>在NOCS上效果非常好，在YCBInEOAT上效果与se(3)-TrackNet类似（比se差一点）</li>\n</ul>\n<h2 id=\"GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation（CVPR-2021）（基于几何信息指导的单目6D物体姿态直接回归算法）\"><a href=\"#GDR-Net-Geometry-Guided-Direct-Regression-Network-for-Monocular-6D-Object-Pose-Estimation（CVPR-2021）（基于几何信息指导的单目6D物体姿态直接回归算法）\" class=\"headerlink\" title=\"GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation（CVPR 2021）（基于几何信息指导的单目6D物体姿态直接回归算法）\"></a>GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation（CVPR 2021）（基于几何信息指导的单目6D物体姿态直接回归算法）</h2><h2 id=\"EfficientPose-An-efficient-accurate-and-scalable-end-to-end-6D-multi-object-pose-estimation-approach\"><a href=\"#EfficientPose-An-efficient-accurate-and-scalable-end-to-end-6D-multi-object-pose-estimation-approach\" class=\"headerlink\" title=\"EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach\"></a>EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach</h2><p><a href=\"https://blog.csdn.net/qq_41707788/article/details/120832323\">【6D Pose/论文阅读】EfficientPose_遗梦少年的博客-CSDN博客</a></p>\n<p><a href=\"https://blog.csdn.net/rush9838465/article/details/112475562\">https://blog.csdn.net/rush9838465/article/details/112475562</a></p>\n<p>代码： <a href=\"https://github.com/ybkscht/EfficientPose\">https://github.com/ybkscht/EfficientPose</a></p>\n<p>论文： <a href=\"https://click.endnote.com/viewer?doi=10.48550%2Farxiv.2011.04307&amp;token=WzM0NTc1MTMsIjEwLjQ4NTUwL2FyeGl2LjIwMTEuMDQzMDciXQ.TMWGnTMXlvPuSUwFgmZJHcon4Ek\">https://click.endnote.com/viewer?doi=10.48550%2Farxiv.2011.04307&amp;token=WzM0NTc1MTMsIjEwLjQ4NTUwL2FyeGl2LjIwMTEuMDQzMDciXQ.TMWGnTMXlvPuSUwFgmZJHcon4Ek</a></p>\n<p>测试失败</p>\n<h2 id=\"DOPE-在ros上跑的\"><a href=\"#DOPE-在ros上跑的\" class=\"headerlink\" title=\"DOPE  在ros上跑的\"></a>DOPE  在ros上跑的</h2><p><a href=\"https://github.com/yehengchen/DOPE-ROS-D435\">yehengchen/DOPE-ROS-D435: Object 6DoF Pose Estimation for Assembly Robots Trained on Synthetic Data - ROS Kinetic/Melodic Using Intel® RealSense D435 (github.com)</a></p>\n<p><a href=\"https://github.com/NVlabs/Deep_Object_Pose\">NVlabs/Deep_Object_Pose: Deep Object Pose Estimation (DOPE) – ROS inference (CoRL 2018) (github.com)</a></p>\n<h2 id=\"PVNet-Pixel-wise-Voting-Network-for-6DoF-Pose-Estimation\"><a href=\"#PVNet-Pixel-wise-Voting-Network-for-6DoF-Pose-Estimation\" class=\"headerlink\" title=\"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation\"></a>PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</h2><p><a href=\"https://github.com/zju3dv/pvnet\">https://github.com/zju3dv/pvnet</a></p>\n<p><a href=\"https://github.com/zju3dv/clean-pvnet\">https://github.com/zju3dv/clean-pvnet</a></p>\n<p>作者还开源了他们用<a href=\"https://so.csdn.net/so/search?q=blender&amp;spm=1001.2101.3001.7020\">blender</a>合成数据代码：<a href=\"https://github.com/zju3dv/pvnet-rendering\">https://github.com/zju3dv/pvnet-rendering</a></p>\n<p>其他博客：</p>\n<p>PVNET代码复现讲解<a href=\"https://www.freesion.com/article/85511243426/\">https://www.freesion.com/article/85511243426/</a></p>\n<p>pvnet——总结<a href=\"https://blog.csdn.net/Marilynviolet/article/details/100747094\">https://blog.csdn.net/Marilynviolet/article/details/100747094</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/AI/姿态估计基础/image-20220308170517455.png\" alt=\"image-20220308170517455\"></p>\n<pre><code>ROOT=/home/young/code/clean-pvnet\ncd $ROOT/lib/csrc\nexport CUDA_HOME=\"/usr/local/cuda-11.5\"\ncd ransac_voting\npython setup.py build_ext --inplace\ncd ../nn\npython setup.py build_ext --inplace\ncd ../fps\npython setup.py build_ext --inplace\n\n# If you want to run PVNet with a detector\ncd ../dcn_v2\npython setup.py build_ext --inplace\n\n# If you want to use the uncertainty-driven PnP\ncd ../uncertainty_pnp\nsudo apt-get install libgoogle-glog-dev\nsudo apt-get install libsuitesparse-dev\nsudo apt-get install libatlas-base-dev\npython setup.py build_ext --inplace\n\n\nROOT=/path/to/clean-pvnet\ncd $ROOT/data\nln -s /media/young/young/dataset/LINEMOD/LINEMOD linemod\nln -s /path/to/linemod_orig linemod_orig\nln -s /path/to/occlusion_linemod occlusion_linemod\n\n# the following is used for tless\nln -s /path/to/tless tless\nln -s /path/to/cache cache\nln -s /path/to/SUN2012pascalformat sun\n</code></pre><p>安装中遇到cuda版本问题</p>\n<pre><code>conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch\n</code></pre><p>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/young/code/pvnet/lib/utils/extend_utils/lib</p>\n<pre><code>ln -s /media/young/young/dataset/LINEMOD/LINEMOD $ROOT/data/LINEMOD\n</code></pre><p>本文解决了在严重遮挡或截断下从单个 RGB 图像进行 6DoF 姿态估计的挑战。 最近的许多工作表明，一种两阶段方法，首先检测关键点，然后解决用于姿势估计的 Perspective-n-Point (PnP) 问题，取得了显着的性能。 然而，这些方法中的大多数仅通过回归图像坐标或热图来定位一组稀疏关键点，这对遮挡和截断很敏感。 相反，我们引入了逐像素投票网络 (PVNet) 来回归指向关键点的逐像素单位向量，并使用这些向量使用 RANSAC 对关键点位置进行投票。 这为定位被遮挡或截断的关键点创建了灵活的表示。 这种表示的另一个重要特征是它提供了关键点位置的不确定性，PnP 求解器可以进一步利用这些不确定性。 实验表明，所提出的方法在 LINEMOD、Occlusion LINEMOD 和 YCBVideo 数据集上大大优于现有技术，同时对实时姿态估计也很有效。 我们进一步创建了一个截断 LINEMOD 数据集，以验证我们的方法对截断的鲁棒性。 该代码将在 <a href=\"https://zju-3dv.github.io/pvnet/\">https://zju-3dv.github.io/pvnet/</a> 上提供。</p>\n<p>传统方法依赖手工特征，对于图像变化和背景干扰不够鲁棒（why？）</p>\n<p>传统网络关键点回归+pnp，有遮挡问题</p>\n<p>我们提出了一种使用<strong>像素级投票网络</strong> (PVNet) 进行 6D 姿势估计的新框架。 基本思想如图 1 所示。PVNet 不是直接回归关键点的图像坐标，而是预测表示从对象的每个像素到关键点的方向的单位向量。 然后这些方向根据 RANSAC 对关键点位置进行投票。——关键点的<strong>向量场</strong>表示方法</p>\n<p>该思想的动机在于，刚体的特性——即我们看到其一部分就能推断出其他部分的相对方向。</p>\n<p>不确定性驱动的pnp</p>\n<h2 id=\"学习路线\"><a href=\"#学习路线\" class=\"headerlink\" title=\"学习路线\"></a>学习路线</h2><p>这篇博客的评论区贼强</p>\n<p><a href=\"https://blog.csdn.net/dsoftware/article/details/97955570?spm=1001.2101.3001.6650.9&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-9.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-9.no_search_link\"> 刚体6D位姿估计方法综述_dsoftware的博客-CSDN博客_6d位姿估计</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20211125112415235.png\" alt=\"image-20211125112415235\" style=\"zoom:50%;\"></p>\n<p>纹理特征和几何细节丰富的用对应点的方法、纹理特征和几何细节弱的用基于模板的方法、有遮挡的或者是类别级对象的用基于投票的方法</p>\n<p>当有遮挡时，基于模板的方法效果最差</p>\n<p>了解物体6Dpose的意义，多种表示李代数欧拉角四元数等，用PnP求解6Dpose的原理等；其次，阅读综述论文，看业界为了求解6D姿态用了哪些方法，传统的以及深度学习的，可以只看摘要和前沿；再次，挑选几篇代表性、前沿、开源的代码，实际复现测试，分析优缺点；最后，结合自身项目，确定输入数据是纯RGB，还是RGB-D，还是Lidar点云，有没有对应的3D模型，对于速度或者精度要求多高，是不是只针对实例级别物体等，选择最接近的某个算法，在其上面改进</p>\n<p>有很多基于RGB图像的6D位姿估计方法：</p>\n<p>PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation；</p>\n<p>Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation；</p>\n<p>Implicit 3D Orientation Learning for 6D Object Detection from RGB Images；</p>\n<p>EPOS: Estimating 6D Pose of Objects with Symmetries；</p>\n<p>DPOD: 6D Pose Object Detector and Refiner；</p>\n<p>CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation；</p>\n<p>Segmentation-driven 6D Object Pose Estimation等等</p>\n<p><a href=\"https://blog.csdn.net/yong_qi2015/article/details/117004423?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.pc_relevant_default&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=2\">物体6-Dof pose estimation主流方法汇总_3D视觉工坊-CSDN博客</a></p>\n<p>Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image</p>\n<h2 id=\"经典特征点检测算法\"><a href=\"#经典特征点检测算法\" class=\"headerlink\" title=\"经典特征点检测算法\"></a>经典特征点检测算法</h2><p><a href=\"https://www.cnblogs.com/multhree/p/11296945.html\">sift、surf、orb 特征提取及最优特征点匹配 - 闽A2436 - 博客园 (cnblogs.com)</a></p>\n<p>如果对计算实时性要求非常高，可选用ORB算法，但基本要保证正对拍摄；如果对实行性要求稍高，可以选择SURF；基本不用SIFT。</p>\n<p><a href=\"https://www.bilibili.com/video/BV1Qb411W7cK?p=4\">6.SIFT(尺度不变特征变换)_哔哩哔哩_bilibili</a></p>\n<p><a href=\"https://blog.csdn.net/dcrmg/article/details/52601010\">Surf算法特征点检测与匹配_牧野的博客-CSDN博客_surf算法</a></p>\n<p><a href=\"https://blog.csdn.net/zizi7/article/details/50379973/\">最大稳定极值区域（MSER）检测_zizi7的专栏-CSDN博客_mser算法</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/261966288\">图像特征算法(三)——ORB算法简述及Python中ORB特征匹配实践 - 知乎 (zhihu.com)</a></p>\n<p>使用传统的方法进行单目相机位姿估计：<a href=\"https://github.com/nanfeng-dada/pose_estimation\">GitHub - nanfeng-dada/pose_estimation: 单目位姿估计，传统机器视觉，opencv pnp算法</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/74597564\"></a><a href=\"https://www.cnblogs.com/wangguchangqing/p/8287585.html\">SLAM入门之视觉里程计(5)：单应矩阵 - Brook_icv - 博客园 (cnblogs.com)</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220217110036174.png\" alt=\"image-20220217110036174\"></p>\n<p>文博分享：</p>\n<p><a href=\"https://github.com/Awesome-Image-Registration-Organization\">Awesome-Image-Registration-Organization · GitHub</a></p>\n<p>医学上的应用<a href=\"https://github.com/fabio86d/HipHop_2D3Dregistration\">https://github.com/fabio86d/HipHop_2D3Dregistration</a></p>\n<p><a href=\"https://blog.csdn.net/dbdxnuliba/article/details/108215073\">高翔博士slam课程深度图像数据除以5000的含义<em>dbdxnuliba的博客-CSDN博客</em>高翔博士slam</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RiZHhudWxpYmE=,size_16,color_FFFFFF,t_70.png\" alt=\"img\"></p>\n<h3 id=\"位姿的刚体变换与坐标系转换-–-数值之刃-MathSword\"><a href=\"#位姿的刚体变换与坐标系转换-–-数值之刃-MathSword\" class=\"headerlink\" title=\"位姿的刚体变换与坐标系转换 – 数值之刃 MathSword\"></a><a href=\"http://www.mathsword.com/rigidtrans_coordinatetrans/\">位姿的刚体变换与坐标系转换 – 数值之刃 MathSword</a></h3><h3 id=\"经典的单目RGB视觉跟踪算法，没有深度学习各种网络\"><a href=\"#经典的单目RGB视觉跟踪算法，没有深度学习各种网络\" class=\"headerlink\" title=\"经典的单目RGB视觉跟踪算法，没有深度学习各种网络\"></a>经典的单目RGB视觉跟踪算法，没有深度学习各种网络</h3><p>输入一个包含目标物体的序列帧（offline的视频序列或相机输入视频流），然后还需要一个目标物体的三维模型。</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/102024694?utm_source=wechat_timeline\">啥是Region-based实时单目RGB三维(刚性)物体跟踪（一） - 知乎 (zhihu.com)</a></p>\n<h3 id=\"Sparse-Representation-for-3D-Shape-Estimation-A-Convex-Relaxation-Approach\"><a href=\"#Sparse-Representation-for-3D-Shape-Estimation-A-Convex-Relaxation-Approach\" class=\"headerlink\" title=\"Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach\"></a>Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach</h3><p>现有问题：</p>\n<p>large number of acquisitions from multiple views are often required in order to obtain a complete 3D model, which is not preferred in some real-time applications; the depth sensors in general cannot work outdoor and have a limited sensing range;</p>\n<h2 id=\"MediaPipe\"><a href=\"#MediaPipe\" class=\"headerlink\" title=\"MediaPipe\"></a>MediaPipe</h2><p><a href=\"https://google.github.io/mediapipe/solutions/objectron#model_name\">https://google.github.io/mediapipe/solutions/objectron#model_name</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=f-Ibri14KMY&amp;t=231s\">https://www.youtube.com/watch?v=f-Ibri14KMY&amp;t=231s</a></p>\n<p><img src=\"/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220310162325247.png\" alt=\"image-20220310162325247\"></p>\n<h2 id=\"大组会——刚体6D位姿估计\"><a href=\"#大组会——刚体6D位姿估计\" class=\"headerlink\" title=\"大组会——刚体6D位姿估计\"></a>大组会——刚体6D位姿估计</h2><p><a href=\"https://www.bilibili.com/video/BV1fa411c7hc?spm_id_from=333.337.search-card.all.click\">基于深度学习的特征匹配与位姿估计+端-云协同的AR技术与平台_哔哩哔哩_bilibili</a></p>\n<p><a href=\"https://www.xzhou.me/\">Xiaowei Zhou’s Homepage (xzhou.me)</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1C34y1S7ik/\">基于深度学习的特征点提取，特征点检测的方法总结_哔哩哔哩_bilibili</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1bA411E7Y9?p=6\">【官方字幕】“3D几何与视觉技术”全球在线研讨会_哔哩哔哩_bilibili</a></p>\n<h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p><a href=\"https://blog.csdn.net/dsoftware/article/details/106101681\">(21条消息) 物体6D位姿的含义<em>Guoguang Du的博客-CSDN博客</em>物体6d位姿</a></p>\n<h2 id=\"ORB-SLAM\"><a href=\"#ORB-SLAM\" class=\"headerlink\" title=\"ORB-SLAM\"></a>ORB-SLAM</h2><p>BA优化存在实时性问题</p>\n<p>要实时BA的话，需要提供以下条件：</p>\n<ol>\n<li>关键帧的关键点匹配</li>\n<li>关键帧的数量要尽可能少，避免冗余</li>\n<li>关键帧要具有显著的视差和大量的回环匹配</li>\n<li>关键帧位姿的初始化以及关键点位置的初始化</li>\n<li>A local map in exploration where optimization is focused to achieve scalability.</li>\n<li>The ability to perform fast global optimizations (e.g., pose graph) to close loops in real time</li>\n</ol>\n","categories":["位姿估计"],"tags":["Pose Estimation"]},{"title":"飞机6D位姿估计","url":"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/","content":"<p>数据集不是问题，只需要模型，每张图片的姿态，mask（这个可以由前两者知道）</p>\n<p>可以用来制作数据集<a href=\"https://github.com/3D-FRONT-FUTURE/3D-FUTURE-ToolBox#model-aligned-rendering\">GitHub - 3D-FRONT-FUTURE/3D-FUTURE-ToolBox: FUTURE3D Toolbox: Rendering, Projection, and Re-Projection</a></p>\n<p>opencv姿态估计</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220218151001060.png\" alt=\"image-20220218151001060\"></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220218151033771.png\" alt=\"image-20220218151033771\"></p>\n<h2 id=\"相机参数-（opencv-是竖着来的）\"><a href=\"#相机参数-（opencv-是竖着来的）\" class=\"headerlink\" title=\"相机参数 （opencv 是竖着来的）\"></a>相机参数 （opencv 是竖着来的）</h2><p><strong>牛博相机</strong></p>\n<p>内参</p>\n<p>792.216153833603    0    0<br>0    791.177146289919    0<br>308.699841948659    229.681830886849    1</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220219165155403.png\" alt=\"image-20220219165155403\"></p>\n<p>畸变系数</p>\n<p>​      cameraParams.RadialDistortion                          0.159289987523910    -0.944011768029619</p>\n<p>​      cameraParams.TangentialDistortion                  0    0</p>\n<p>​      cameraParams.EstimateTangentialDistortion   0</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222191218258.png\" alt=\"image-20220222191218258\"></p>\n<p><strong>涛哥相机</strong></p>\n<p>内参</p>\n<p>1312.06174094713    0    0<br>0    1309.31954409816    0<br>916.121844560839    518.441493882532    1</p>\n<p>畸变系数</p>\n<p>​    cameraParams.RadialDistortion                          -0.369910731594361    0.123663102946957</p>\n<p>​      cameraParams.TangentialDistortion                  0    0</p>\n<p>​      cameraParams.EstimateTangentialDistortion  0</p>\n<p><strong>工业相机</strong></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220306194950707.png\" alt=\"image-20220306194950707\"></p>\n<p><a href=\"https://www.cnblogs.com/gooutlook/p/15240265.html\">https://www.cnblogs.com/gooutlook/p/15240265.html</a></p>\n<p>然后就可以用cv2.VideoCapture()运行了</p>\n<p>12mm镜头：</p>\n<p>内参</p>\n<p>3603.41629066370                               0                                              0<br>0                                                          3598.94564763421                   0<br>717.982210742481                          516.831199201901                   1</p>\n<p>畸变系数</p>\n<p>cameraParams.RadialDistortion          0.125212949613634    -0.596274067781849</p>\n<p>cameraParams.TangentialDistortion                  0                                  0</p>\n<p>cameraParams.EstimateTangentialDistortion  0</p>\n<p>6mm镜头：</p>\n<p>内参：</p>\n<p>1839.64945443219    0    0<br>0    1839.25847451371    0<br>675.840419167291    550.401546804488    1</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220716191300260\">/image-20220716191300260.png)</p>\n<p>畸变系数：</p>\n<p>cameraParams.RadialDistortion         0.0154582469945801    -0.0450341498973294</p>\n<p>cameraParams.TangentialDistortion                  0                                  0</p>\n<p>cameraParams.EstimateTangentialDistortion  0</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220716191529975\">/image-20220716191529975.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220716191619052\">/image-20220716191619052.png)</p>\n<p><strong>手机相机</strong></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220429103711210\">/image-20220429103711210.png)</p>\n<p>2304.000000, 2304.000000, 960.000000, 540.000000, 0.000000, 0.000000, 0.000000, 0.000000</p>\n<p>opencv格式：</p>\n<p>2304.000000        0        960.000000    0     2304.000000    540.000000   0     0       1 </p>\n<p>python调用手机ip摄像头<a href=\"https://www.csdn.net/tags/MtjakgxsMDQ3Ni1ibG9n.html\">(42条消息) python调用手机摄像头 - CSDN</a></p>\n<h2 id=\"特征点提取测试\"><a href=\"#特征点提取测试\" class=\"headerlink\" title=\"特征点提取测试\"></a>特征点提取测试</h2><h3 id=\"sift\"><a href=\"#sift\" class=\"headerlink\" title=\"sift\"></a>sift</h3><p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220219171551879.png\" alt=\"image-20220219171551879\"></p>\n<p>匹配成功的特征点数目为：142，筛选后的特征点数目为：60,ransac后的特征点数目为：28<br>时间：0.240354s</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222094720459.png\" alt=\"image-20220222094720459\"></p>\n<h3 id=\"orb\"><a href=\"#orb\" class=\"headerlink\" title=\"orb\"></a>orb</h3><p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220219171614674.png\" alt=\"image-20220219171614674\"></p>\n<p>匹配成功的特征点数目为：468，筛选后的特征点数目为：180,ransac后的特征点数目为：75<br>时间：0.237791s</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222094802357.png\" alt=\"image-20220222094802357\"></p>\n<p>3dmodel的细节太少，完全配不准</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222093435654.png\" alt=\"image-20220222093435654\"></p>\n<h3 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h3><p>基于对应关系的方法主要针对纹理丰富的目标物体，首先将需要计算位姿的目标物体的3D模型投影到N个角度，得到N张2D模板图像，记录这些模板图上2D像素和真实3D点的对应关系。当单视角相机采集到RGB图像后，通过特征提取（SIFT，FAST，ORB等），寻找特征点与模板图片之间的对应关系。通过这种方式，可以得到当前相机采集图像的2D像素点与3D点的对应关系。最后使用PnP算法即可恢复当前视角下图像的位姿。</p>\n<p>如果目标物体有丰富纹理，可以寻找当前目标2D特征点与模版图像中特征点的对应，而模板图像的特征点具有3D坐标，这样就可以使用PNP算法得到物体6D位姿了；具体可以参考ORB-SLAM2，先构建场景的稀疏3D点，同时保存一些关键帧，关键帧上的2D ORB特征点对应3D点，这样可以对当前目标图像，寻找最相似关键帧，然后根据ORB特征点的匹配使用PNP得到粗位姿(重定位)，也可以再用BA优化；如果想要去除背景干扰，可以检测出目标物体再匹配；</p>\n<p>2.如果目标是弱纹理或者没有纹理，那么可以利用轮廓匹配，此时模板图像可以是轮廓图，可以看看LineMode或者一些基于草图的3D模型检索算法，将当前目标提取的轮廓图和已有的模版轮廓图匹配，寻找最相似轮廓图对应的6D位姿；Vuforia有物体3D检测跟踪的Demo，变换视角使当前目标的轮廓和预先设定的轮廓重合，表示得到对应的位姿，再进行跟踪；3.基于RGB图像获得物体的深度图，转为点云和已有3D模型的点云进行粗配准和细配准，单目的有很多基于深度学习的算法，双目的可以自己算；4.如果能够具有高逼真3D模型，可以基于仿真环境生成大量虚拟6D位姿训练集，再在少量真实采集的6D位姿数据集上FineTune，应该是也可以的；你看情况试一试，方法的鲁棒性得看目标物体本身的特性以及具体应用场景的复杂程度吧；</p>\n<p>采集数据（包括RGB和深度图）</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222165734657.png\" alt=\"image-20220222165734657\"></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222210155972.png\" alt=\"image-20220222210155972\"></p>\n<p>杂乱背景下</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220222211656964.png\" alt=\"image-20220222211656964\"></p>\n<ol>\n<li>RGB-D 图像中的rgb图片提供了像素坐标系下的x，y坐标，而深度图直接提供了相机坐标系下的𝑍坐标，也就是相机与点的距离。</li>\n<li>根据 RGB-D 图像的信息和相机的内参，可以计算出任何一个像素点在<strong>相机坐标系</strong>下的坐标。</li>\n<li>根据 RGB-D 图像的信息和相机的内参与外参，可以计算出任何一个像素点在世界坐标系下的坐标。</li>\n<li>相机视野范围内，相机坐标系下的障碍物点的坐标，就是<a href=\"https://so.csdn.net/so/search?q=点云&amp;spm=1001.2101.3001.7020\">点云</a>传感器数据，也就是相机坐标系下的点云数据。点云传感器数据可以根据 RGB-D 图像提供的坐标与相机内参算出来。</li>\n<li>所有世界坐标系下的障碍物点的坐标，就是点云地图数据，也就是世界坐标系下的点云数据。点云地图数据可以根据RGB-D 图像提供的坐标与相机内参和外参算出来。</li>\n</ol>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220224172757631.png\" alt=\"image-20220224172757631\"></p>\n<p>svm分类 2d-3d配准</p>\n<h3 id=\"将图像沿x轴翻转后验证匹配效果-翻转不同于旋转\"><a href=\"#将图像沿x轴翻转后验证匹配效果-翻转不同于旋转\" class=\"headerlink\" title=\"将图像沿x轴翻转后验证匹配效果(翻转不同于旋转)\"></a>将图像沿x轴翻转后验证匹配效果(翻转不同于旋转)</h3><p>orb+单应</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220301103819128.png\" alt=\"image-20220301103819128\"></p>\n<p>sift+单应 （效最好）</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220301104133380.png\" alt=\"image-20220301104133380\"></p>\n<p>sift+基础矩阵（误配太多）</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220301104729842.png\" alt=\"image-20220301104729842\"></p>\n<p>经测试，sift+单应ransac效果最好</p>\n<p>flann匹配效果还不如暴力匹配，且时间也不会快很多，如下为flann匹配效果</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220301140828199.png\" alt=\"image-20220301140828199\"></p>\n<p>ransac 次数太少导致算的不准确</p>\n<p><a href=\"https://github.com/Celebrandil/CudaSift\">Celebrandil/CudaSift: A CUDA implementation of SIFT for NVidia GPUs (1.2 ms on a GTX 1060) (github.com)</a></p>\n<p>ransac算法当匹配不到的时候会出现这种情况</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220330153303463\">/image-20220330153303463.png)</p>\n<p>或许自己编写ransac，可以排除多点汇聚一点的情况</p>\n<p><a href=\"https://github.com/hughesj919/HomographyEstimation/blob/master/Homography.py\">HomographyEstimation/Homography.py at master · hughesj919/HomographyEstimation (github.com)</a></p>\n<h3 id=\"LoFTR-效果\"><a href=\"#LoFTR-效果\" class=\"headerlink\" title=\"LoFTR 效果\"></a>LoFTR 效果</h3><p><a href=\"https://colab.research.google.com/drive/1BgNIOjFHauFoNB95LGesHBIjioX74USW?usp=sharing#scrollTo=BSBHOV3GFUc3\">LoFTR_demo_single_pair.ipynb - Colaboratory (google.com)</a></p>\n<p>outdoor_ds</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/计算机视觉/位姿估计/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220810150819782\">/image-20220810150819782.png)</p>\n<p>indoor_ds_new</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/计算机视觉/位姿估计/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220810150737221\">/image-20220810150737221.png)</p>\n<pre class=\" language-lang-python\"><code class=\"language-lang-python\"># Draw \n# M, mask = cv2.findHomography(mkpts0, mkpts1, method=cv2.RANSAC, ransacReprojThreshold=1.0,confidence=0.9999,maxIters=10000)\nM, mask = cv2.findFundamentalMat(mkpts0, mkpts1, method=cv2.RANSAC, ransacReprojThreshold=1.0,confidence=0.9999,maxIters=10000)\nprint(mkpts0.shape)\nkps0 = []\nkps1 = []\nmconf_1 = []\nfor i in range(len(mask)):\n    if mask[i] == 1:\n        kps0.append(mkpts0[i])\n        kps1.append(mkpts1[i])\n        mconf_1.append(mconf[i])\nmkpts0 = np.float32(kps0).reshape(-1,2)\nmkpts1 = np.float32(kps1).reshape(-1,2)\nmconf = mconf_1\nprint(mkpts0.shape)\nprint(mconf)\ncolor = cm.jet(mconf, alpha=0.7)\ntext = [\n    'LoFTR',\n    'Matches: {}'.format(len(mkpts0)),\n]\nfig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text)\n\n# A high-res PDF will also be downloaded automatically.\nmake_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text, path=\"LoFTR-colab-demo.pdf\")\nfiles.download(\"LoFTR-colab-demo.pdf\")\n</code></pre>\n<p>不加ransac</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722110121475\">/image-20220722110121475.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722105527038\">/image-20220722105527038.png)</p>\n<p>加ransac</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722105557805\">/image-20220722105557805.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"Image\">/E-7cRlXXEAQm4rP)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"Image\">/E-7fUp5WEAAda8S)</p>\n<p>对于旋转图像不行</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805141445728\">/image-20220805141445728.png)</p>\n<p>ransac后</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805141537159\">/image-20220805141537159.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805141954237\">/image-20220805141954237.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805141937782\">/image-20220805141937782.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805142217265\">/image-20220805142217265.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805142224530\">/image-20220805142224530.png)</p>\n<p>对于upright图像效果就很好</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220805142459812\">/image-20220805142459812.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/选区_015.png\" alt=\"选区_015\" style=\"zoom: 20%;\"><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/选区_016-1659870333888.png\" alt=\"选区_016\" style=\"zoom: 20%;\"></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807190447071\">/image-20220807190447071.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807190323929\">/image-20220807190323929.png)</p>\n<p>旋转测试 10°</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807191915518\">/image-20220807191915518.png)</p>\n<p>20°</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807192011326\">/image-20220807192011326.png)</p>\n<p>30°</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807193601106\">/image-20220807193601106.png)</p>\n<p>40°</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807193249541\">/image-20220807193249541.png)</p>\n<p>单应矩阵</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807193446637\">/image-20220807193446637.png)</p>\n<p>基础矩阵</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807200756731\">/image-20220807200756731.png)</p>\n<p>50°</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807193851167\">/image-20220807193851167.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807193858105\">/image-20220807193858105.png)</p>\n<p>60°</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807193951562\">/image-20220807193951562.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807194001962\">/image-20220807194001962.png)</p>\n<p>orb具有旋转不变性</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807201347923\">/image-20220807201347923.png)</p>\n<p>其他视角测试</p>\n<p>感觉下面这张图，一些语义上的明显特征并没有检测出来，比如转角处</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807195418356\">/image-20220807195418356.png)</p>\n<p>用findFundamentalMat后还行</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807200350733\">/image-20220807200350733.png)</p>\n<p>orb效果不行</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220807202758873\">/image-20220807202758873.png)</p>\n<h3 id=\"orb的测试\"><a href=\"#orb的测试\" class=\"headerlink\" title=\"orb的测试\"></a>orb的测试</h3><p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722105926685\">/image-20220722105926685.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722110035632\">/image-20220722110035632.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722110443971\">/image-20220722110443971.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722110533388\">/image-20220722110533388.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722110213392\">/image-20220722110213392.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220722110326300\">/image-20220722110326300.png)</p>\n<h3 id=\"可以尝试下面的方法\"><a href=\"#可以尝试下面的方法\" class=\"headerlink\" title=\"可以尝试下面的方法\"></a>可以尝试下面的方法</h3><p><a href=\"https://www.cnblogs.com/shuimuqingyang/p/14428270.html\">修改OpenCV一行代码，提升14%图像匹配效果 - 水木清扬 - 博客园 (cnblogs.com)</a></p>\n<p><a href=\"https://github.com/iago-suarez/beblid-opencv-demo/blob/main/demo.py\">beblid-opencv-demo/demo.py at main · iago-suarez/beblid-opencv-demo (github.com)</a></p>\n<p>经过测试，该描述子（BEBLID）效果比ORB更好，匹配对数更多，且计算更快</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220325134532890\">/image-20220325134532890.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220325134541777\">/image-20220325134541777.png)</p>\n<h2 id=\"制作6D姿态数据集\"><a href=\"#制作6D姿态数据集\" class=\"headerlink\" title=\"制作6D姿态数据集\"></a>制作6D姿态数据集</h2><p><a href=\"https://github.com/DLR-RM/BlenderProc\">https://github.com/DLR-RM/BlenderProc</a></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/AI/飞机姿态估计/image-20220303204720768.png\" alt=\"image-20220303204720768\"></p>\n<p>While distance and depth images sound similar, <strong>they are not the same</strong>: In <a href=\"https://en.wikipedia.org/wiki/Range_imaging\">distance images</a>, each pixel contains the actual distance from the camera position to the corresponding point in the scene. In <a href=\"https://en.wikipedia.org/wiki/Depth_map\">depth images</a>, each pixel contains the distance between the camera and the plane parallel to the camera which the corresponding point lies on.</p>\n<p>在距离图像中，每个像素包含从相机位置到场景中相应点的实际距离。</p>\n<p>在深度图像中，每个像素包含相机和平行于相机的平面之间的距离，对应点位于该平面上。</p>\n<p>要算PNP应该用 <strong>depth image</strong></p>\n<p>在bop_object_pose_sampling中ply格式带颜色可以直接显示颜色</p>\n<pre><code>blenderproc run examples/datasets/bop_object_on_surface_sampling/main.py \n              &lt;path_to_bop_data&gt; \n              &lt;bop_dataset_name&gt; \n              resources/cctextures \n              examples/datasets/bop_object_on_surface_sampling/output\n\nblenderproc run examples/datasets/bop_object_on_surface_sampling/main.py /media/young/young/dataset/linemod lm cc1 examples/datasets/bop_object_on_surface_sampling/output\n</code></pre><p>blenderproc run blenderproc/scripts/download_cc_textures_1.py cc1</p>\n<pre><code>blenderproc run examples/datasets/bop_challenge/config_lm_upright.yaml /media/young/young/dataset/linemod lm /home/young/code/bop_toolkit cc1 examples/datasets/bop_challenge/output\n</code></pre><p>blenderproc run blenderproc/scripts/download_cc_textures_1.py cc1</p>\n<p>​              </p>\n<pre><code>blenderproc run examples/datasets/bop_challenge/config_lm_upright.yaml /media/young/young/dataset/linemod lm /home/young/code/bop_toolkit cc1 examples/datasets/bop_challenge/output\nblenderproc run examples/datasets/bop_challenge/main_lm_upright.py /media/young/young/dataset/linemod cc1  examples/datasets/bop_challenge/output --num_scenes=1\n\nblenderproc run examples/datasets/bop_challenge/main_tless_random_test.py /media/young/young/dataset/linemod cc1  examples/datasets/bop_challenge/output2 --num_scenes=1\n</code></pre><p><strong>进入debug模式</strong></p>\n<pre><code>import pydevd_pycharm\npydevd_pycharm.settrace('localhost', port=12345, stdoutToServer=True, stderrToServer=True)\n</code></pre><h4 id=\"生成自定义数据集步骤\"><a href=\"#生成自定义数据集步骤\" class=\"headerlink\" title=\"生成自定义数据集步骤\"></a>生成自定义数据集步骤</h4><p>1.在blenderproc主目录下运行（base环境）<strong>生成深度图和RGB</strong></p>\n<p>blenderproc run examples/datasets/bop_challenge/main_tless_random_test.py /media/young/young/dataset/linemod cc1  examples/datasets/bop_challenge/output2 —num_scenes=1</p>\n<p>然后在生成的数据集目录下加入模型文件</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220307221151331.png\" alt=\"image-20220307221151331\" style=\"zoom:50%;\"></p>\n<p>2.运行calc_gt_masks.py（test环境）在/home/young/code/bop_toolkit/下  <strong>生成mask</strong></p>\n<p>python calc_gt_masks.py</p>\n<h4 id=\"后续安排\"><a href=\"#后续安排\" class=\"headerlink\" title=\"后续安排\"></a>后续安排</h4><ol>\n<li>选择合适的算法并测试效果，最好是能直接接入视频流的</li>\n<li>自己用数据集训练</li>\n<li>训练测试效果</li>\n<li>思考改进</li>\n</ol>\n<p>▪</p>\n<p>▪ 测试传统方案模板切换效果</p>\n<p>▪测试传统方案模板切换效果，拍摄了从飞机头向机尾方向拍摄的十几张模板图像，根据匹配的数量来切换模板：当匹配数低于一定数量时，切换模板，遍历所有模板并选择匹配数最高的那一个作为当前模板。</p>\n<p>▪ 研究svm识别特征点的方案</p>\n<p>▪1. 数据集制作：先用sift提取特征点，每张图手动选取空间位置相同的N个关键点，在这些关键点周围选择一个patch，即每张图选择N个以关键点为中心的patch</p>\n<p>▪2. 训练N个SVM对来识别输入的patch是不是属于这一类</p>\n<p>▪3. 最终测试时也是先提取特征点周围的patch作为SVM的输入，判断是否属于某一类关键点</p>\n<p>▪4. 根据识别出的关键点进行pnp解算位姿</p>\n<h2 id=\"有价值的参考\"><a href=\"#有价值的参考\" class=\"headerlink\" title=\"有价值的参考\"></a>有价值的参考</h2><p><a href=\"https://www.youtube.com/watch?v=iIAoc99doI4\">Object pose estimation and tracking using OpenCV APIs. - YouTube</a></p>\n<p>核心思想：</p>\n<p>1.Capture the scene from two views.2. Estimate the sift features in both image. 3. Pair the matching features and using triangulation, compute the corresponding 3D coordinates wrt local object frame. 4. Now you have object image, its Sift features and descriptors, and corresponding to each features you have its 3D coordinates.  5. Compute the sift features on new image and compare with the saved features and you will have your object pose in new image.</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220326100404001.png\" alt=\"image-20220326100404001\" style=\"zoom:30%;\"></p>\n<p><a href=\"https://www.youtube.com/watch?v=UJtpBxdDVDU\">[IROS’12] 3D Textureless Object Detection and Tracking: An Edge-based Approach - YouTube</a></p>\n<p>传统的基于边缘的无纹理姿态估计（有空可以参考一下）</p>\n<p><a href=\"https://www.youtube.com/watch?v=lbgl2u6KrDU\">Measure the size of an object | with Opencv, Aruco marker and Python - YouTube</a></p>\n<p>通过使用Aruco marker来进行物体的尺寸估计(矩形物体)</p>\n<p>其实和姿态估计关联性不大，相机只能正对，通过检测标志，进行等比例的尺寸估计</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220326100539423.png\" alt=\"image-20220326100539423\" style=\"zoom:25%;\"></p>\n<p><a href=\"https://www.youtube.com/watch?v=Eq2WNv1blfc\">OpenCV tutorial: Real Time pose estimation of a textured object (BRISK) - YouTube</a></p>\n<p>opencv官方实例，c++实现</p>\n<p><a href=\"https://www.youtube.com/watch?v=US9p9CL9Ywg\">Pose Estimation of Objects in OpenCV Python - YouTube</a></p>\n<p>估计棋盘格的位姿，不过默认深度为零，值得参考</p>\n<p><a href=\"https://www.youtube.com/watch?v=LjjJQ81RbX0\">Optical Flow Tracking Grid and its use for Real-Time Object Detection - YouTube</a></p>\n<p>光流法进行物体实时跟踪</p>\n<p><strong>这个代码的思想可以借鉴，以及写代码的格式可以学习</strong></p>\n<p><a href=\"https://github.com/GigaFlopsis/image_pose_estimation/blob/63926444e53b227d3e65ca9a27f4b2512ee72386/src/image_processing.py#L150\">image_pose_estimation/image_processing.py at 63926444e53b227d3e65ca9a27f4b2512ee72386 · GigaFlopsis/image_pose_estimation (github.com)</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=V6yGv8Z46hM\">Ros Image Pose Estimation with blur detector - YouTube</a></p>\n<p><strong>这个人的视频挺好的</strong>    不过主要是了解一些库的使用</p>\n<p><a href=\"https://www.youtube.com/watch?v=US9p9CL9Ywg\">Pose Estimation of Objects in OpenCV Python - YouTube</a></p>\n<p><a href=\"https://github.com/niconielsen32/ComputerVision\">niconielsen32/ComputerVision (github.com)</a></p>\n<p>飞机图像分割</p>\n<p><a href=\"https://github.com/rguitar96/airplane-segmentation/blob/main/image-segmentation.ipynb\">airplane-segmentation/image-segmentation.ipynb at main · rguitar96/airplane-segmentation (github.com)</a></p>\n<p><strong>orb-slam值得借鉴</strong></p>\n<p><a href=\"https://github.com/raulmur/ORB_SLAM/blob/master/src/Initializer.cc\">ORB_SLAM/Initializer.cc at master · raulmur/ORB_SLAM (github.com)</a></p>\n<p><a href=\"https://github.com/borgwang/orb-slam-py/blob/master/vo.py\">orb-slam-py/vo.py at master · borgwang/orb-slam-py (github.com)</a></p>\n<h2 id=\"计算单应分数\"><a href=\"#计算单应分数\" class=\"headerlink\" title=\"计算单应分数\"></a>计算单应分数</h2><p>symmetric transfer errors<a href=\"https://blog.csdn.net/sinat_17496535/article/details/51673285\">(23条消息) 重投影误差与对称转移误差<em>MeJnCode的博客-CSDN博客</em>重投影误差</a></p>\n<p>matchsocre</p>\n<p>​    91530.96435108903</p>\n<p>knnmatchscore</p>\n<p>​    126337.44244072789      34.97s      兼具速度和分数    </p>\n<p>bfsocre</p>\n<p>​    131447.43567962793  速度最慢但是分数高</p>\n<p>灰度+knnmatch        用灰度图效果貌似和彩色差不多</p>\n<p>total_score: 128866.53040396319<br>总时间：34.372146s</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220330105008400\">/image-20220330105008400.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220330170325486\">/image-20220330170325486.png)</p>\n<p>以上的情况要么是初步筛选不够，要么是点选的太多了，</p>\n<p>飞机轮廓</p>\n<p>sfm</p>\n<p>算rotation</p>\n<pre><code>orb.detect_and_compute_beblid_knn(max_temp_pic,input_image,0.93,ransacReprojThreshold=2,confidence=0.95,maxIters=10000)\ntotal_score: 108451.39269577163\n\n orb.detect_and_compute_beblid_bf(max_temp_pic,input_image,3,ransacReprojThreshold=2,confidence=0.95,maxIters=10000)\ntotal_score: 94354.40717807441\n\norb.detect_and_compute_orb_bf(max_temp_pic,input_image,3,ransacReprojThreshold=2,confidence=0.95,maxIters=10000)\ntotal_score: 49999.934895973216\n\n\nself.orb = cv.ORB_create(extract_number, 1.2, 8, 10, 0, 2, cv.ORB_HARRIS_SCORE, 10, 20)\nself.descriptor = cv.xfeatures2d.BEBLID_create(2)\ntotal_score: 115350.02440894888\n</code></pre><h2 id=\"三维重建\"><a href=\"#三维重建\" class=\"headerlink\" title=\"三维重建\"></a>三维重建</h2><p>完整流程参考<a href=\"https://www.youtube.com/watch?v=bDHJM6nAKtc\">Creating 3D Game Models from Video using Photogrammetry - YouTube</a></p>\n<h3 id=\"colmap\"><a href=\"#colmap\" class=\"headerlink\" title=\"colmap\"></a>colmap</h3><p><a href=\"https://icode.best/i/80715147507186\">【踩坑】colmap中的相机位姿定义及其可视化结果的隐含转换-爱代码爱编程 (icode.best)</a></p>\n<p><a href=\"https://www.jianshu.com/p/94196a92cc18\">SIFT特征详解 - 简书 (jianshu.com)</a></p>\n<p>使用17张图进行3维重建 <img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220224200905982.png\" alt=\"image-20220224200905982\"></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/AI/飞机姿态估计.assets/2022-02-28 16-49-13 的屏幕截图.png\" alt=\"2022-02-28 16-49-13 的屏幕截图\"></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/data/blog/AI/飞机姿态估计.assets/2022-02-28 16-50-05 的屏幕截图.png\" alt=\"2022-02-28 16-50-05 的屏幕截图\"></p>\n<p><a href=\"https://colmap.github.io/format.html\">Output Format — COLMAP 3.8 documentation</a></p>\n<p><a href=\"https://blog.csdn.net/GarfieldEr007/article/details/50333157\">(25条消息) 尺度不变特征转换SIFT_GarfieldEr007的博客-CSDN博客</a></p>\n<p>读取colmap生成的文件信息：</p>\n<p><a href=\"https://github.com/SBCV/Blender-Addon-Photogrammetry-Importer/blob/f79c09ea9eda34574d20ed05ceda9226ce7f778c/photogrammetry_importer/ext/read_write_model.py\">Blender-Addon-Photogrammetry-Importer/read_write_model.py at f79c09ea9eda34574d20ed05ceda9226ce7f778c · SBCV/Blender-Addon-Photogrammetry-Importer (github.com)</a></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220422114317283.png\" alt=\"image-20220422114317283\" style=\"zoom:50%;\"></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220429111227223.png\" alt=\"image-20220429111227223\" style=\"zoom:50%;\"></p>\n<p>black plane 关键点3d坐标：</p>\n<p>-0.750285, 1.89654, 2.12803</p>\n<p>2.09217, 2.07706, 3.77801</p>\n<p>3.06018, 1.19712, 5.24177</p>\n<p>3.62335, 1.34729, 5.57277</p>\n<p>3.14614, 2.24611, 4.19743</p>\n<p>4.50369, 1.61071, 3.76958（可能不准）</p>\n<p>4.71773, 2.53198, 2.84707</p>\n<p>3.36851, 3.18192, 3.22556</p>\n<p>4.18717, 4.45633, 2.43622</p>\n<p>3.66017, 4.333, 2.20296</p>\n<p>2.20928, 2.97569, 2.84228</p>\n<p>blue plane 关键点3d坐标：</p>\n<p>-0.280649, 1.68422, 2.36646</p>\n<p>2.54254, 1.73229, 3.39873</p>\n<p>3.73118, 0.98228, 4.81425</p>\n<p>4.29214, 1.09763, 4.95887</p>\n<p>3.77839, 1.93858, 3.73114</p>\n<p>4.86345, 1.44302, 3.34977</p>\n<p>4.89913, 2.31779, 2.45851</p>\n<p>3.80463, 2.78573, 2.84414</p>\n<p>4.49756, 3.98496, 2.07535</p>\n<p>3.95248, 3.8895, 1.9103</p>\n<p>2.59718, 2.55167, 2.60149</p>\n<h3 id=\"opencv-sfm\"><a href=\"#opencv-sfm\" class=\"headerlink\" title=\"opencv sfm\"></a>opencv sfm</h3><p><a href=\"https://github.com/Ashok93/Structure-From-Motion-SFM-\">Ashok93/Structure-From-Motion-SFM-: Structure from Motion (Sfm) in Python using OpenCV (github.com)</a></p>\n<p><a href=\"https://github.com/CesarAsturias/SFM-and-numerical-optimization\">CesarAsturias/SFM-and-numerical-optimization: Structure from Motion with Python and OpenCV, using numerical optimization techniques (github.com)</a>这个代码注释不错</p>\n<p><a href=\"https://github.com/mapillary/OpenSfM\">mapillary/OpenSfM: Open source Structure-from-Motion pipeline (github.com)</a></p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/image-20220405000136370.png\" alt=\"image-20220405000136370\" style=\"zoom:50%;\"></p>\n<p>这个纯python而且有教程，可以尝试</p>\n<p><a href=\"https://github.com/muneebaadil/how-to-sfm\">muneebaadil/how-to-sfm: A self-reliant tutorial on Structure-from-Motion (github.com)</a></p>\n<h2 id=\"精度测试\"><a href=\"#精度测试\" class=\"headerlink\" title=\"精度测试\"></a>精度测试</h2><p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220517164105523\">/image-20220517164105523.png)</p>\n<h2 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h2><p><a href=\"https://opencv.org/evaluating-opencvs-new-ransacs/\">https://opencv.org/evaluating-opencvs-new-ransacs/</a>  ransac的改进</p>\n<h1 id=\"关键点检测\"><a href=\"#关键点检测\" class=\"headerlink\" title=\"关键点检测\"></a>关键点检测</h1><ul>\n<li><p><a href=\"https://blog.csdn.net/weixin_43229348/article/details/123500917\">如何使用 PyTorch 训练自定义关键点检测模型<em>求则得之，舍则失之的博客-CSDN博客</em>关键点检测模型</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/weixin_41782172/article/details/119249916?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-3-119249916-blog-123500917.pc_relevant_aa2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-3-119249916-blog-123500917.pc_relevant_aa2&amp;utm_relevant_index=4\">关键点检测数据准备和基于U-net网络的模型设计——工业组件4个关键点的检测模型<em>君名余曰正则的博客-CSDN博客</em>工业关键点检测</a></p>\n<p>  <a href=\"https://github.com/ExileSaber/KeyPoint-Detection\">ExileSaber/KeyPoint-Detection: 关键点检测的模型，包括直接使用关键点坐标作为返回值、使用heatmap作为返回值的两种模型 (github.com)</a></p>\n</li>\n<li><p><a href=\"https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/keypoint\">PaddleDetection/configs/keypoint at release/2.4 · PaddlePaddle/PaddleDetection · GitHub</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/lz0499/article/details/80978433\">OpenCV 中图像坐标系统与Python中NumPy Arrays之间的关系_大熊背的博客-CSDN博客_numpy坐标原点</a></p>\n</li>\n<li><p>数据增强<a href=\"https://github.com/DefTruth/torchlm/blob/main/docs/api/transforms.md\">torchlm/transforms.md at main · DefTruth/torchlm (github.com)</a></p>\n</li>\n</ul>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220712144612104\">/image-20220712144612104.png)</p>\n<p>torch.backends.cudnn.benchmark 可以加速网络</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220717212727457\">/image-20220717212727457.png)</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220717212735537\">/image-20220717212735537.png)</p>\n<p>Traceback (most recent call last):<br>  File “airplane<em>kps<em>pose\\main\\test_main_yolo.py”, line 11, in <module><br>  File “<frozen importlib._bootstrap=\"\">“, line 991, in _find_and_load<br>  File “<frozen importlib._bootstrap=\"\">“, line 975, in _find_and_load_unlocked<br>  File “<frozen importlib._bootstrap=\"\">“, line 671, in _load_unlocked<br>  File “PyInstaller\\loader\\pyimod03_importers.py”, line 495, in exec_module<br>  File “torch__init</frozen></frozen></frozen></module></em></em>.py”, line 135, in <module><br>    raise err<br>OSError: [WinError 126] 找不到指定的模块。 Error loading “C:\\Users\\admin\\AppData\\Local\\Temp_MEI467162\\torch\\lib\\torch_python.dll” or one of its dependencies.</module></p>\n<p>把torch_utils.py转成torch_utils.pyc后即可完美运行。(但遇到在别的电脑上不能运行的问题，最后发现与cuda的库没有加进去有关系)</p>\n<h2 id=\"打包\"><a href=\"#打包\" class=\"headerlink\" title=\"打包\"></a>打包</h2><p><a href=\"https://blog.csdn.net/qq_42811827/article/details/124035548?ops_request_misc=%7B%22request%5Fid%22%3A%22165811149016781435485512%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&amp;request_id=165811149016781435485512&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~hot_rank-2-124035548-null-null.142^v32^pc_rank_34,185^v2^control&amp;utm_term=pyinstaller打包pytorch&amp;spm=1018.2226.3001.4187\">【Pyinstaller打包Pytorch框架】<em>Sleeep</em>的博客-CSDN博客_pytorch 打包</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_41870706/article/details/101780731\">python项目（非单一.py文件）用Pyinstaller打包发布成exe，在windos上运行程序。_隔壁敲代码的王先生的博客-CSDN博客</a></p>\n<pre><code>pyinstaller test_main_yolo.py -D -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\weights -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\runs -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\\models -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\\sub_main -p  D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\\utils -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose\\main\n</code></pre><p>pyinstaller test_main_yolo.py -D -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\weights -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\runs -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\\models -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\\sub_main -p  D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main\\utils -p D:\\code\\kps_detection\\KeyPoint-Detection\\airplane_kps_pose_1\\main</p>\n<h4 id=\"问题及解决方案\"><a href=\"#问题及解决方案\" class=\"headerlink\" title=\"问题及解决方案\"></a>问题及解决方案</h4><p>spec文件如下：</p>\n<pre class=\" language-lang-python\"><code class=\"language-lang-python\"># -*- mode: python ; coding: utf-8 -*-\n\n\nblock_cipher = None\n\n\na = Analysis(\n    ['test_main_yolo.py'],  #打包的主程序\n    pathex=['D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\weights',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\runs',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\models',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\sub_main',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\utils',\n    'D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main',\n    'C:\\\\Windows\\\\System32', #这个貌似不用加\n    'C:\\\\Windows\\\\System32\\\\downlevel', #这个貌似不用加\n    'D:\\\\programs\\\\Anaconda\\\\envs\\\\pytorch\\\\Lib\\\\site-packages\\\\torch\\\\lib'], #pathex添加py文件的搜索路径，以及加一些库文件(dll)\n    binaries=[],  \n    datas=[('D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\weights\\\\best.pt','weights'),\n    ('D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\weights\\\\min_loss_7_15.pth','weights'),\n    ('D:\\\\code\\\\kps_detection\\\\KeyPoint-Detection\\\\airplane_kps_pose_1\\\\main\\\\utils\\\\torch_utils.pyc','utils')],   #datas放非python文件或非dll文件,每个括号后面的路径的根是生成的dist目录\n    hiddenimports=[],\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    [],\n    exclude_binaries=True,\n    name='test_main_yolo',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    console=True,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\ncoll = COLLECT(\n    exe,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    name='test_main_yolo',\n)\n</code></pre>\n<p>首先安装pyinstaller，推荐使用<code>pip install https://github.com/pyinstaller/pyinstaller/archive/develop.zip</code> (据说这样能装最新的版本，错误更少)</p>\n<p>切到主程序所在的目录下，使用 <code>pyinstaller -D XXX.py</code>  会打包成文件夹的形式，这样方便调试。（推荐！）  而使用-F会生成单个exe，运行时exe会解压到c盘的临时目录运行，可能会有路径问题或者运行更慢？</p>\n<p>如果提示缺少什么文件，就去修改spec文件，修改后使用pyinstaller XXX.spec 再次打包。</p>\n<p>！！！！打包好后可能在自己的电脑上可以正常运行，但在别的电脑上会遇到torch调用的问题，（涉及CUDA），类似下面的情况：</p>\n<p>OSError: [WinError 126] 找不到指定的模块。 Error loading “G:\\airplane_kps_pose_exe\\test_main_yolo\\torch\\lib\\caffe2_detectron_ops_gpu.dll” or one of its dependencies.</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220719185432781\">/image-20220719185432781.png)</p>\n<p>解决方式，通过<a href=\"https://github.com/lucasg/Dependencies\">安装</a>工具Dependens，通过这个工具，我们可以得到dll文件所需要的依赖项。如下：</p>\n<p><img src=\"/2021/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E9%A3%9E%E6%9C%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/飞机姿态估计(杨业鹏-20220310203222\" alt=\"image-20220719185746611\">/image-20220719185746611.png)</p>\n<p>发现在其他电脑上没有上述依赖，以及很多cuda相关的依赖，于是直接将本机上CUDA\\v11.1\\bin目录下的所有dll文件复制到pyinstaller打包后生成的文件中torch的lib文件夹中。</p>\n<p>再次运行可能还会报错说找不到torch的lib下的其他dll或依赖，这时直接删除掉报错的文件，可能再运行还报错，那么再删除，我删了2-3个之后可以正常运行了。</p>\n","categories":["位姿估计"]}]