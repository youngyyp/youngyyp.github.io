<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/Iron-Man-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/Iron-Man-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"youngyyp.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Satellite Pose Estimation Challenge: Dataset,Competition Design and Results 背景：为了解决地球轨道拥堵问题和延长地球静止轨道卫星的寿命，清除碎片和在轨服务等任务概念越来越受到学术界和工业界的关注。执行这些任务的一个关键是目标航天器相对于服务航天器的位置和姿态(即姿态)的可用性。相对于其他方法，使用基于单目视觉的方法对质量和">
<meta property="og:type" content="article">
<meta property="og:title" content="卫星姿态估计调研">
<meta property="og:url" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/index.html">
<meta property="og:site_name" content="young&#39;s blog">
<meta property="og:description" content="Satellite Pose Estimation Challenge: Dataset,Competition Design and Results 背景：为了解决地球轨道拥堵问题和延长地球静止轨道卫星的寿命，清除碎片和在轨服务等任务概念越来越受到学术界和工业界的关注。执行这些任务的一个关键是目标航天器相对于服务航天器的位置和姿态(即姿态)的可用性。相对于其他方法，使用基于单目视觉的方法对质量和">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101091807417.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101111534454.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220214152033019.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101113240159.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101164942148.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211022093553834.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211104204920303.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105130442853.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211111094705105.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211104204858051.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105162747501.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105162830993.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105163010801.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105185913215.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211108213040358.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109112921791.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211108211901970.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220120102831564.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206214920301.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206221242701.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206221605058.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207095921055.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208101923121.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211209163724502.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/v2-1fad96cf5150c0d3a42a8580cdead3c9_720w.jpg">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207194451711.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207194548714.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208103411496.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208104306785.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208105825232.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208105838151.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208110509594.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208110521554.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208140720710.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208141002214.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211210103613085.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211210103622008.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211214103709819.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211214103620809.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211215103709168.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105160244996.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105160316940.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105165415536.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105165427442.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109163033078.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109163041316.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143216549.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143457174.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143633683.png">
<meta property="og:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211117102805335.png">
<meta property="article:published_time" content="2021-10-20T01:49:41.000Z">
<meta property="article:modified_time" content="2023-03-11T06:09:37.755Z">
<meta property="article:author" content="young">
<meta property="article:tag" content="位姿估计">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101091807417.png">

<link rel="canonical" href="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>卫星姿态估计调研 | young's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="young's blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">young's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">慢慢来，比较快</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">30</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://raw.githubusercontent.com/youngyyp/blogpicture/master/img/image-20210212125129052.png">
      <meta itemprop="name" content="young">
      <meta itemprop="description" content="你的征途当是星辰大海">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="young's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          卫星姿态估计调研
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-20 09:49:41" itemprop="dateCreated datePublished" datetime="2021-10-20T09:49:41+08:00">2021-10-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-03-11 14:09:37" itemprop="dateModified" datetime="2023-03-11T14:09:37+08:00">2023-03-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Satellite-Pose-Estimation-Challenge-Dataset-Competition-Design-and-Results"><a href="#Satellite-Pose-Estimation-Challenge-Dataset-Competition-Design-and-Results" class="headerlink" title="Satellite Pose Estimation Challenge: Dataset,Competition Design and Results"></a>Satellite Pose Estimation Challenge: Dataset,Competition Design and Results</h2><p>背景：为了解决地球轨道拥堵问题和延长地球静止轨道卫星的寿命，清除碎片和在轨服务等任务概念越来越受到学术界和工业界的关注。执行这些任务的一个关键是目标航天器相对于服务航天器的位置和姿态(即姿态)的可用性。相对于其他方法，使用基于单目视觉的方法对质量和功率的要求很小，且系统结构更为简单。</p>
<p>难点：</p>
<ol>
<li><p>用于航天器位姿估计的数据集是缺乏的。主要原因是很难获得数以千计的具有精确注释的姿态标签的目标航天器的星载图像。<br> 此外，由于缺乏通用数据集，无法系统地评估和比较不同位姿估计算法的性能。</p>
</li>
<li><p>目标距离和背景是主要的挑战</p>
</li>
</ol>
<p>与直接位姿估计方法相比，基于透视n点(PNP)解算器的位姿估计方法具有更高的精确度。</p>
<p>相关工作：</p>
<p>数据集：</p>
<ol>
<li>SPEED：第一个公开可用的用于航天器姿态估计的机器学习数据集，最初于2019年2月发布。包括合成和真实数据集两部分（真实数据集是用相机拍的1：1模型）</li>
</ol>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101091807417.png" alt="image-20211101091807417" style="zoom:50%;"></p>
<ol>
<li><p>URSO：采用虚幻4引擎进行仿真的合成卫星图像数据集</p>
</li>
<li><p>SPEED+:</p>
<p> <img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101111534454.png" alt="image-20211101111534454"></p>
</li>
<li><p>BOP数据集？</p>
</li>
</ol>
<p>又出现了一个吊打其他所有的SPARK数据集</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220214152033019.png" alt="image-20220214152033019"></p>
<p>比赛：Kelvins Pose Estimation Challenge(KPEC)   欧洲航天局和斯坦福大学合作组织的比赛</p>
<p><a target="_blank" rel="noopener" href="https://kelvins.esa.int/satellite-pose-estimation-challenge/">Kelvins - Pose Estimation Challenge - Home (esa.int)</a></p>
<p><a target="_blank" rel="noopener" href="https://kelvins.esa.int/pose-estimation-2021/home/">Kelvins - Pose Estimation 2021 - Home (esa.int)</a></p>
<p>2021误差估计方法：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101113240159.png" alt="image-20211101113240159"></p>
<p>凯尔文姿势估计比赛的入门工具包：</p>
<p><a target="_blank" rel="noopener" href="https://gitlab.com/EuropeanSpaceAgency/speed-utils">EuropeanSpaceAgency / speed-utils · GitLab</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/janblumenkamp/esa-kelvin-pose-estimation">janblumenkamp/esa-kelvin-pose-estimation (github.com)</a></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211101164942148.png" alt="image-20211101164942148" style="zoom:70%;"></p>
<h2 id="Deep-Learning-for-Spacecraft-Pose-Estimation-from-Photorealistic-Rendering-（2020-ICRA）"><a href="#Deep-Learning-for-Spacecraft-Pose-Estimation-from-Photorealistic-Rendering-（2020-ICRA）" class="headerlink" title="Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering （2020 ICRA）"></a>Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering （2020 ICRA）</h2><p>此论文的最佳解决方案在ESA位姿挑战[5]的合成数据集上获得了<strong>第三名</strong>，在真实数据集上获得了第二名。（不依赖于pnp的最佳方案）</p>
<p>该方案：基于resnet的架构直接回归3D位置，该团队使用基于高斯混合模型的软分类来估计角度</p>
<p>前两名的方案：2D关键点回归；图像裁剪+缩放和鲁棒的PnP</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211022093553834.png" alt="image-20211022093553834"></p>
<p>该网络采用带有预先训练的权重的ResNet架构作为网络主干（backbone），为了保持空间特征分辨率，去掉了原网络的最后一个全连接层和全局平均池化层，只在第二层留下了一个池化层。全局池化层被替换为了stride为2，的3<em>3卷积层（bottleneck layer），用来压缩CNN的特征。该网络的缺点是网络本身<em>*不能处理多个对象</em></em>。</p>
<p><strong>3D位置(Location)估计</strong>是一个简单的含有两个全连接层的分支。</p>
<p>不是最小化绝对欧几里得距离，而是最小化相对误差，即下式的第1项</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211104204920303.png" alt="image-20211104204920303" style="zoom:50%;"></p>
<p><strong>旋转角度估计</strong>包括两种不同的方法，硬分类和软分类，硬分类输出一个四元数；软分类分两部分：第一个部分输出一组四元数，第二部分输出对应每个四元数的概率</p>
<p><strong>sim-to-real augmentation pipeline</strong> 将5张带标签的真实数据集进行扩充，得到了很好的效果</p>
<p>此论文方法的精度依赖于大量的参数（500M），且与前两名的得分还有较大差距</p>
<p>此论文的实验揭示了几个网络超参数选择和不同估计旋转方向方法的影响。结果表明，基于软分类的方向估计方法比直接回归方法具有更好的估计效果。</p>
<p>存在的问题：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105130442853.png" alt="image-20211105130442853" style="zoom:40%;"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211111094705105.png" alt="image-20211111094705105"></p>
<p>展望：一个很有前途的方向是使用递归神经网络和使用URSO生成的视频序列来解决跟踪问题。作为未来的工作，作者还计划将URSO扩展到SLAM，以定位几何形状未知的目标。</p>
<p>其他：</p>
<p>在benchmark for 6d object pose estimation这篇论文中提出<strong>目前基于点对特征的方法表现最好，优于模板匹配法、基于学习的方法和基于3D局部特征的方法</strong>   (20年基于学习的方法表现更好)</p>
<p>不过通过避免直接估计姿势，而是使用CNN来回归预定义3D关键点的2D投影，并最终使用稳健的PNP解决方案(例如嵌入在RANSAC中)来估计姿势也可取得很好的效果</p>
<h2 id="A-Review-on-Object-Pose-Recovery-from-3D-Bounding-Box-Detectors-to-Full-6D-Pose-Estimators"><a href="#A-Review-on-Object-Pose-Recovery-from-3D-Bounding-Box-Detectors-to-Full-6D-Pose-Estimators" class="headerlink" title="A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators"></a>A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators</h2><h2 id="Vision-based-attitude-estimation-for-spacecraft-docking-operation-through-deep-learning-algorithm"><a href="#Vision-based-attitude-estimation-for-spacecraft-docking-operation-through-deep-learning-algorithm" class="headerlink" title="Vision-based attitude estimation for spacecraft docking operation through deep learning algorithm"></a>Vision-based attitude estimation for spacecraft docking operation through deep learning algorithm</h2><p>这篇感觉就在urso那篇论文上改了下网络和损失函数，没太多创新点，但是对urso论文中一些概念解释还不错</p>
<p>而且实验的结果也不好，对于旋转的角度误差大的离谱</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211104204858051.png" alt="image-20211104204858051" style="zoom:60%;"></p>
<h2 id="POSE-ESTIMATION-FOR-NON-COOPERATIVE-SPACECRAFT-RENDEZVOUS-USING-NEURAL-NETWORKS"><a href="#POSE-ESTIMATION-FOR-NON-COOPERATIVE-SPACECRAFT-RENDEZVOUS-USING-NEURAL-NETWORKS" class="headerlink" title="POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS"></a>POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS</h2><p>这篇论文提出SPN网络以及发布了<strong>SPEED</strong>数据集</p>
<p><strong>SPN</strong>网络总体结构如下：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105162747501.png" alt="image-20211105162747501"></p>
<p>下图解释了参考系、相对位置和相对姿态的定义：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105162830993.png" alt="image-20211105162830993" style="zoom:40%;"></p>
<p>下图说明了SPN方法中所使用的卷积神经网络：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105163010801.png" alt="image-20211105163010801"></p>
<p>该网络的branch1用到了《Faster R-CNN: Towards Real-Time Object Detection with <strong>Region Proposal Networks</strong> 》，输出目标的矩形框</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211105185913215.png" alt="image-20211105185913215" style="zoom:40%;"></p>
<p>k是指每个特征点对应的先验框anchors的数量</p>
<p>在每个滑动窗口位置，我们同时预测 k 个候选矩形框，那么 reg layer 有 4k 个输出用于表示 k 个矩形框的坐标位置及尺寸大小信息。 cls layer 输出 2k 个概率用于表示每个矩形框包含/不包含物体的概率信息</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1436729">重温目标检测—Faster R-CNN - 云+社区 - 腾讯云 (tencent.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq314000558/article/details/82082911">Region Proposal Networks 详解_qq314000558的专栏-CSDN博客</a></p>
<p>branch2和３使用了一种混合分类与回归的方法，来确定相对旋转角度。(也可称为基于软分类的<strong>方向估计</strong>)</p>
<p>方向估计使用两个头分支：一个进行硬分类，给出一组预定义的四元数，以找到距离真值最近N个的四元数，然后第二个分支估计这N个四元数的权重，最终的方向由加权平均四元数给出。</p>
<p>branch2 执行分类任务。设置m（1000）个预定义旋转角度，输出每个角度是前n（3）个最接近真值的角度的概率。branch2的输出为向量v，其维度为m*1。</p>
<p>branch3使用branch2的输出作为输入，执行回归任务，输出向量w，其维度也为m*1。输出brach2每个结果的权重，但后面只取了前n个概率最大结果的权重（即最大的n个vj所对应的结果）使用，其余m-n个权重输出并未使用。然后将这n个权重通过softmax函数</p>
<p>由brach1输出的二维边界框、brach2和3所得到的相对旋转角度、几何约束相结合，使用高斯-牛顿算法估计相对位置。（基于包围框检测的<strong>位置估计</strong>）</p>
<h2 id="Satellite-Pose-Estimation-with-Deep-Landmark-Regression-and-Nonlinear-Pose-Refinement（第一名）"><a href="#Satellite-Pose-Estimation-with-Deep-Landmark-Regression-and-Nonlinear-Pose-Refinement（第一名）" class="headerlink" title="Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement（第一名）"></a>Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement（第一名）</h2><p>姿态估计分为三步：<strong>目标检测、关键点回归、PnP求解</strong></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211108213040358.png" alt="image-20211108213040358"></p>
<p>这篇文章的采用了HRNet</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109112921791.png" alt="image-20211109112921791"></p>
<h2 id="Segmentation-driven-Satellite-Pose-Estimation（第二名）"><a href="#Segmentation-driven-Satellite-Pose-Estimation（第二名）" class="headerlink" title="Segmentation-driven Satellite Pose Estimation（第二名）"></a>Segmentation-driven Satellite Pose Estimation（第二名）</h2><p><a target="_blank" rel="noopener" href="https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf">ppt——EPFL_CVLAB</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cvlab-epfl/segmentation-driven-pose">GitHub - cvlab-epfl/segmentation-driven-pose: Segmentation-driven 6D Object Pose Estimation. CVPR 2019.</a>根据他们实验室的这篇论文做的</p>
<p>这个实验室的另外一篇6D姿态估计论文<a target="_blank" rel="noopener" href="https://github.com/cvlab-epfl/single-stage-pose">GitHub - cvlab-epfl/single-stage-pose: Single-Stage 6D Object Pose Estimation, CVPR 2020</a>：</p>
<p>之前很多工作，都是先检测出2D图片上的一些关键点，然后建立2D-3D的correspondings，最后通过基于 <strong><em>RANSAC</em></strong> 的 <strong><em>Pnp</em></strong> 算法，求出最后的6D姿态。这篇文章主要的创新点是把基于 <strong><em>RANSAC</em></strong> 的 <strong><em>Pnp</em></strong> 算法集成到了网络之中，形成了一个End-to-end的网络。</p>
<h2 id="REAL-TIME-FLIGHT-READY-NON-COOPERATIVE-SPACECRAFT-POSE-ESTIMATION-USING-MONOCULAR-IMAGERY"><a href="#REAL-TIME-FLIGHT-READY-NON-COOPERATIVE-SPACECRAFT-POSE-ESTIMATION-USING-MONOCULAR-IMAGERY" class="headerlink" title="REAL-TIME, FLIGHT-READY, NON-COOPERATIVE SPACECRAFT POSE ESTIMATION USING MONOCULAR IMAGERY"></a>REAL-TIME, FLIGHT-READY, NON-COOPERATIVE SPACECRAFT POSE ESTIMATION USING MONOCULAR IMAGERY</h2><p>在保证精度的同时又十分轻量级</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211108211901970.png" alt="image-20211108211901970"></p>
<p>姿态估计分为三步：<strong>目标检测、关键点回归、PnP求解</strong></p>
<p>目标检测将感兴趣区域裁剪后输出给关键点回归网络。关键点回归网络对航天器模型上预定的三维表面关键点的二维位置进行回归，然后利用PnP求得姿态。</p>
<h2 id="Segmentation-driven-6D-Object-Pose-Estimation（CVPR-2019-by-cvlab）"><a href="#Segmentation-driven-6D-Object-Pose-Estimation（CVPR-2019-by-cvlab）" class="headerlink" title="Segmentation-driven 6D Object Pose Estimation（CVPR 2019 by cvlab）"></a>Segmentation-driven 6D Object Pose Estimation（CVPR 2019 by cvlab）</h2><p><a target="_blank" rel="noopener" href="https://github.com/cvlab-epfl/segmentation-driven-pose">cvlab-epfl/segmentation-driven-pose: Segmentation-driven 6D Object Pose Estimation. CVPR 2019. (github.com)</a></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220120102831564.png" alt="image-20220120102831564"></p>
<h2 id="Single-Stage-6D-Object-Pose-Estimation（CVPR-2020-by-cvlab）"><a href="#Single-Stage-6D-Object-Pose-Estimation（CVPR-2020-by-cvlab）" class="headerlink" title="Single-Stage 6D Object Pose Estimation（CVPR 2020 by cvlab）"></a>Single-Stage 6D Object Pose Estimation（CVPR 2020 by cvlab）</h2><p>传统方法采用网络回归关键点，再加上ransac+pnp(但是这部分不属于神经网络的一部分)。</p>
<p>本文将ransac+pnp融入到深度神经网络之中。</p>
<h2 id="Wide-Depth-Range-6D-Object-Pose-Estimation-in-Space-CVPR-2021-by-cvlab"><a href="#Wide-Depth-Range-6D-Object-Pose-Estimation-in-Space-CVPR-2021-by-cvlab" class="headerlink" title="Wide-Depth-Range 6D Object Pose Estimation in Space(CVPR 2021 by cvlab)"></a>Wide-Depth-Range 6D Object Pose Estimation in Space(CVPR 2021 by cvlab)</h2><p><a target="_blank" rel="noopener" href="https://github.com/cvlab-epfl/wide-depth-range-pose">cvlab-epfl/wide-depth-range-pose: Wide-Depth-Range 6D Object Pose Estimation in Space, CVPR 2021 (github.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/440557881">洛桑理工CVPR-21：太空中大深度范围6D物体位姿估计 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/APbqBaxLoZfcrjOi69jQLA">洛桑理工CVPR-21：太空中大深度范围6D物体位姿估计 (qq.com)</a></p>
<h3 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h3><p>空间6D姿态估计带来了独特的挑战，这在地面环境中并不常见。最显著的区别之一是<strong>缺乏大气散射</strong>，这使得物体可以从很远的距离看到，同时使<strong>照明条件复杂化</strong>。目前可用的基准数据集没有充分强调这一方面，并且大多描述了非常接近的目标。</p>
<p>在处理<strong>大深度范围变化</strong>下的姿态估计之前的工作依赖于<strong>两个阶段</strong>的方法，首先估计尺度，然后在调整大小的图像块上进行姿态估计。相反，我们提出了一种<strong>单级</strong>分层端到端可训练网络，该网络对规模变化更具鲁棒性。我们证明，它不仅在合成图像以类似于在空间拍摄的图像上，而且在标准基准上都优于现有方法。</p>
<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206214920301.png" alt="image-20211206214920301" style="zoom:40%;"></p>
<p>目前面临的挑战：目标尺度和方向变化大（需要不同的图像特征）、空间材料镜面反射、图像某些部分的过度/不足曝光以及其他部分的细节缺失</p>
<p>19年的卫星姿态估计比赛中最好的方法采用的是两阶段的方法：探测器发现一个围绕目标的轴对齐的盒子，该盒子被重新采样到一个统一的大小，最后由一个6D姿态估计器进行处理。但这个方法在几个方面仍有缺点，如下：</p>
<ol>
<li>目标检测和姿态估计被视为两个单独的过程，这排除了联合的训练</li>
<li>其次，它只向正在使用的编解码器结构的最后一层提供监督信号，而不是向解码金字塔的所有级别提供监督信号，这将增加鲁棒性。</li>
<li>两个过程都执行了许多相似的特征提取计算，这导致了不必要的重复工作</li>
<li>这些方法依赖于基于深度学习的6D对象姿势估计的主要方法，包括训练网络以最小化预定义3D关键点的2D重投影误差，该误差无法处理较大的深度范围变化：如图2所示，重投影误差受单个关键点到相机的距离的强烈影响，不明确考虑这一点会降低性能</li>
</ol>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206221242701.png" alt="image-20211206221242701" style="zoom:40%;"></p>
<p>为了解决这些缺点，我们引入了一个单一阶段的分层端到端可训练网络，如图3所示，该网络可产生鲁棒且不区分比例的6D姿势。</p>
<p>这大多数仅从最后一层估计姿势的网络不同。为了跨尺度使用信息，该网络逐步缩小学习到的特征，为结果金字塔的每个层级导出3D到2D对应，最后使用基于RANSAC的PnP策略从这些对应集合推断出单个可靠姿势。</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211206221605058.png" alt="image-20211206221605058" style="zoom:50%;"></p>
<p>为了跨尺度使用信息，它逐步缩小学习到的特征，为结果金字塔的每个层级导出3D到2D对应，最后使用基于RANSAC的PnP策略从这些对应集合推断出单个可靠姿势。这与大多数仅从最后一层估计姿势的网络不同。为了解决图2中的问题，我们<strong>基于3D位置而不是2D投影</strong>来最小化训练损失，从而使该方法对目标距离保持不变。我们使用特征金字塔网络（FPN）[24]作为主干，但与大多数依赖此类网络的方法不同，我们将每个训练实例分配到多个金字塔级别，以促进多尺度信息的联合使用。</p>
<p>简言之，我们的贡献是一种新的6D姿势估计架构，它可以可靠地处理具有<strong>挑战性条件下的大规模变化</strong>。我们将证明，在已建立的<strong>SPEED</strong>数据集上，它<strong>优于所有最先进的方法，同时速度也快得多</strong>。此外，我们还介绍了一个<strong>更大规模的卫星姿态估计数据集</strong>，该数据集具有比SPEED更真实、更复杂的图像，并且我们表明，我们的方法在这个更具挑战性的场景中提供了相同的好处。最后，我们证明了我们的方法即使在<strong>深度变化较小的图像</strong>上（例如具有挑战性的遮挡LINEMOD数据集）也优于最新技术。</p>
<h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><p>一般来说标准6D姿态估计流程为：首先建立三维到二维的对应关系，然后使用PnP解算器计算姿势</p>
<p>之前的其他方法的设计都是为了在标准的计算机视觉基准上有效，其特点是尺度变化较小，当描绘对象的深度范围在不同的图像中发生显著变化时，它们的性能较差。部分尝试处理缩放问题的工作多采用两阶段的方法，使得目标检测和姿态估计分离开来，使得网络结构大大复杂化，引入大量冗余操作，进而导致实时性不高。</p>
<p>我们的<strong>主要贡献</strong>是利用单个网络固有的层次结构，在各个层次上共享权重，以处理尺度变化问题。我们证明了这一点既健壮又高效。</p>
<p>分层处理，如图像金字塔，是多尺度图像理解的经典理念。最近，这一想法已经通过特征金字塔网络（FPN）转化为深度学习领域，现在它是许多目标检测框架的标准组件，我们将此想法用于6D姿态估计。然而，与大多数将每个金字塔级别显式关联到单个预定义比例的对象检测方法不同，我们引入了一种<strong>动态采样策略</strong>，其中每个训练实例利用所有金字塔级别，尽管权重不同。这使我们能够在推理时融合来自不同层面的预测，从而得到更稳健的6D姿势估计。</p>
<p>我们将实验重点放在星载物体的6D姿态估计上，因为在这种情况下，对缩放的鲁棒性非常重要，特别是在接近需要运动同步的非合作目标（如空间垃圾）时。空间工程界有自己关于6D姿态估计的文献。虽然它的发展方式类似于计算机视觉的进步，但它主要关注手工制作的方法，只有少数作品提出了基于深度学习的方法。其主要原因是缺乏大量的空间物体注释数据。</p>
<p><strong>第二个贡献</strong>是提出了一个基于物理渲染创建的<strong>SwissCube数据集</strong>。该数据集中的图像是使用基于物理的光谱光传输模拟创建的，该模拟涉及一个立方体卫星的精确参考3D模型，该模型考虑了太阳、地球、恒星等的影响。</p>
<p>之前由ESA提出的SPEED数据集有以下的缺点：</p>
<ol>
<li>没提供卫星的三维模型，重建模型有误差</li>
<li>通过基于非物理的渲染技术合成的，不能反映空间照明的复杂性</li>
<li>深度分布不均匀，只有很少的图像描述了距离相机很远的卫星。</li>
</ol>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207095921055.png" alt="image-20211207095921055" style="zoom:50%;"></p>
<h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h3><p>使用特征金字塔在多尺度上回归预定义3D点的2D投影</p>
<h4 id="3-1-网络金字塔结构（Pyramid-Network-Architecture）"><a href="#3-1-网络金字塔结构（Pyramid-Network-Architecture）" class="headerlink" title="3.1 网络金字塔结构（Pyramid Network Architecture）"></a>3.1 网络金字塔结构（Pyramid Network Architecture）</h4><p>大多数6D姿势估计深度网络依赖于编码器-解码器体系结构。因此，为了处理6D对象姿态估计的大范围变化，我们<strong>使用编码器网络固有的层次结构，而不是依赖于额外的对象检测网络</strong>，它提取不同尺度的特征。具体而言，我们在框架中使用Darknet-53[34]作为主干，并采用与FPN[24]中设计的用于目标检测的网络架构相同的网络架构，该网络架构由k=5级特征图组成，{F1、F2、F3、F4、F5}，<strong>每个特征图具有越来越大的感受野</strong>。</p>
<p>我们从金字塔的<strong>每一层回归对象3D关键点的2D位置，而不是仅从特征图F5计算单个姿势估计</strong>。为此，我们依赖于[11]中的分段驱动方法，在每个特征地图的每个空间位置生成特征向量，以输出3D关键点的2D投影（表示为相对于相应单元中心的偏移量）和每个对象类的对象性分数。因此，每个单元的特征向量是一个C×（2×8+1）维向量，由8个2D关键点的偏移位置（见下图）和1个objectness(物体存在于该cell的概率)。要对分割掩码进行编码，所有特征单元都需要参与对象性预测，包括不包含目标对象的单元。相反，如下所述，只有选定的单元参与姿势回归器的训练。</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208101923121.png" alt="image-20211208101923121" style="zoom:50%;"></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011984148/article/details/112504800">(29条消息) 理解物体检测中的Objectness_AI公园-CSDN博客</a></p>
<h4 id="3-2-集合感知采样（Ensemble-Aware-Sampling）"><a href="#3-2-集合感知采样（Ensemble-Aware-Sampling）" class="headerlink" title="3.2 集合感知采样（Ensemble-Aware Sampling）"></a>3.2 集合感知采样（Ensemble-Aware Sampling）</h4><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/86f39172b68a">特征向量（Feature Vectors） - 简书 (jianshu.com)</a></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211209163724502.png" alt="image-20211209163724502" style="zoom:40%;"></p>
<p>处理尺度变化过大问题的通用方法为：分治策略——将训练集分为不重复的组，之后在训练过程中将不同的pyramid层分配到不同尺度。此策略在目标检测中是够用的，因为可以在不同的层输出最优结果。但是在6D估计中却不能够联合多层信息提升鲁棒性，单个层的输出会引入较高的噪声。</p>
<p>具体采样过程如下，需要根据某一个训练实例的尺寸大小动态确定在每一层的采样数量。λ控制着每层的激活cell数量，λ=0时在所有层取相同的cell数量；λ&gt;20过大时退化为FPN结构中的“hard assignment”</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/v2-1fad96cf5150c0d3a42a8580cdead3c9_720w.jpg" alt="img"></p>
<h4 id="3-3-三维空间中的损失函数-（Loss-Function-in-3D-Space）"><a href="#3-3-三维空间中的损失函数-（Loss-Function-in-3D-Space）" class="headerlink" title="3.3 三维空间中的损失函数 （Loss Function in 3D Space）"></a>3.3 三维空间中的损失函数 （Loss Function in 3D Space）</h4><p>在采样过程中选择的特征向量用于回归3D包围盒的8个角的2D投影。在回归2D关键点位置时，大多数方法采用的损失函数为：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207194451711.png" alt="image-20211207194451711" style="zoom:50%;"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211207194548714.png" alt="image-20211207194548714" style="zoom:50%;"></p>
<p>但正如图2所示，这种损失函数是次优的，特别是在存在大深度变化的情况下，因为(a)它更强调某些关键点而不是其他关键点(不同关键点误差对损失函数的影响不同)，(b)并且还取决于物体的相对位置（物体不同位置的误差影响也不同）。</p>
<p>为了解决以上问题，本文在3D空间引入损失函数（其实就是把预测的2D关键点根据位姿还原到3D，然后再和真实值比较）</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49981234">Focal loss论文详解 - 知乎 (zhihu.com)</a> </p>
<h4 id="3-4-多尺度融合推理-（Inference-via-Multi-Scale-Fusion）"><a href="#3-4-多尺度融合推理-（Inference-via-Multi-Scale-Fusion）" class="headerlink" title="3.4 多尺度融合推理 （Inference via Multi-Scale Fusion）"></a>3.4 多尺度融合推理 （Inference via Multi-Scale Fusion）</h4><p>Ensemble-awaresampling（智能采样策略）使得每个层都能够生成有效pose信息，因此可以在所有层中使用阈值=0.3过滤objectness score。所有结果可以统一使用RANSAC+PnP方法融合，也可以使用基于学习的方法融合（Hu-Single stage 6D pose）。</p>
<p>本文为简单使用RANSAC+PnP方法。首先估计物体尺寸S，选取所有层中objectness score最高的特征向量，通过8个box角计算S。明确尺寸后，从每个层中选取Nk个高分特征cell，以此来构造如下2D-3D对应关系。最后通过RANSAC+PnP方法完成姿势估计。</p>
<p><strong>推理总结</strong>：依然使用智能采样策略，通过最高分确定尺寸S，之后正常使用采样策略从各个层中选取特征数量，全部用于姿势估计。</p>
<h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4.实验"></a>4.实验</h3><p>首先在SPEED数据集上测试，采用的评估指标为比赛的指标。</p>
<p>然后介绍SwissCube数据集，采用ADI-0.1d指标，以评估不同深度范围的性能。并在真实图像进行了测试(请注意，这些真实的图像不是在太空中捕获的，而是在实验室环境中使用目标的模型和OptiTrack运动捕获系统来获取一些图像的地面真实姿势信息。)。</p>
<p>最后为了证明该方法的通用性，在描述小深度变化的Occluded-LINEMOD标准数据集上对其进行了评估。</p>
<h4 id="4-1-Evaluation-on-the-SPEED-Dataset"><a href="#4-1-Evaluation-on-the-SPEED-Dataset" class="headerlink" title="4.1 Evaluation on the SPEED Dataset"></a>4.1 Evaluation on the SPEED Dataset</h4><p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208103411496.png" alt="image-20211208103411496" style="zoom:50%;"></p>
<h4 id="4-2-Evaluation-on-the-SwissCube-Dataset"><a href="#4-2-Evaluation-on-the-SwissCube-Dataset" class="headerlink" title="4.2 Evaluation on the SwissCube Dataset"></a>4.2 Evaluation on the SwissCube Dataset</h4><p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208104306785.png" alt="image-20211208104306785" style="zoom:50%;"></p>
<p>SwissCube Dataset 删除了进入地球阴影的部分，因为这部分的图像基本纯黑。一共有500个场景，每个场景100帧图像。</p>
<p>相机距离在1d-10d之间，d为SwissCube 卫星的直径（不含天线），分为近、中、远三类，分别对应距离[1d-4d], [4d-7d],  [7d-10d]</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208105825232.png" alt="image-20211208105825232"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208105838151.png" alt="image-20211208105838151" style="zoom:50%;"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208110509594.png" alt="image-20211208110509594" style="zoom:50%;"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208110521554.png" alt="image-20211208110521554" style="zoom:50%;"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208140720710.png" alt="image-20211208140720710" style="zoom:50%;"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211208141002214.png" alt="image-20211208141002214" style="zoom:50%;"></p>
<h4 id="4-3-Results-on-Real-Images"><a href="#4-3-Results-on-Real-Images" class="headerlink" title="4.3. Results on Real Images"></a>4.3. Results on Real Images</h4><h4 id="4-4-Evaluation-on-Occluded-LINEMOD"><a href="#4-4-Evaluation-on-Occluded-LINEMOD" class="headerlink" title="4.4. Evaluation on Occluded-LINEMOD"></a>4.4. Evaluation on Occluded-LINEMOD</h4><h3 id="代码实测结果"><a href="#代码实测结果" class="headerlink" title="代码实测结果"></a>代码实测结果</h3><p>python test.py —config_file ./configs/swisscube.yaml —num_workers 0 —weight_file ‘./swisscube_pretrained.pth’ —running_device ‘cuda’</p>
<p>python train.py —config_file ./configs/swisscube.yaml —num_workers 0 —weight_file ‘./swisscube_pretrained.pth’ —running_device ‘cuda’</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211210103613085.png" alt="image-20211210103613085"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211210103622008.png" alt="image-20211210103622008"></p>
<p>作者提供的模型精度远高于论文中的精度</p>
<p>backbone的输出：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211214103709819.png" alt="image-20211214103709819" style="zoom:50%;"></p>
<p>fpn的输出：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211214103620809.png" alt="image-20211214103620809" style="zoom:60%;"></p>
<p>PoseHead的输出：</p>
<p>测试时总model的输出pred：由score, cls_id, R, T组成 </p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211215103709168.png" alt="image-20211215103709168"></p>
<p>speed数据集模型重建：</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105160244996.png" alt="image-20220105160244996"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105160316940.png" alt="image-20220105160316940"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105165415536.png" alt="image-20220105165415536"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20220105165427442.png" alt="image-20220105165427442"></p>
<h2 id="申请书要求"><a href="#申请书要求" class="headerlink" title="申请书要求"></a>申请书要求</h2><p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109163033078.png" alt="image-20211109163033078"></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211109163041316.png" alt="image-20211109163041316"></p>
<h2 id="相关调研"><a href="#相关调研" class="headerlink" title="相关调研"></a>相关调研</h2><h3 id="马鸿英学长："><a href="#马鸿英学长：" class="headerlink" title="马鸿英学长："></a><strong>马鸿英</strong>学长：</h3><p>天线姿态估计</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143216549.png" alt="image-20211116143216549" style="zoom:33%;"></p>
<p>目标：只用测量出偏角θ</p>
<p>方法：最小化重投影误差</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143457174.png" alt="image-20211116143457174" style="zoom: 33%;"></p>
<p>这个方法在《POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS》这篇卫星姿态估计论文中使用。</p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211116143633683.png" alt="image-20211116143633683" style="zoom: 33%;"></p>
<h3 id="张澜涛学长："><a href="#张澜涛学长：" class="headerlink" title="张澜涛学长："></a>张澜涛学长：</h3><p>他们目前还在做锥套的检测，飞机的姿态估计还没怎么开始做，目前打算用某个关键点检测的方法，两相机各看到一些固定的关键点，三角测量这些关键点得到关键点坐标，算点集空间变换。</p>
<p>经讨论觉得卫星姿态估计和飞机的姿态估计几乎差不多，后面我们可以一起做，只不过他们的项目对于检测速度的要求特别高，而我目前看到的一篇轻量化的卫星姿态估计的网络最快也只能达到6.6Hz（不过这个得看在什么设备上跑）。</p>
<h2 id="2021官方入门工具包"><a href="#2021官方入门工具包" class="headerlink" title="2021官方入门工具包"></a>2021官方入门工具包</h2><p><a target="_blank" rel="noopener" href="https://gitlab.com/EuropeanSpaceAgency/speedplus-utils">EuropeanSpaceAgency / speedplus-utils · GitLab</a></p>
<p><img src="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20211117102805335.png" alt="image-20211117102805335"></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>young
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://youngyyp.github.io/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%8D%AB%E6%98%9F%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/" title="卫星姿态估计调研">https://youngyyp.github.io/2021/10/20/计算机视觉/位姿估计/卫星姿态估计/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/" rel="tag"><i class="fa fa-tag"></i> 位姿估计</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%A1%80(%E6%9D%A8%E4%B8%9A%E9%B9%8F-20220310203222)/" rel="prev" title="位姿估计基础及论文">
      <i class="fa fa-chevron-left"></i> 位姿估计基础及论文
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8%E5%8E%BB%E9%9B%BE/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%95%B4%E7%90%86/" rel="next" title="图像去雾去雨论文整理">
      图像去雾去雨论文整理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <!--插入访客地图-->
      <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=oY_lgWsBgu9UBtnnlW5wbM7G1cJY_9ZHdsx3MAT2Yao&cl=ffffff&w=a"></script>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Satellite-Pose-Estimation-Challenge-Dataset-Competition-Design-and-Results"><span class="nav-number">1.</span> <span class="nav-text">Satellite Pose Estimation Challenge: Dataset,Competition Design and Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning-for-Spacecraft-Pose-Estimation-from-Photorealistic-Rendering-%EF%BC%882020-ICRA%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering （2020 ICRA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Review-on-Object-Pose-Recovery-from-3D-Bounding-Box-Detectors-to-Full-6D-Pose-Estimators"><span class="nav-number">3.</span> <span class="nav-text">A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vision-based-attitude-estimation-for-spacecraft-docking-operation-through-deep-learning-algorithm"><span class="nav-number">4.</span> <span class="nav-text">Vision-based attitude estimation for spacecraft docking operation through deep learning algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POSE-ESTIMATION-FOR-NON-COOPERATIVE-SPACECRAFT-RENDEZVOUS-USING-NEURAL-NETWORKS"><span class="nav-number">5.</span> <span class="nav-text">POSE ESTIMATION FOR NON-COOPERATIVE SPACECRAFT RENDEZVOUS USING NEURAL NETWORKS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Satellite-Pose-Estimation-with-Deep-Landmark-Regression-and-Nonlinear-Pose-Refinement%EF%BC%88%E7%AC%AC%E4%B8%80%E5%90%8D%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement（第一名）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Segmentation-driven-Satellite-Pose-Estimation%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%90%8D%EF%BC%89"><span class="nav-number">7.</span> <span class="nav-text">Segmentation-driven Satellite Pose Estimation（第二名）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#REAL-TIME-FLIGHT-READY-NON-COOPERATIVE-SPACECRAFT-POSE-ESTIMATION-USING-MONOCULAR-IMAGERY"><span class="nav-number">8.</span> <span class="nav-text">REAL-TIME, FLIGHT-READY, NON-COOPERATIVE SPACECRAFT POSE ESTIMATION USING MONOCULAR IMAGERY</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Segmentation-driven-6D-Object-Pose-Estimation%EF%BC%88CVPR-2019-by-cvlab%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">Segmentation-driven 6D Object Pose Estimation（CVPR 2019 by cvlab）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-Stage-6D-Object-Pose-Estimation%EF%BC%88CVPR-2020-by-cvlab%EF%BC%89"><span class="nav-number">10.</span> <span class="nav-text">Single-Stage 6D Object Pose Estimation（CVPR 2020 by cvlab）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wide-Depth-Range-6D-Object-Pose-Estimation-in-Space-CVPR-2021-by-cvlab"><span class="nav-number">11.</span> <span class="nav-text">Wide-Depth-Range 6D Object Pose Estimation in Space(CVPR 2021 by cvlab)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0-%E6%91%98%E8%A6%81"><span class="nav-number">11.1.</span> <span class="nav-text">0. 摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="nav-number">11.2.</span> <span class="nav-text">1. 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">11.3.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%96%B9%E6%B3%95"><span class="nav-number">11.4.</span> <span class="nav-text">3. 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E7%BD%91%E7%BB%9C%E9%87%91%E5%AD%97%E5%A1%94%E7%BB%93%E6%9E%84%EF%BC%88Pyramid-Network-Architecture%EF%BC%89"><span class="nav-number">11.4.1.</span> <span class="nav-text">3.1 网络金字塔结构（Pyramid Network Architecture）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E9%9B%86%E5%90%88%E6%84%9F%E7%9F%A5%E9%87%87%E6%A0%B7%EF%BC%88Ensemble-Aware-Sampling%EF%BC%89"><span class="nav-number">11.4.2.</span> <span class="nav-text">3.2 集合感知采样（Ensemble-Aware Sampling）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%EF%BC%88Loss-Function-in-3D-Space%EF%BC%89"><span class="nav-number">11.4.3.</span> <span class="nav-text">3.3 三维空间中的损失函数 （Loss Function in 3D Space）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%9E%8D%E5%90%88%E6%8E%A8%E7%90%86-%EF%BC%88Inference-via-Multi-Scale-Fusion%EF%BC%89"><span class="nav-number">11.4.4.</span> <span class="nav-text">3.4 多尺度融合推理 （Inference via Multi-Scale Fusion）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="nav-number">11.5.</span> <span class="nav-text">4.实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Evaluation-on-the-SPEED-Dataset"><span class="nav-number">11.5.1.</span> <span class="nav-text">4.1 Evaluation on the SPEED Dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Evaluation-on-the-SwissCube-Dataset"><span class="nav-number">11.5.2.</span> <span class="nav-text">4.2 Evaluation on the SwissCube Dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Results-on-Real-Images"><span class="nav-number">11.5.3.</span> <span class="nav-text">4.3. Results on Real Images</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-Evaluation-on-Occluded-LINEMOD"><span class="nav-number">11.5.4.</span> <span class="nav-text">4.4. Evaluation on Occluded-LINEMOD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">11.6.</span> <span class="nav-text">代码实测结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%B3%E8%AF%B7%E4%B9%A6%E8%A6%81%E6%B1%82"><span class="nav-number">12.</span> <span class="nav-text">申请书要求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E8%B0%83%E7%A0%94"><span class="nav-number">13.</span> <span class="nav-text">相关调研</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E9%B8%BF%E8%8B%B1%E5%AD%A6%E9%95%BF%EF%BC%9A"><span class="nav-number">13.1.</span> <span class="nav-text">马鸿英学长：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E6%BE%9C%E6%B6%9B%E5%AD%A6%E9%95%BF%EF%BC%9A"><span class="nav-number">13.2.</span> <span class="nav-text">张澜涛学长：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2021%E5%AE%98%E6%96%B9%E5%85%A5%E9%97%A8%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="nav-number">14.</span> <span class="nav-text">2021官方入门工具包</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="young"
      src="https://raw.githubusercontent.com/youngyyp/blogpicture/master/img/image-20210212125129052.png">
  <p class="site-author-name" itemprop="name">young</p>
  <div class="site-description" itemprop="description">你的征途当是星辰大海</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">young</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

<span id="busuanzi_container_site_uv">
  本站访问次数：<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
</span>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="//cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js"></script>
    <script defer src="//cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Xwh80uoDDnqi8uKvWVp8c9rr-gzGzoHsz',
      appKey     : 'I7r0pki5w3TvCpU6LC8I3b2R',
      placeholder: "欢迎大家评论哦~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-CN' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
